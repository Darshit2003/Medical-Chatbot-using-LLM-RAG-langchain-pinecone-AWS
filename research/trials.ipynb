{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a94aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%pwd\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36020736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\ML_Projects\\\\medical-chatbot\\\\Medical-Chatbot-using-LLM-RAG-langchain-pinecone-AWS'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2778a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "095fe75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_file(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "        )\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "077e0f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf_file(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa10f0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d024cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def filter_to_minimal_docs(docs: List[Document]) -> List[Document]:\n",
    "    minimal_docs: List[Document] = []\n",
    "    for doc in docs:\n",
    "        src = doc.metadata.get(\"source\")\n",
    "        minimal_docs.append(\n",
    "            Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\n",
    "                    \"source\": src,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return minimal_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42f5cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_docs = filter_to_minimal_docs(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4d686f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\Advances_in_Distributed_Computing_and_Artificial_Intelligence_Journal.pdf'}, page_content='Advances in Distributed Computing\\nand Artificial Intelligence Journal\\nDiscipline Artificial intelligence, distributed\\ncomputing\\nLanguage English\\nEdited by Juan M. Corchado, Sigeru\\nOmatu\\nPublication details\\nHistory 2012–present\\nPublisher Ediciones Universidad de\\nSalamanca (Spain)\\nFrequency Continuous\\nOpen access Yes\\nLicense CC BY\\nImpact factor 1.7 (2023)\\nStandard abbreviations\\nISO 4 Adv. Distrib. Comput. Artif.\\nIntell. J.\\nIndexing\\nISSN 2255-2863 (https://www.worldca\\nt.org/search?fq=x0:jrnl&q=n2:22\\n55-2863) (print)\\n2255-2863 (https://www.worldca\\nt.org/search?fq=x0:jrnl&q=n2:22\\n55-2863) (web)\\nOCLC no. 862779541 (https://www.worldc\\nat.org/oclc/862779541)\\nLinks\\nJournal homepage (https://adcaij.usal.es)\\nOnline access to volumes (https://revistas.u\\nsal.es/cinco/index.php/2255-2863/index)\\nGuidelines for authors (https://adcaij.usal.e\\ns/authors/guidelines)\\nAdvances in Distributed Computing and\\nArtificial Intelligence Journal\\nAdvances in Distributed Computing and Artificial\\nIntelligence Journal is a peer-reviewed scientific\\njournal covering the fields of artificial intelligence and\\ndistributed computing and applying these techniques in\\nareas like machine learning, electronic commerce,\\ngenerative AI, the Internet of Things, smart grids, etc.\\nThe journal is published by the Ediciones Universidad\\nde Salamanca, a subsidiary of the University of\\nSalamanca and follows a diamond open access\\npublishing model.\\nThe journal was established in 2012 by the University\\nof Salamanca. The frequency of issues was initially\\nquarterly, but the journal changed to publication in\\ncontinuous mode in 2023. The editors-in-chief are Juan\\nM. Corchado (University of Salamanca) and Sigeru\\nOmatu (Hiroshima University).\\nThe journal is abstracted and indexed in:\\nDirectory of Open Access Journals\\nProQuest databases\\nSherpa ROMEO\\nEmerging Sources Citation Index\\nAccording to the Journal Citation Reports, the journal\\nhas a 2023 impact factor of 1.7.[1]\\n1. \"Advances in Distributed Computing and\\nArtificial Intelligence Journal\". 2023 Journal\\nHistory\\nAbstracting and indexing\\nReferences'),\n",
       " Document(metadata={'source': 'data\\\\Advances_in_Distributed_Computing_and_Artificial_Intelligence_Journal.pdf'}, page_content='Citation Reports (Emerging Sources ed.). Clarivate. 2024 – via Web of Science.\\nOfficial website (https://adcaij.usal.es)\\nRetrieved from \"https://en.wikipedia.org/w/index.php?\\ntitle=Advances_in_Distributed_Computing_and_Artificial_Intelligence_Journal&oldid=1250335827\"\\nExternal links'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le (1).pdf'}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/355044684\\nGenerative Adversarial Neural Networks and Deep Learning: Successful\\nCases and Advanced Approaches\\nArticle\\xa0\\xa0in \\xa0\\xa0International Journal of Computing · September 2021\\nDOI: 10.47839/ijc.20.3.2278\\nCITATIONS\\n24\\nREADS\\n1,364\\n2 authors, including:\\nYuriy Kondratenko\\nPetro Mohyla Black Sea National University\\n220 PUBLICATIONS\\xa0\\xa0\\xa02,043 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Yuriy Kondratenko on 01 March 2022.\\nThe user has requested enhancement of the downloaded file.'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le (1).pdf'}, page_content='VOLUME 20(3), 2021 339 \\nDate of publication SEP-30, 2021, date of current version JUL-28, 2021. \\nwww.computingonline.net / computing@computingonline.net \\nPrint ISSN 1727-6209 \\nOnline ISSN 2312-5381 \\nDOI 10.47839/ijc.20.3.2278 \\nGenerative Adversarial Neural Networks \\nand Deep Learning: Successful Cases \\nand Advanced Approaches \\nOLEKSANDR STRIUK, YURIY KONDRATENKO \\nPetro Mohyla Black Sea National University, 10 68th Desantnykiv st., Mykolaiv, 54003, Ukraine  \\n(e-mail: oleksandr.striuk@gmail.com, y_kondrat2002@yahoo. com) \\nCorresponding author: Oleksandr Striuk (e-mail: oleksandr.striuk@gmail.com). \\n \\n ABSTRACT Cross-domain artificial intelligence (AI) frameworks are the keys to amplify progress in science. \\nCutting edge deep learning methods offer novel opportunities for retrieving, optimizing, and improving different \\ndata types. AI techniques provide new ways for enhancing and polishing existing models that are used in applied \\nsciences. New breakthroughs in generative adversarial neural networks (GANNs/G ANs) and deep learning allow \\nto drastically increase the quality of diverse graphic samples obtained with research equipment. All these \\ninnovative approaches can be compounded into a unified academic and technological pipeline that can radically \\nelevate and accelerate scientific research and development. The authors analyze a number of successful cases of \\nGAN and deep learning applications in applied scientific fields (including observational astronomy, health care, \\nmaterials science, deep fakes, bioinforma tics, and typography) and discuss advanced approaches for increasing \\nGAN and DL efficiency in terms of performance calibration using modified data samples, algorithmic \\nenhancements, and various hybrid methods of optimization. \\n \\n KEYWORDS generative adversa rial network; neural networks; deep learning; machine learning; artificial \\nintelligence. \\n \\nI. INTRODUCTION \\nURRENT artificial intelligence technologies such as \\ndeep learning (DL) and artificial neural networks – AI \\nsystems inspired by the structure and principles of the human \\nbrain – become true amplifiers of scientific discovery and \\ndevelopment. AI helps to speed up experimental simulations, \\ngather and process new data, prove brand new theoretical \\nhypotheses in many scientific fields. AI is literally relevant \\nto any intellectual task [1]. \\nDeep learning is one of the machine learning methods \\nthat is grounded on an artificial neural networks framework \\nthat can be trained based on supervised and unsupervised \\nlearning algorithms. Deep learning architectures are \\neffectively used in different fields including autonomous \\nvehicles, computer vision, natural language processing, \\nrecommendation services, bioinformatics, medical image \\nanalysis, and generation of new functio nal samples, where \\nthey have shown similar to human experts ’ results or even \\noutperformed them. The main concept of artificial neural \\nnetworks was inspired by real biological systems. Generative \\nadversarial network (GAN) is the implementation of a \\ndeep/machine unsupervised learning algorithm class that \\nrepresents the architecture of two artificial neural networks \\nthat compete with each other in a zero-sum game. \\nToday’s AI methods show incredibly successful practical \\nresults at doing science [2 -6]. AI system s are used as an \\neffective mechanism in diverse scientific fields transforming \\nconventional research practices and expediting discoveries. \\nThe main advantage of AI is that it can outperform humans \\nwhen it comes to processing large amounts of data, detecting \\npatterns and abnormalities that human experts could never \\nhave spotted.  \\nFig. 1 demonstrates the integrated liaisons between key \\nelements of AI. \\nC'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le (1).pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349 \\n340 VOLUME 20(3), 2021 \\n \\nFigure 1. Interconnections and nesting of artificial \\nintelligence systems. \\nThe driving force that triggered an increased interest in \\nthe more intense integration of AI into science were massive \\narrays of data accumulated over many years of research and \\nthe development of high -performance computing platforms \\nthat were able to process and analyze these data sets. In \\nrecent years, artificial intelligence systems have made a great \\ncontribution to the intensification of scientific research. \\nII. SCRUTINIZING HEURISTICS OF SUCCESSFUL \\nCASES \\nThis paper is an overview analysis and considers the \\npractical aspects of the use of machine learning and GAN in \\nthe applied fields of science . In particular, observational \\nastronomy, health care, materials science and deep fake \\ndetection have been selected as illustrative examples.  \\nCases were selected from the most important areas that \\naffect the core scientific research and quality of human life. \\nThe next sections of the paper analyze the fragmented \\ndata of efficient research in this area and assess future \\nprospects. \\nA. GAN METHOD RECOVERS FEATURES IN \\nASTROPHYSICAL IMAGES OF GALAXIES \\nSchawinski et al. demonstrated a machine learning method \\nthat was  able to successfully recover elements in \\nastronomical images of galaxies [7]. The said ML \\nmethodology allows to overcome the deconvolution limit \\nusing higher quality training data sets and makes it possible \\nto reconstruct information from poor quality sam ples by \\nsuccessfully building priors [7].  \\nThe Nyquist–Shannon sampling theorem sets limitations \\nin terms of removing the effect of  the point spread function \\nespecially when there is noise, sequentially sampled material \\ncannot be completely deconvolved without violating the \\ntheorem postulates [7, 8]. As a workaround for this issue , \\nSchawinski et al. applied a generative adversarial neural \\nnetwork (GANN/GAN).  \\nA GAN is a state-of-the-art deep learning algorithm that \\nallows two neural networks to contest with each other in the \\nform of a zero -sum game. This framework can create \\nrealistic artificial graphical samples similar and almost \\nidentical to the images from a training set [9]. \\nGAN works on the following principle: the first network, \\nthe generator, creates  samples (candidates), and the second \\nnetwork, the discriminator, evaluates them, trying to \\ndistinguish real from fakes. The generative network tries to \\nform a new sample by combining primary samples using \\nlatent space variables. The discriminator network learns to \\ndistinguish between real and counterfeit samples. \\nConventional deep learning models are used in GANs as \\ncomponents. For example, the discriminator can be \\nimplemented as a convolutional classifier network. \\nThe suggested method can drastically impr ove the \\nquality of obtained image samples of galaxies by recovering \\nits properties and bypassing the deconvolution constraints \\nmentioned above.  \\nDuring the experiment the GAN has been trained on a \\ndata set that included 4,550 graphic samples of nearby \\ngalaxies in the redshift range 0.01 < z < 0.02 that were taken \\nfrom the Sloan Digital Sky Survey. The results were assessed \\nthrough ten cross-validation iterative cycles. GAN was able \\nto restore artificially corrupted image samples with bad \\nseeing and high noise levels (compared to the original image) \\nand showed results that greatly outperform standard \\ndeconvolution.  \\nThe results proved the effectiveness of the method in \\nrestoring important characteristics of celestial phenomena \\nand in expanding the range of study of existing astronomical \\ndata gathered by telescopes [7]. The images clearly show the \\nability of GAN to restore features that cannot be recovered \\nusing conventional deconvolution techniques. \\nFig. 2 is a graphic illustration of the training process of \\nthe method described by Schawinski et al. A set of original \\nimages is the input. Image degradation is achieved by \\nblurring, adding noise, and through convolution with a worse \\npoint spread function. Degraded images are automatically \\ngenerated and used for GA N training. Only the generator is \\nused for recovering images during the testing phase. \\nHowever, this technique is not without drawbacks and \\nhas its limitations. The main constraint is related to the \\nlimited capacity of the training set that drastically imp acts \\nthe restorative capabilities of the method. Small training data \\nresults in a bad approximation. A model that is trained on \\nsuch poor data will likely demonstrate low performance due \\nto overfitting.  As a possible solution, the training sets that \\nconsist of synthesized simulation images can be considered \\nas an additional reinforcement technique in terms of \\nlearning.'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le (1).pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349  \\nVOLUME 20(3), 2021 341 \\n \\nFigure 2. Graphic illustration of training process of GAN: (a) data preparation; (b) training of GAN \\nB. DEEP LEARNING AND PHOTOMETRIC REDSHIFT \\nESTIMATION \\nA. D’Isanto and K. L. Polsterer proposed a new experimental \\ndeep learning technique the aim of which was developing a \\nnew method for photometric redshift estimation [10]. The \\nsuggested approach demonstrates a novel technique for \\nestimating PDF (probability density function) for redshifts \\nbased on imaging data in such a way tha t ultimately there is \\nno need for additional steps of feature-extraction and feature-\\nselection. The PDF is presented by the following equation \\n(1) [10]: \\n \\n               𝑝(𝑥) = ∑ \\u200a𝑛\\n𝑗=1 𝜔𝑗𝒩(𝑥 ∣ 𝜇𝑗,𝜎𝑗),               (1) \\n \\nwhere 𝒩(𝑥 ∣ 𝜇𝑗,𝜎𝑗) is a normal d istribution, 𝜇𝑗 is a given \\nmean, 𝜎𝑗 is standard deviation, 𝑥 is a given value, 𝜔𝑗 is a \\nweighting factor of each component (all weights sum to one). \\nIn order to achieve the objective, the researchers \\ncombined a deep convolutional network  with a mixture \\ndensity network. The evaluation was presented as Gaussian \\nmixture models as a representation of the probability density \\nfunctions in the redshift space.  As an addition to the \\nconventional estimation methods, the continuous ranked \\nprobability score (CRPS) and the probability integral \\ntransform (PIT) were implemented as performance criteria. \\nThe proposed method was capable of predicting redshift \\nprobability density functions regardless of the type of source \\n(e.g., galaxies, stars, quasars) and showed better results that \\nwere performed by reference techniques and that were \\ndescribed in the scientific literature. This deep learning \\nmethod is highly universal and is able to address any kind of \\nprobabilistic regression problem based on imaging data [10]. \\nThe reviewed example demonstrates the flexibility and \\nversatility of artificial intelligence systems in astrophysical \\nresearch related to the analysis of imaging data. \\nC. GALAXY EVOLUTION RESEARCH WITH \\nGENERATIVE MODELS \\nGenerative models demonstrate the potential for processing \\nastronomical intelligence in a way that focuses on a data -\\ndriven approach. Kevin Schawinski, M. Dennis Turp, and Ce \\nZhang described a method that applies generative models to \\nprobe and research hypotheses in astrophysics and other \\nscientific fields.  \\nDuring the experiment, using a latent space \\nrepresentation of the data, the Fader artificial neural network \\nhas been trained to produce synthes ized data for hypothesis \\nverification [11].  \\nThe architecture of the Fader network is the \\nimplementation of the encoder -decoder system with a \\ndomain adversarial training element that implies researching \\nand processing graphical samples in accordance with t heir \\nphysical features [12]. The Fader network tries to minimize \\nthe following objectives (given pairs of graphic samples and \\nlabels {x, y}) [12]: \\n \\nℒ𝑎𝑒 = −\\n1\\n𝑚 ∑ ∥ 𝐷(𝐸(𝑥),𝑦)−𝑥 ∥2\\n2−𝜆𝐸log\\u2061(𝑃(1−\\n\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061𝑦 ∣ 𝐸(𝑥))),                             (2) \\n \\n             ℒ𝑑𝑖𝑠 = −\\n1\\n𝑚 ∑log\\u2061(𝑃(𝑦 ∣ 𝐸(𝑥))),               (3) \\n \\nwhere 𝑥 represents images, 𝐸(𝑥) is the encoder (neural \\nnetwork) input, which is responsible for mapping from the \\nimage space to a latent representation of fixed dimension \\n[12], 𝐷(𝐸(𝑥),𝑦) is the decoder (a neural network as well), it \\ntakes attempts to rebuild 𝑥, {𝑥,𝑦} are binary labels, ℒ𝒶ℯ and \\nℒ𝒹𝒾𝓈 are two loss functions that interact with each other \\nthrough adversarial cooperation. \\nThe quenching of star formation in galaxies was used as \\nan illustration of the effectiveness of the method because this \\nprocess is well described  in astrophysical literature. In \\naddition to approaches that are based on simulations and \\nobservations, this method can be useful in exploring \\nimportant astronomical and other celestial phenomena from \\na different perspective [12]. The underlying reason why  \\nresearchers picked this architecture is that the Fader can \\ndistinguish two data distributions and learn and visualize \\nthese differences.  \\nAs for the limitations of this approach, it is important to'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le (1).pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349 \\n342 VOLUME 20(3), 2021 \\nstress that the described method is mainly applicable to test \\nhypotheses but not to prove them in a conventional way. \\nAlso, there is always room for mismatch when it comes to \\nthe collation of real data and imperfection of training sets and \\nnetwork design.  \\nThe proposed method requires domain knowledge \\nmanagement by the user since it is not completely \\nunmanned. Anyway, the proposed method of applying \\nFader-like generative models in testing hypotheses and \\nphysical processes modeling showed noteworthy potential in \\nastronomy and other scientific fields [12]. \\nD.4 SPATIAL-GANS AND SYNTHETIC IMAGING \\nMichael J. Smith and James E. Geach focused their research \\nendeavor on the problem of the small size of images \\nproduced by generative adversarial neural networks \\n(GAN/GANN, which are mentioned above) and on the \\nability of the  framework known as Spatial Generative \\nAdversarial Networks (SGANs, designed and described by \\nJetchev et al.) to generate large graphic images, if training \\nimage samples demonstrate a certain level of periodicity – \\nisotropy (cosmological principle) makes deep imaging \\nsurveys fit the criterion [13, 14]. \\nSGAN was trained to produce images resembling the \\neXtreme Deep Field (XDF) – the photo portrait of the \\nuniverse that was assembled by combining 10 years of \\nNASA Hubble Space Telescope photographs, which \\ncontains about 5,500 galaxies even within its smaller field of \\nview. As a result, generated images of fake  galaxies got a \\nhigh level of fidelity with real samples from the XDF in \\nterms of abundance, morphology, magnitude distributions, \\nand colors. In this particular example, researchers have \\ngenerated a 7.6-billion pixel ‘generative deep field’ spanning \\n1.45 degrees, showing that this approach can be extrapolated \\nto other training sets for producing realistic pseudo surveys \\nthat can be successfully applied in astrophysics and other \\nfields [13]. \\nDespite some limitations of the proposed method \\n(generated images a re dependent on the training set; \\nresearchers couldn’t reach stable learning output with more \\nthan three photometric bands) it also has undeniable \\nadvantages. The method is empirically driven because the \\ndata is used as the model, and it can be applied to generate \\nextremely realistic artificial images for the design, \\ndevelopment, and exploitation of new astronomical surveys. \\nFor instance, the technique allows assembling large training \\nsets for different fractionalization and classification tasks in \\nastrophysics [13]. The suggested generative technique \\nmakes it possible to expand small pieces of information \\nretrieved from the early phases of a new survey to a level that \\nwill be applicable for training deep learning models. \\nDescribed categorization and groupin g algorithms can be \\neffectively trained on the generated data and be implemented \\ntowards new data which can lead to expediting the \\nprocessing of data of new surveys. Equation 4 represents a \\nschematic architecture of the method [13]: \\n \\n𝐷𝑅(𝑥) = {\\n𝑆(𝐶(𝑥)−𝔼𝑥𝑓∼𝑄𝐶(𝑥𝑓)) for real 𝑥\\n𝑆(𝐶(𝑥)−𝔼𝑥𝑟∼ℙ𝐶(𝑥𝑟)) for generated 𝑥\\n,  (4) \\n \\nwhere 𝑆 is the activation function (sigmoid), 𝑥 is data, 𝐷𝑅(𝑥) \\nis the discriminator, 𝐶(𝑥) is the output of the final layer \\nwithout activation function, 𝑥𝑓 stands for faked images, 𝑥𝑟 – \\nreal images, 𝔼 is an expectation. \\nE. MORPHEUS – DL TOOL FOR ANALYSIS OF \\nASTRONOMICAL IMAGES \\nSurveying galaxies is a major instrument of observational \\nastronomy. Ryan Hausen and Brant Robertson designed and \\ndescribed a deep learning framework for pixel-level analysis \\nof astronomical image data – Morpheus [15]. This model \\nhelps astronomers to automatically classify galaxies by their \\nshape or morphology. It implements deep learning methods \\nin order to perform diverse astronomical tasks such as source \\ndetection, segmentation, and morphological classification \\nwhich is carried out pixel -by-pixel through  a semantic \\nsegmentation approach – a modified version of the computer \\nvision algorithm.  \\nTechnically speaking, Morpheus is implemented as a \\nconvolutional neural network (Figure 3) similar to the U-Net \\nframework and designed through a combination of Python 3 \\nas the main tool and TensorFlow as a machine learning \\nlibrary. It is constructed from a series of so -called “blocks” \\nthat unify multiple reusable operations [15, 16]. \\n \\n \\nFigure 3. Convolutional neural network. \\nRepresented by the Morpheus team pixel -by-pixel \\nclassification technique of astronomical images can be \\nconsidered as an effective method of data analysis with wide \\napplicability provided that suitable training datasets are \\navailable. The framework showed promising results with \\ndifferent datasets. A s a performance assessment tool, \\nCANDELS HLF and 3D-HST data were used, and Morpheus \\ndemonstrated a strong capability for morphological \\nclassification and object detection. \\nF. GAN & DEVELOPING NEW MOLECULES \\nZhavoronkov et al. described the process of developing new \\nmolecules that lasted only 21 days using artificial \\nintelligence and GAN; the molecules have been successfully \\ntested in mice. Customized and proprietary data were used'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le (1).pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349  \\nVOLUME 20(3), 2021 343 \\nas training and test datasets. The desc ribed approach has \\nsuccessfully passed experimental validation. The cost of the \\nmethod is only a small fraction of the cost associated with \\nthe traditional approach to conventional drug discovery [17]. \\nThe newly developed deep generative model GENTRL \\n(generative tensorial reinforcement learning) showed \\nsuccessful results in terms of de novo small-molecule design; \\nit was applied as a system for discovering potent inhibitors \\nof discoidin domain receptor 1 (DDR1), molecules that are \\ninvolved in the regulation of cell functions and related to \\nfibrosis and other diseases. It is anticipated that this method \\ncan be improved further as a perspective approach to identify \\ndrug candidates. \\nIt was also reported that GANs are able to successfully \\ndesign novel molecules for different inflammation-, fibrosis-\\n, and cancer-inducing protein targets [18]. During this work, \\ntwo conditional GANs were stacked as one functional deep \\nlearning chain ( conditional GANs  and Wasserstein GAN  \\nwith gradient penalty (WGAN -GP)) in order to ac hieve \\nexperimental expectations: the second network improved the \\nresults of the first one (stage 2 and stage 1). The following \\nequations represent loss functions for the two stages \\nmentioned above (stage 1 and stage 2 respectively, equations \\n5–8) [18]: \\n \\n        \\nℒ𝐷0 = 𝔼𝑥∼𝑝real [−𝐷0(𝑥)]\\n+𝔼𝑧∼𝑝𝑧,𝑐∼𝑝real [𝐷0(𝐺0(𝑧,𝑐))]\\n+𝜆𝔼𝑥̂∼𝑝𝑥[(∥∥∇𝑥̂𝐷0(𝑥̂)2 −1∥∥)2]\\n,          (5) \\n \\n \\nℒ𝐺0 = 𝔼𝑧∼𝑝𝑧,𝑐∼𝑝real \\n[−𝐷0(𝐺0(𝑧,𝑐))−𝛼log\\u2061(𝑓0(𝐺0(𝑧,𝑐),𝑐))],      (6) \\n \\nwhere the generator is conditioned by a variable 𝑐  – 𝐺0(𝑧,𝑐), \\n𝐺 and 𝐷 are generator and discriminator respectively, 𝑧 is a \\nrandom noise vector sampled from (𝑝𝑧), 𝑥 is data (𝑥 and 𝑐 \\nalso represent a molecule representation and a gene \\nexpression sig nature), 𝑝real\\u2061 – real data distribution, 𝑓0 is a \\nfunction that represents a neural network and measures the \\nprobability of a gene expression signature, 𝜆 and 𝛼 are \\nregularization parameters; \\n \\n                 \\nℒ𝐷1 = 𝔼𝑥∼𝑝real [−𝐷1(𝑥)]\\n+𝔼𝑠0∼𝑝𝐺0,𝑐∼𝑝real [𝐷1(𝐺1(𝑠0,𝑐))]\\n+𝜆𝔼𝑥̂∼𝑝𝑥̂ [(∥∥∇𝑥̂𝐷1(𝑥̂)∥∥2 −1)\\n2\\n]\\n,           (7) \\n \\nℒ𝐺1 = 𝔼𝑠0∼𝑝𝐺0,𝑐∼𝑝real  \\n[−𝐷1(𝐺1(𝑠0,𝑐))−𝛼log\\u2061(𝑓1(𝐺1(𝑠0,𝑐),𝑐))], (8) \\n \\nwhere (𝐺1(𝑠0,𝑐)) is the generator, (𝐷1(𝑥)) is the \\ndiscriminator, 𝐺1 takes the output of 𝐺0(𝑠0 = 𝐺0(𝑧,𝑐)) and \\nthe gene expression signature (𝑐) as an input (instead of \\nrandom noise). \\nG. GANS IN DISCOVERING NEW MATERIALS & \\nPREDICTING CRYSTAL STRUCTURE \\nThe combined techniques of machine learning and GAN \\nhave confirmed their applied effectiveness in discovering \\nnew stable materials and predicting their crystal structure, \\nwhich was described by Schmidt et al. [19]. A similar \\nmethod called CrystalGAN was proposed by Asma Nouira et \\nal., which made it possible to identify cross -domain \\nconnections in real data and to create new crystal structures. \\nThe proposed approach has demonstrated that it can \\nefficiently integrate knowledge sets provided by human \\nexperts [20]. The GAN model showed its capability to \\ngenerate new solid crystallographic structures.  \\nCrystalGAN was abl e to successfully identify cross -\\ndomain connections in real data and generate novel \\nstructures. The model can be considered as the first GAN that \\nhas been explicitly designed to generate scientific data in \\nmaterials science. CrystalGAN showed promising res ults \\nand coped with the challenging task of discovering novel \\nmaterials for hydrogen storage. Diverse GAN architectures \\nare currently being studied in order to receive data of even \\nhigher complexity (compounds that consist of four or five \\nchemical elements). It is important to stress that CrystalGAN \\nas a general model that can be effectively adapted to any \\nscientific task [20]. \\nH. DEEP LEARNING FOR DEEPFAKES CREATION AND \\nDETECTION \\nDeepfakes pose a serious threat to a person’s personal safety \\nsince fabricated media data can be used to discredit a person \\nby damaging their reputation (fake pornography, fake news, \\nfraud, hoaxes, etc); they can provoke political instability, \\ntrigger violence, or even a war conflict.  \\nDeepfakes are  usually created with a special type of \\nneural network that is called an autoencoder that studies \\neffective information codings in an unsupervised way. Using \\nthe encoder-decoder chain the method allows replacing the \\nface of one person with another (in video or photograph; for \\ninstance, Reface and DeepNude applications).  \\nOne of the powerful tools that enhance the capabilities of \\ndeepfakes is a generative adversarial network (GAN). A \\nGAN trains a decoder (generator) and a discriminator in an \\nadversarial inte rplay that makes fabricated data difficult to \\nidentify since two networks are consecutively evolving. As \\nsoon as the deepfake is identified, the system will \\nimmediately correct the defect and its further detection will \\nbe difficult. Due to the increasing q uality of deepfake \\nsamples, detection methods also need to be improved. It has \\nbeen suggested to make a benchmark data set of deepfakes \\nthat will help in developing effective detection methods [21].  \\nThis should simplify the process of training detection \\nalgorithms that require a massive training set. To better \\nunderstand the possible methods of countering fakes, it is \\nhighly important to thoroughly study GANs. One of the \\nperspective methods suggests detecting deepfakes by \\nanalyzing convolutional traces [22 ]. The approach is based \\non examining digital “fingerprints” to discriminate generated \\nimages and distinguish them from real photographic data.'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le (1).pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349 \\n344 VOLUME 20(3), 2021 \\nI. GANS IN BIOMEDICAL INFORMATICS \\nDeep learning and GAN methods are actively used in \\nmedical imaging (X -ray radi ography, magnetic resonance \\nimaging, positron emission tomography, etc.) as a powerful \\ntool that allows health-care experts and radiologists to detect \\nserious medical conditions and diseases at early stages with \\na high percentage of accuracy.  \\nSince image data for many diseases are scarce, there is an \\nurgent need for additional sources of information for training \\nmodels. When there are not enough images for model \\ntraining, GANs can generate high -quality samples that can \\nbe successfully applied as a solution  in medical image \\nanalysis [23]. CycleGAN utilizes a cycle sequence loss to \\nensure model learning without paired data.  \\nThus, it can map from one domain (A in Figure 4) to \\nanother (B in Figure 4) without pairwise alignments between \\nthe source and target domain [24]. \\nWolterink et al. [25] described an application of Cycle -\\nGAN in the radiotherapy treatment planning to CT and MRI \\nimages of patients with brain tumors. \\n \\n \\nFigure 4. CycleGAN schematics. \\nResults demonstrate that Cycle -GAN outperforms a \\nconventional single GAN trained with paired images. Li et \\nal. [26] applied the GAN approach to predicting the \\npossibility of whether a patient has a rare disease or not; the \\nprediction accuracy was 5% higher  compared to standard \\nmethods. Recent studies also suggest that GANs can be \\nengaged in solving the problem of lack of data in \\nbioinformatics due to the capability of the network to \\ngenerate high-quality data samples [27].  \\nModels trained on small datasets demonstrate high bias, \\ntend to overfit, and produce inaccurate predictions in terms \\nof classification tasks.  \\nJ. GANS FOR CREATING FONT EXAMPLES \\nGAN can be also effective as a tool for creating new fonts \\nand unique hand -written symbols like digits and lett ers. A \\nstandard GAN method in a combination with ADAM \\noptimizer was tested on the MNIST dataset. The experiment \\nconducted by the authors showed the potential of the said \\napproach to generate high -quality graphic samples of hand -\\nwritten symbols. The overwhe lming majority of obtained \\nsymbols looked similar to real digits, they were \\ndistinguishable by a sufficient level of clarity, structure, \\nshape, and there was no significant graphic noise (Fig. 5) \\n[28].  \\n \\n \\nFigure 5. Samples generated by a GAN based on the \\nMNIST dataset. \\nTo obtain results of even higher quality the GAN \\nparameters can be experimentally readjusted. \\nIt should also be noted that a similar architecture can be \\nused in typography to generate new font models, as well as \\nin applied forensics to generate new training handwritten \\nsamples that can be used as part of datasets for training \\nhandwriting recognition and identification systems.  \\nMore examples can be reviewed and analyzed, but it is \\nbarely possible to list them all. The approaches reviewed \\nabove and summarized in Table 1 are practical confirmation \\nof the high efficiency of using deep learning and gene rative \\nadversarial neural networks in a wide range of applied areas \\nin the context of sample and image data processing. \\nAlternatively, other generative models can be considered \\nfor the similar range of tasks: \\n• Variational Autoencoders (VAE) have density \\nestimation, invertible, stable training, better \\ndiversity, but (a) the quality of the synthesized \\nsamples is much lower than GANs can produce and \\n(b) slow learning speed. \\n• Autoregressive Model – more diverse samples, but \\nlearning requires supervision. \\n• Flow Models – lower quality samples. \\n• Hybrid Models – less stable. \\nGANs are still the best option when realistic generation \\nis the main goal.'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le (1).pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349  \\nVOLUME 20(3), 2021 345 \\nTable 1. Peculiarities and possible modifications of the reviewed heuristics. \\nApplication Area Technology/Models Specificity Possible Modification \\nMethods \\n \\n \\n \\nObservational Astronomy \\nGAN and DL for recovering \\nfeatures in galaxy images \\nand photometric redshift \\nestimation; Fader network, \\nSpatial GAN (SGAN), \\nMorpheus. \\nAn important component of \\ncosmological research. \\nLimited capacity of the \\ntraining sets; domain know -\\nledge oriented; generated \\nimage samples depend on \\nthe training datasets. \\nApplying synthesized \\nsamples for enlarging \\nexisting data -sets; dropout \\nand random forest methods \\nto avoid overfitting; \\narchitecture optimization. \\n \\n \\nHealth Care/Longevity \\nDL and GAN; Generative \\nTensorial Reinforcement \\nLearning (GENTRL); \\nWasserstein GAN (WGAN). \\nA highly effective approach \\nwith promising results; \\nfurther applications of the \\nmethod and its limitations \\nare being actively studied. \\nPotentially more effective \\ncombinations of models re -\\nquire further research. \\n \\n \\nMaterials Science \\nML/DL; GAN; Crystal -\\nGAN. \\nA successful approach for \\nthe design of new materials. \\nCross-domain knowledge \\nbases in physics and chem-\\nistry are needed for further \\noptimization and designing \\neven more productive \\nmodels. \\n \\n \\nDeep Fake \\nStandard GANs, autoenco -\\nder network. \\nA high degree of influence \\non the social factor and pub-\\nlic safety. \\nFurther improvement of \\ndetection methods; creation \\nof a benchmark data set of \\ndeepfakes; convolutional \\ntrace identification appro-\\naches. \\n \\n \\n \\n \\nBioinformatics \\nDL and GAN; CycleGAN. The method maps one \\ndomain to another without \\npairwise alignments be t-\\nween the source and target \\ndo-main. The suggested \\napproach can help to solve \\nthe problem of lack of data \\nin bioinformatics. \\nOptimizing the architecture, \\nexamining the possibility of \\nadding Gaussian noise bet -\\nween loops and analyzing its \\nimpact on system perfor-\\nmance.  \\n \\n \\nTypography and \\nHandwritten Samples \\nGAN; Deep Convolution \\nGANs. \\nRequires high -quality data -\\nsets and sophisticated algo -\\nrithmic optimization me -\\nthods. \\nNetwork parameters re -\\nadjustment; normalizing a \\nnumber of training epochs; \\napplying additional combi -\\nnations of optimization \\ntechniques. \\n \\nIII. ADVANCED APPROACHES FOR IMPROVING DEEP \\nLEARNING AND GANS APPLIED EFFICIENCY \\nGiven the above descriptions, there are several possible ways \\nthat could help to improve the effectiveness of all these \\napproaches for various scientific fields. \\nA. IMPROVING PERFORMANCE USING DATA \\nAs for improving performance with data, it needs to be \\nstressed that the more training data collected, the better the \\nperformance since this directly affects the quality of the deep \\nlearning models used. The efficiency of any algorithm loses \\nits value if the amount of data is insufficient for full -fledged \\ntraining. \\nIf there is not enough data in the training set, it is \\nadvisable to consider artificial data generation as an option. \\nAs it was mentioned by Schawinski et al. [7], the main \\nconstraint of their approach was the limited capacity of the \\navailable training set. Fabricated images generated by GANs \\nmay be considered as a possible solut ion for this issue. In \\nother words, special GAN architectures can assist other \\nGANs to improve their performance and results. When it \\ncomes to image data, we can either synthesize new images \\nor randomly modify samples of existing images; we can also \\nuse random rotation or shifting images or adding simulated \\nnoise. The principle of data augmentation applies to other \\ndata types as well (vectors of numbers, text, etc). \\nLack of training data can result in overfitting, thus the \\nbest way to avoid it is to provide a deep neural network with \\nincreased quantity of quality training data. Generative \\nmodeling with GANs can fill the gap by reinforcing smaller \\ndatasets with new synthesized high-quality images. The deep \\nconvolutional GAN architecture (DCGAN) is capable of \\ncreating photorealistic graphical samples that accurately'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le (1).pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349 \\n346 VOLUME 20(3), 2021 \\ncorrespond to the feature distributions of real galaxies in \\nterms of statistical estimation [29]. It is worth noticing, \\nthough, that these are limited by the accuracy of the \\nprobability density estimates. \\nLevi Fussell and Ben Moews experimentally proved that \\nStackGAN can be applied as a second-stage architecture and \\nform a combination system with DCGAN in order to \\nsynthesize fabricated galaxy images with higher resolutions, \\navoiding the obstructions that DCGAN models experience \\nwith such resolutions [30, 31].  \\nData rescaling in the context of the applied activation \\nfunctions also plays an important role. It’s sometimes useful \\nto normalize data values and rescale them; between 0 and 1 \\nif it comes to si gmoid activation functions, between 0 and \\ninfinity if it’s rectified linear unit (ReLU), -1 and 1 if it’s \\nhyperbolic tangent (tanh), for instance. This rescaling \\nprinciple can be applied to other activation functions as well. \\nB. ALGORITHMIC APPROACH \\n \\nFigure 6. Structure of interactions between Generator and \\nDiscriminator networks in GAN. \\nAs for the algorithmic approach, if GANs mainly use the \\noriginal algorithm designed by Goodfellow et al. (Figure 6) \\nand its modifications, other deep learning algorithms u se \\ndiverse models and approaches that vary from case to case \\n[9]. \\nEquation 9 represents a mathematical description of \\nGAN as a variation of the minimax two-player game [9]: \\n \\n𝑚𝑖𝑛\\u2061\\n𝐺\\n\\u200a𝑚𝑎𝑥\\n𝐷\\n\\u200a𝑉(𝐷,𝐺) = 𝔼𝒙∼𝑝data (𝒙)[log\\u2061𝐷(𝒙)]+\\n\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061𝔼𝒛∼𝑝𝒛(𝒛)[log\\u2061(1−𝐷(𝐺(𝒛)))],              (9) \\n \\nwhere 𝐺 is the generator network; 𝐷 is the discriminator \\nnetwork; 𝑥 is a sample of real data; 𝑝𝑧(𝑧) – is a prior on input \\nnoise variables; 𝑧 is noise;  𝐷(𝑥) is the probabil ity that 𝑥 \\nactually is from the data rather than from the generator; 𝐺(𝑧) \\nis the generator output; 𝔼 is prediction (expectation), the first \\npart represents the discriminator ’s predictions on the real \\ndata, the second one – the discriminator’s predictions on the \\nfake/generated data; 𝑉(𝐷,𝐺) is the value function of \\ndiscriminator and generator in the two -player minimax \\ngame. \\nEach time proper algorithmic diagnostics should be \\napplied to the desired deep learning model. All examples \\nreviewed in the paper ca n be experimentally modified in \\nterms of weight configuration, network topology, types of \\nactivation functions, learning rate, batches, and number of \\nepochs. The network topology depends on the task at hand. \\nSince at the moment it is generally accepted that there are no \\nunified rules regarding how many layers or how many \\nneurons are needed for a particular configuration of a neural \\nnetwork, these parameters are selected experimentally. \\nC. HYBRID METHODS AND OPTIMIZATION \\nThe new technique proposed by Karras et al. allows training \\ngenerative adversarial networks with limited data. They have \\nsuggested methods of adaptive discriminator augmentation \\nin GANs and described the following overfitting heuristics \\n[32]: \\n \\n𝑟𝑣 =\\n𝔼[𝐷train ]−𝔼[𝐷validation ]\\n𝔼[𝐷train ]−𝔼[𝐷generated ] \\u2061\\u2061\\u2061\\u2061\\u2061\\u2061𝑟𝑡 = 𝔼[sign(𝐷train )], (10) \\n \\nwhere the discriminator outputs are denoted by 𝐷𝑡𝑟𝑎𝑖𝑛 for the \\ntraining set, 𝐷𝑣𝑎𝑙𝑖𝑑𝑎𝑡𝑖𝑜𝑛 for the validation set, and 𝐷𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑒𝑑 \\nfor the generated images,  𝔼 denotes their mean over 𝑁 \\nconsecutive minibatches. During the experiment, 𝑁 was \\nequal to 4, which corresponds to 4 × 64 = 256 images. For \\nboth parts of heuristics, 𝑟\\u2061 = \\u20610 stands for no overfitting and \\n𝑟\\u2061 = \\u20611 implies complete overfitting. The goal of the \\nexperiment was to adjust the augmentation probability 𝑝 so \\nthat the selected heuristic fits an appropriate target value. \\nTwo heuristic units:  𝑟𝑣 signifies the output for a validation \\nset relative to the training set and generated images, 𝑟𝑡 \\nevaluates the fraction of the training set that gets positive \\ndiscriminator outputs [32]. Additional augmentation and \\nregularization approaches in the GAN context have been \\nsuggested by Cubuk et al. (September 2019) and Zhang et al. \\n(February 2020) [33, 34]. \\nRecent research shows that a combination of multiple \\nGANs can create generated data with higher quality \\ncompared to a conventional single GAN [32]. It should also \\nbe noted that sometimes it happens that due to oversimplified \\nloss function GANs do not learn th e way they are expected \\nto (mode collapse, vanishing gradients, convergence). This \\nproblem remains one of the active areas of research at the \\nmoment. \\nSophisticated regularization methods, such as dropout \\n(equation 11), can also help to avoid overfitting in neural \\nnetworks [35]: \\n \\n                      ŵ𝑗 = {w𝑗,     with 𝑃(𝑐)\\n0,     otherwise ,              (11) \\n \\nwhere P(c) is the probability of c (stands for “keeping a \\nweight” factor) to keep a row in the weight matrix, w𝑗 is a \\nreal row in the weight matrix before dropout, ŵ𝑗 is a diluted \\nrow in the weight matrix. Dropout randomly omits (or “drops \\nout”) neurons of a neural network (both hidden and visible) \\nduring the training process. It should be not ed that zeroing \\nout the node does not impact the end result. \\nK-fold cross -validation is another method that can be \\napplied to a model in order to avoid overfitting. Data is'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le (1).pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349  \\nVOLUME 20(3), 2021 347 \\nseparated into K randomly -assigned fragments where one \\nfragment is earmarked as test data. The remaining combined \\nK-1 (minus one) fragments are used for training and after \\nthat, results need to be evaluated with the test set. This cycle \\nthen reiterates for each fragment and the average of the K r -\\nsquared scores is calculated, or the results displayed in a box \\nplot to give the median value and identify outliers. \\nAnother possible way to improve the results obtained \\nusing deep learning methods is algorithmic model rotation. \\nTo solve the same problem, completely different types of \\nneural netw orks and methods of their modification can be \\napplied. Restarts indicate the impact of local minima and \\nsaddle points in the loss function. \\nExperimenting with linear and non -linear methods \\n(logistic regression, polynomial regression, and multiple \\nregression) along with tree methods like gradient boosting, \\nclassification and regression trees, and random forest \\ndecreases variance and could give different results in terms \\nof productivity. For classification and regression analysis in \\nneural networks, support -vector networks and k -nearest \\nneighbors algorithms also demonstrate high-level efficiency. \\nSometimes, to obtain more efficient results, it makes sense \\nto resort to the hybridization of models, combining the \\nfollowing approaches with each other: learning vec tor \\nquantization, Boltzmann machines, multilayer perceptron, \\nconvolutional neural network, long short -term memory \\narchitecture, competitive networks such as GANs, \\nautoencoder networks, deep stacking networks. The list of \\narchitectures given is only illustrative since the number of all \\npossible combinations increases exponentially. \\nPerformance can be also improved by involving \\nalgorithms for gradient -based optimization such as \\nRMSProp, AdaGrad, Momentum, Adagrad, Adadelta, and \\nADAM. Upon experimental evaluat ion, ADAM \\ndemonstrated strong results with logistic regression, multi -\\nlayer neural networks, convolutional neural networks, and \\nperformed equal or better than RMSProp , regardless of \\nhyper-parameter settings [36]. Using ADAM as an example, \\nand considering it takes its name from “adaptive moment \\nestimation,” we can see that this method utilizes evaluations \\nof first and second moments of gradient adjusting the \\nlearning ra te for each weight parameter of the neural \\nnetwork. The moment is a numerical characteristic of the \\ndistribution of a given random variable (expected value of \\nthe variable to the n-th power, equation 12): \\n \\n                              𝑀𝑛 = 𝐸[𝑉𝑟𝑎𝑛.\\n𝑛],                            (12) \\n \\nwhere 𝑀 is the moment, 𝐸 is the expected value of the \\nvariable, 𝑉𝑟𝑎𝑛. is a random variable. \\nDeep learning optimization models are still an open \\ndomain and still require in -depth research, both in terms of \\nmathematical groundings and in terms of software and \\nhardware implementation. \\nIV. CONCLUSIONS \\nSince the amount of information is constantly increasing, \\nscientists need modern and efficient tools to examine and \\nanalyze the data they receive. Nowadays researchers have \\nunprecedented access to advanced AI tools for gathering, \\nretrieving, processing, and recovering images and statistical \\ndata [37, 38, 39]. It is certain that machine learning can \\nprocess and analyze information much faster than humans or \\nother computational methods, furthermore, it can \\ncomprehend data patterns and liaisons that we do not even \\nrecognize, e.g., it may detect diverse types of galaxies before \\nscientists know they exist.  \\nIn the article, the authors analyze the types of modern \\nGAN architectures an d existing approaches to their design, \\nas well as the main advantages and prospects for widespread \\nimplementation of GAN and deep learning (DL) for solving \\ntopical problems of artificial intelligence. The above analysis \\nof successful cases illustrates and confirms the high \\nefficiency of GANs and DL in astronomy, molecular \\nbiology, materials science, bioinformatics, handwriting \\nrecognition, and deepfake detection. The authors provide an \\nanalysis and offer proposals for the development of \\nadvanced approaches in terms of design and implementation \\nof GANs and DL. In the future, the authors plan to develop \\nsoftware for the implementation of GAN and DL based on \\nthe discussed advanced approaches. \\nProcessing instrumental images using artificial neural \\nnetworks can a ccelerate further research and help in \\nreconstructing imaging data even for nonstandard and \\nunstudied phenomena. GANs can effectively remove noise \\nand provide as clear of an image as possible due to their \\nability to recover graphical data that have damaged  or \\nmissing pixels, or unwanted instrumental artifacts. Thus, the \\nmachine learning methods and GANs should be considered \\nas the most promising assistive technologies for science as a \\nwhole. \\nGAN is one of the relatively new DL technologies \\n(2014), requires a thorough research and analysis, \\nsignificantly affects various aspects of scientific and \\ntechnological development (creation of new drugs, space \\nexploration) and socio -political life including Deep Fake \\nproblem (photo, video, audio). \\nAs for drawbacks and limitations, it is worth noticing that \\nin order to train a machine learning system we need a lot of \\nlabeled and preprocessed information. Moreover, until \\nrecently, the scientific community simply had no data about \\nsome substantial aspects that are  important for preparing \\neffective training sets. In addition, neural networks are being \\nconsidered as a kind of black box: researchers do not always \\nunderstand exactly how artificial neural networks operate, \\nespecially when it comes to complex architectur es with \\nmany hidden layers of neurons. Using tools without a proper \\nlevel of understanding of how they work is a matter of \\nconcern among scientists. \\nNevertheless, DL systems continue contributing to \\nprogress across a range of different scientific fields [40, 41], \\nand therefore, the prospects for the further use of machine \\nlearning in applied scientific research, as well as methods for \\nits improving and optimizing, should continue to be'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le (1).pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349 \\n348 VOLUME 20(3), 2021 \\ncomprehensively studied. Proper technological unification \\nand combination of research efforts can lead to revolutionary \\nresults. \\nThe main task of the article is an in -depth scientific \\nanalysis of current practical approaches in terms of \\napplication of deep learning methods, including generative \\nmodels, in the most important sp heres of human activity, \\nbecause today DL and GANs still need thorough study and \\nresearch: theoretical basis, areas and limits of application, \\nsecurity and safety. Awareness of the scientific community \\nin modern methods of artificial intelligence and the need for \\naccess to pervasive analytical materials are among the key \\naspects influencing the speed, intensity, and novelty of \\nresearch. The article analyzes innovative approaches in DL \\nand GANs, provides an integral assessment of their \\neffectiveness, and off ers practical and theoretical \\nsuggestions for improvement. \\nIt is expected that modern deep learning technologies will \\nmake a significant contribution to science and the \\ndevelopment of research methodology, provided that proper \\nconvergence between the vast majority of scientific fields is \\nachieved. \\nReferences \\n[1] S. Russell, P. Norvig, Artificial Intelligence: A Modern Approach, 3rd \\ned., Upper Saddle River, New Jersey: Prentice Hall, 2009, 1 p. \\n[2] P. Barmby, Astronomical observations: a guide for allied researchers, \\n2019, [Online]. Available at: https://arxiv.org/abs/1812.07963, \\nhttps://doi.org/10.21105/astro.1812.07963. \\n[3] E. C. Sutton, Observational Astronomy. Techniques and \\nInstrumentation, Cambridge University Press, 2011, 1 p. \\n[4] J. Lee, P. L. Freddolino, Y. Zhang, Ab initio protein structure \\nprediction, D.J. Rigden (ed.), From Protein Structure to Function with \\nBioinformatics, 2017, pp. 1 –33. https://doi.org/10.1007/978-94-024-\\n1069-3_1. \\n[5] DeepMind, “AlphaFold: Using AI for scientific discovery,” Nature, \\nvol. 577, pp. 706–710, 2020. \\n[6] S. Roberts, A. McQuillan, S. Reece, S. Aigrain, “Astrophysically \\nrobust systematics removal using variational inference: application to \\nthe first month of Kepler data, ” Monthly Notices of the Royal \\nAstronomical Society , vol. 435, pp. 36 39–3653, 2013.  \\nhttps://doi.org/10.1093/mnras/stt1555. \\n[7] K. Schawinski, C. Zhang, H. Zhang, L. Fowler, G. K. Santhanam, \\n“Generative adversarial networks recover features in astrophysical \\nimages of galaxies beyond the deconvolution limit,” Monthly Notices \\nof the Royal Astronomical Society: Letters, vol. 467, issue 1, pp. 110–\\n114, 2017. https://doi.org/10.1093/mnrasl/slx008. \\n[8] P. Magain, F. Courbin, S. Sohy, “Deconvolution with Correct \\nSampling,” The Astrophysical Journal , pp. 472 –477, 1998.  \\nhttps://doi.org/10.1086/305187. \\n[9] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, \\nS. Ozair, A. Courville, J. Bengio, “Generative Adversarial Networks,” \\nProceedings of the International Conference on Neural Informatio n \\nProcessing Systems (NIPS), 2014, pp. 2672–2680. \\n[10] A. D’Isanto, K. L. Polsterer, “Photometric redshift estimation via deep \\nlearning. Generalized and pre -classification-less, image based, fully \\nprobabilistic redshifts,” Astronomy & Astrophysics , vol. 609, A1 11, \\npp. 1–16, 2018. https://doi.org/10.1051/0004-6361/201731326. \\n[11] G. Lample, N. Zeghidour, N. Usunier, A. Bordes, L. Denoyer, M. \\nRanzato, “Fader networks: Manipulating images by sliding attributes,” \\nProceedings of 31st Conference on Neural Information Processing \\nSystems, USA, 2017, pp. 1–10.  \\n[12] K. Schawinski, M. D. Turp, C. Zhang, “Exploring galaxy evolution \\nwith generative models,” Astronomy & Astrophysics , vol. 616, L16, \\npp. 1–4, 2018. https://doi.org/10.1051/0004-6361/201833800. \\n[13] M. J. Smith, J. E. Geach, “Generative deep fields: arbitrarily sized, \\nrandom synthetic astronomical images through deep learning,” \\nMonthly Notices of the Royal Astronomical Society, vol. 490, issue 4, \\npp. 4985–4990, 2019. https://doi.org/10.1093/mnras/stz2886. \\n[14] N. Jetchev, U. Bergmann, R. Vollgraf, “Texture Synthesis with Spatial \\nGenerative Adversarial Networks,” Proceedings of Workshop on \\nAdversarial Training, NIPS’2016, Barcelona, Spain, 2016, pp. 1–11.  \\n[15] R. Hausen, B. Robertson, Morpheus: A Deep Learning Framework for \\nPixel-Level Analysis of Astronomical Image Data, 2019, [Onli ne]. \\nAvailable at: https://arxiv.org/abs/1906.11248 \\n[16] O. Ronneberger, P. Fischer, T. Brox, “U-Net: Convolutional networks \\nfor biomedical image segmentation ,” Proceedings of the 18th \\nInternational Conference Medical Image Computing and Computer -\\nAssisted Intervention, Munich, Germany, October 5-9, 2015, pp. 1–8. \\n[17] A. Zhavoronkov, Y. A. Ivanenkov, A. Aliper et al., “Deep learning \\nenables rapid identification of potent DDR1 kinase inhibitors,” Nature \\nBiotechnology, vol. 37, pp. 1038 –1040, 2019.  \\nhttps://doi.org/10.1038/s41587-019-0224-x. \\n[18] O. Méndez-Lucio, B. Baillif, D.-A. Clevert, D. Rouquié, J. Wichard , \\nDe novo generation of hit -like molecules from gene expression \\nsignatures using artificial intelligence, 2020. \\nhttps://doi.org/10.1038/s41467-019-13807-w. \\n[19] J. Schmidt, M. R. G. Marques, S. Botti et al., Recent advances and \\napplications of machine learning in solid-state materials science, 2019. \\nhttps://doi.org/10.1038/s41524-019-0221-0. \\n[20] A. Nouira, N. Sokolovska, J. -C. Crivello, “CrystalGAN: Learning to \\ndiscover crystallographic structures with generative adversarial \\nnetworks,” Proceedings of the AAAI Spring Symposium: Combining \\nMachine Learning with Knowledge Engineering, Stanford University, \\nUSA, March 25-27, 2019, pp. 1–9. \\n[21] T. T. Nguyen, C. M. Nguyen, D. T. Nguyen, D. T. Nguyen, S. \\nNahavandi, Deep Learning for Deepfakes Creation and Detection: A \\nSurvey, 2020, [Online]. Available at: https://arxiv.org/abs/1909.11573 \\n[22] L. Guarnera, O. Giudice, S. Battiato, “DeepFake detection b y \\nanalyzing convolutional traces,” Proceedings of the IEEE Conference \\non Computer Vision and Pattern Recognition Workshops , 2020, pp. \\n1–10. https://doi.org/10.1109/CVPRW50498.2020.00341. \\n[23] L. Lan, L. You, Z. Zhang, Z. Fan, W. Zhao, N. Zeng, Y. Chen, X. \\nZhou, Generative Adversarial Networks and its Applications in \\nBiomedical Informatics, 2020 . \\nhttps://doi.org/10.3389/fpubh.2020.00164. \\n[24] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, Unpaired Image-to-Image \\nTranslation using Cycle -Consistent Adversarial Networks , 2020, \\n[Online]. Available at: https://arxiv.org/abs/1703.10593 \\n[25] J. M. Wolterink et al., Deep MR to CT Synthesis Using Unpaired Data, \\nin Tsaftaris S., Gooya A., Frangi A., Prince J. (eds) Simulation and \\nSynthesis in Medical Imaging., SASHIMI 2017, Lecture Notes in \\nComputer Science, 2017, vol. 10557, Springer, Cham, pp. 14–23. \\n[26] W. Li, Y . Wang, Y. Cai, C. Arnold, E. Zhao, Y. Yuan, Semi -\\nsupervised Rare Disease Detection Using Generative Adversarial \\nNetwork, 2018, [Online]. Available at: \\nhttps://arxiv.org/abs/1812.00547 \\n[27] M. Marouf, P. Machart, V. Bansal, C. Kilian, D. S. Magruder, C. F. \\nKrebs, S. Bonn, Realistic in silico generation and augmentation of \\nsingle-cell RNA-seq data using generative adversarial networks, 2020. \\nhttps://doi.org/10.1038/s41467-019-14018-z. \\n[28] O. Striu k, Y. Kondratenko, I. Sidenko, A. Vorobyova, “Generative \\nadversarial neural network for creating photorealistic images ,” \\nProceedings of the 2020 IEEE 2nd International Conference on \\nAdvanced Trends in Information Theory , Kyiv, Ukraine, November \\n27, 2020, pp. 1–4.  \\n[29] M. C. Chan, J. P. Stott, “Deep-CEE I: Fishing for galaxy clusters with \\ndeep neural nets,” Monthly Notices of the Royal Astronomical Society, \\nvol. 490, pp. 5770 –5787, 2019.  \\nhttps://doi.org/10.1093/mnras/stz2936. \\n[30] Z. L. Wen, J. L. Han, F. S. Liu, “A Catalog of 132,684 clusters of \\ngalaxies identified from sloan digital sky survey III,” The \\nAstrophysical Journal Supplement, vol. 199, issue 2, article id. 34, pp. \\n1–12, 2012. https://doi.org/10.1088/0067-0049/199/2/34. \\n[31] L. Fussell, B. Moews, “Forging new worlds: high-resolution synthetic \\ngalaxies with chained generative adversarial networks,” Monthly'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le (1).pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349  \\nVOLUME 20(3), 2021 349 \\nNotices of the Royal Astronomical Society, vol. 485, issue 3, pp. 3203–\\n3214, 2019. https://doi.org/10.1093/mnras/stz602. \\n[32] T. Karras, M. Aittala, J. Hellsten, S. Laine, J. Lehtinen, T. Aila, \\n“Training generative adversarial networks with limited data ,” \\nProceedings of the 34th Conference on Neural Information \\nProcessing Systems (NeurIPS 2020), Vancouver, Canada, June 2020, \\npp. 1–37. \\n[33] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, D. Metaxas, \\n“StackGAN: Text to photo-realistic image synthesis with stacked \\ngenerative adversarial networks ,” Proceedings of the 2017 IEEE \\nInternational Conference on Computer Vision (ICCV) , Venice, Italy, \\n2017, pp. 5908-5916. https://doi.org/10.1109/ICCV.2017.629. \\n[34] E. D. Cubuk, B. Zoph, J. Shlens, Q. V. Le, RandAugment : Practical \\nautomated data augmentation with a reduced search space, 2019, \\n[Online]. Available at: https://arxiv.org/abs/1909.13719. \\nhttps://doi.org/10.1109/CVPRW50498.2020.00359. \\n[35] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. \\nSalakhutdinov, “Dropout: A Simple Way to Prevent Neural Networks \\nfrom Overfitting,” Journal of Machine Learning Research, vol. 15, pp. \\n1929–1958, 2014. \\n[36] D. P. Kingma, J. L. Ba, “Adam: A method for stochastic \\noptimization,” Proceedings of the 3rd International Conference for \\nLearning Representations, San Diego, USA, 2015, pp. 1–15. \\n[37] R. Leizerovych, G. Kondratenko, I. Sidenko, Y. Kondratenko, “IoT -\\ncomplex fo r monitoring and analysis of motor highway condition \\nusing artificial neural networks,” Proceedings of the 2020 IEEE 11th \\nInternational Conference on Dependable Systems, Services and \\nTechnologies, DESSERT 2020 , Kyiv; Ukraine; 14 -18 May, 2020; \\nArticle No. 9 125004, pp. 207 –212. \\nhttps://doi.org/10.1109/DESSERT50317.2020.9125004. \\n[38] K. Ivanova, G. Kondratenko, I. Sidenko, Y. Kondratenko, “Artificial \\nintelligence in automated system for w eb-interfaces visual testing,” \\nCEUR Workshop Proceedings , vol. 2604, 2020, 4th International \\nConference on Computational Linguistics and Intelligent Systems, \\nCOLINS 2020; Lviv; Ukraine; 2020; pp. 1019–1031. \\n[39] V. M. Kuntsevich et al. (Eds), Control Systems: Theory and \\nApplications. Series in Automation, Control and Robotics , River \\nPublishers, 2018, 146 p. \\n[40] Y. Kondratenko, D. Simon, Structural and parametric optimization of \\nfuzzy control and decision making systems, In: Zadeh L., Yager R., \\nShahbazova S ., Reformat M., Kreinovich V. (eds), Recent \\nDevelopments and the New Direction in Soft-Computing Foundations \\nand Applications. Studies in Fuzziness and Soft Computing, Springer, \\nCham., vol. 361, 2018, pp. 273–289. \\n[41] Z. Gomolka, E. Dudek -Dyduch, Y. P. Kondrat enko, “From \\nhomogeneous network to neural nets with fractional derivative \\nmechanism,” Proceedings of the International Conference on \\nArtificial Intelligence and Soft Computing, ICAISC-2017, Rutkowski, \\nL. et al. (Eds), Part I, Zakopane, Poland, 11 -15 June, 2017, LNAI \\n10245, Springer, Cham, 2017, pp. 52–63. https://doi.org/10.1007/978-\\n3-319-59063-9_5. \\n[42] J. Brownlee. 18 Impressive Applications of Generative Adversarial \\nNetworks (GANs), 2019,  [Online]. Available at: \\nhttps://machinelearningmastery.com/impressive-applications-of-\\ngenerative-adversarial-networks/ \\n \\n \\nOLEKSANDR STRIUK , Ph.D. student and \\nresearcher at Petro Mohyla  Black Sea National \\nUniversity (PMBSNU). Master of Science in \\nSystem Analysis. Research interests include AGI, \\nartificial neural networks, reasoning in AI, \\nstatistics, and probability theory. \\n \\n \\nYURIY KONDRATENKO , Doctor of Science, \\nProfessor, Honour Inventor of Ukraine (2008), \\nCorr. Academician of Royal Academy of Doctors \\n(Barce-lona, Spain), Head of the Department of \\nIntelligent  Information Systems at  Petro Mohyla \\nBlack Sea National University (PMBSNU), \\nUkraine. He has received (a) the Ph.D. (1983) \\nand Dr.Sc. (1994) in Elements and Devices of \\nComputer and Control Systems from Odessa National Polytechnic \\nUniversity, (b) several international grants and scholarships for \\nconducting research at Institute of Automation of Chongqing \\nUniversity, P.R.China (1988 -1989), Ruhr -University Bochum, \\nGermany (2000, 2010), Nazareth College and Cleveland State \\nUniversity, USA (2003), (c) Fulbright Scholarship for researching in \\nUSA (2015/2016) at the Dept. of Electrical Engineering and \\nComputer Science in Cleveland State University. Research interests \\ninclude robotics, automation, sensors and control systems, intelligent \\ndecision support systems, fuzzy logic. \\n \\n \\n \\nView publication stats'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le.pdf'}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/355044684\\nGenerative Adversarial Neural Networks and Deep Learning: Successful\\nCases and Advanced Approaches\\nArticle\\xa0\\xa0in \\xa0\\xa0International Journal of Computing · September 2021\\nDOI: 10.47839/ijc.20.3.2278\\nCITATIONS\\n24\\nREADS\\n1,362\\n2 authors, including:\\nYuriy Kondratenko\\nPetro Mohyla Black Sea National University\\n220 PUBLICATIONS\\xa0\\xa0\\xa02,043 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Yuriy Kondratenko on 01 March 2022.\\nThe user has requested enhancement of the downloaded file.'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le.pdf'}, page_content='VOLUME 20(3), 2021 339 \\nDate of publication SEP-30, 2021, date of current version JUL-28, 2021. \\nwww.computingonline.net / computing@computingonline.net \\nPrint ISSN 1727-6209 \\nOnline ISSN 2312-5381 \\nDOI 10.47839/ijc.20.3.2278 \\nGenerative Adversarial Neural Networks \\nand Deep Learning: Successful Cases \\nand Advanced Approaches \\nOLEKSANDR STRIUK, YURIY KONDRATENKO \\nPetro Mohyla Black Sea National University, 10 68th Desantnykiv st., Mykolaiv, 54003, Ukraine  \\n(e-mail: oleksandr.striuk@gmail.com, y_kondrat2002@yahoo. com) \\nCorresponding author: Oleksandr Striuk (e-mail: oleksandr.striuk@gmail.com). \\n \\n ABSTRACT Cross-domain artificial intelligence (AI) frameworks are the keys to amplify progress in science. \\nCutting edge deep learning methods offer novel opportunities for retrieving, optimizing, and improving different \\ndata types. AI techniques provide new ways for enhancing and polishing existing models that are used in applied \\nsciences. New breakthroughs in generative adversarial neural networks (GANNs/G ANs) and deep learning allow \\nto drastically increase the quality of diverse graphic samples obtained with research equipment. All these \\ninnovative approaches can be compounded into a unified academic and technological pipeline that can radically \\nelevate and accelerate scientific research and development. The authors analyze a number of successful cases of \\nGAN and deep learning applications in applied scientific fields (including observational astronomy, health care, \\nmaterials science, deep fakes, bioinforma tics, and typography) and discuss advanced approaches for increasing \\nGAN and DL efficiency in terms of performance calibration using modified data samples, algorithmic \\nenhancements, and various hybrid methods of optimization. \\n \\n KEYWORDS generative adversa rial network; neural networks; deep learning; machine learning; artificial \\nintelligence. \\n \\nI. INTRODUCTION \\nURRENT artificial intelligence technologies such as \\ndeep learning (DL) and artificial neural networks – AI \\nsystems inspired by the structure and principles of the human \\nbrain – become true amplifiers of scientific discovery and \\ndevelopment. AI helps to speed up experimental simulations, \\ngather and process new data, prove brand new theoretical \\nhypotheses in many scientific fields. AI is literally relevant \\nto any intellectual task [1]. \\nDeep learning is one of the machine learning methods \\nthat is grounded on an artificial neural networks framework \\nthat can be trained based on supervised and unsupervised \\nlearning algorithms. Deep learning architectures are \\neffectively used in different fields including autonomous \\nvehicles, computer vision, natural language processing, \\nrecommendation services, bioinformatics, medical image \\nanalysis, and generation of new functio nal samples, where \\nthey have shown similar to human experts ’ results or even \\noutperformed them. The main concept of artificial neural \\nnetworks was inspired by real biological systems. Generative \\nadversarial network (GAN) is the implementation of a \\ndeep/machine unsupervised learning algorithm class that \\nrepresents the architecture of two artificial neural networks \\nthat compete with each other in a zero-sum game. \\nToday’s AI methods show incredibly successful practical \\nresults at doing science [2 -6]. AI system s are used as an \\neffective mechanism in diverse scientific fields transforming \\nconventional research practices and expediting discoveries. \\nThe main advantage of AI is that it can outperform humans \\nwhen it comes to processing large amounts of data, detecting \\npatterns and abnormalities that human experts could never \\nhave spotted.  \\nFig. 1 demonstrates the integrated liaisons between key \\nelements of AI. \\nC'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le.pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349 \\n340 VOLUME 20(3), 2021 \\n \\nFigure 1. Interconnections and nesting of artificial \\nintelligence systems. \\nThe driving force that triggered an increased interest in \\nthe more intense integration of AI into science were massive \\narrays of data accumulated over many years of research and \\nthe development of high -performance computing platforms \\nthat were able to process and analyze these data sets. In \\nrecent years, artificial intelligence systems have made a great \\ncontribution to the intensification of scientific research. \\nII. SCRUTINIZING HEURISTICS OF SUCCESSFUL \\nCASES \\nThis paper is an overview analysis and considers the \\npractical aspects of the use of machine learning and GAN in \\nthe applied fields of science . In particular, observational \\nastronomy, health care, materials science and deep fake \\ndetection have been selected as illustrative examples.  \\nCases were selected from the most important areas that \\naffect the core scientific research and quality of human life. \\nThe next sections of the paper analyze the fragmented \\ndata of efficient research in this area and assess future \\nprospects. \\nA. GAN METHOD RECOVERS FEATURES IN \\nASTROPHYSICAL IMAGES OF GALAXIES \\nSchawinski et al. demonstrated a machine learning method \\nthat was  able to successfully recover elements in \\nastronomical images of galaxies [7]. The said ML \\nmethodology allows to overcome the deconvolution limit \\nusing higher quality training data sets and makes it possible \\nto reconstruct information from poor quality sam ples by \\nsuccessfully building priors [7].  \\nThe Nyquist–Shannon sampling theorem sets limitations \\nin terms of removing the effect of  the point spread function \\nespecially when there is noise, sequentially sampled material \\ncannot be completely deconvolved without violating the \\ntheorem postulates [7, 8]. As a workaround for this issue , \\nSchawinski et al. applied a generative adversarial neural \\nnetwork (GANN/GAN).  \\nA GAN is a state-of-the-art deep learning algorithm that \\nallows two neural networks to contest with each other in the \\nform of a zero -sum game. This framework can create \\nrealistic artificial graphical samples similar and almost \\nidentical to the images from a training set [9]. \\nGAN works on the following principle: the first network, \\nthe generator, creates  samples (candidates), and the second \\nnetwork, the discriminator, evaluates them, trying to \\ndistinguish real from fakes. The generative network tries to \\nform a new sample by combining primary samples using \\nlatent space variables. The discriminator network learns to \\ndistinguish between real and counterfeit samples. \\nConventional deep learning models are used in GANs as \\ncomponents. For example, the discriminator can be \\nimplemented as a convolutional classifier network. \\nThe suggested method can drastically impr ove the \\nquality of obtained image samples of galaxies by recovering \\nits properties and bypassing the deconvolution constraints \\nmentioned above.  \\nDuring the experiment the GAN has been trained on a \\ndata set that included 4,550 graphic samples of nearby \\ngalaxies in the redshift range 0.01 < z < 0.02 that were taken \\nfrom the Sloan Digital Sky Survey. The results were assessed \\nthrough ten cross-validation iterative cycles. GAN was able \\nto restore artificially corrupted image samples with bad \\nseeing and high noise levels (compared to the original image) \\nand showed results that greatly outperform standard \\ndeconvolution.  \\nThe results proved the effectiveness of the method in \\nrestoring important characteristics of celestial phenomena \\nand in expanding the range of study of existing astronomical \\ndata gathered by telescopes [7]. The images clearly show the \\nability of GAN to restore features that cannot be recovered \\nusing conventional deconvolution techniques. \\nFig. 2 is a graphic illustration of the training process of \\nthe method described by Schawinski et al. A set of original \\nimages is the input. Image degradation is achieved by \\nblurring, adding noise, and through convolution with a worse \\npoint spread function. Degraded images are automatically \\ngenerated and used for GA N training. Only the generator is \\nused for recovering images during the testing phase. \\nHowever, this technique is not without drawbacks and \\nhas its limitations. The main constraint is related to the \\nlimited capacity of the training set that drastically imp acts \\nthe restorative capabilities of the method. Small training data \\nresults in a bad approximation. A model that is trained on \\nsuch poor data will likely demonstrate low performance due \\nto overfitting.  As a possible solution, the training sets that \\nconsist of synthesized simulation images can be considered \\nas an additional reinforcement technique in terms of \\nlearning.'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le.pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349  \\nVOLUME 20(3), 2021 341 \\n \\nFigure 2. Graphic illustration of training process of GAN: (a) data preparation; (b) training of GAN \\nB. DEEP LEARNING AND PHOTOMETRIC REDSHIFT \\nESTIMATION \\nA. D’Isanto and K. L. Polsterer proposed a new experimental \\ndeep learning technique the aim of which was developing a \\nnew method for photometric redshift estimation [10]. The \\nsuggested approach demonstrates a novel technique for \\nestimating PDF (probability density function) for redshifts \\nbased on imaging data in such a way tha t ultimately there is \\nno need for additional steps of feature-extraction and feature-\\nselection. The PDF is presented by the following equation \\n(1) [10]: \\n \\n               𝑝(𝑥) = ∑ \\u200a𝑛\\n𝑗=1 𝜔𝑗𝒩(𝑥 ∣ 𝜇𝑗,𝜎𝑗),               (1) \\n \\nwhere 𝒩(𝑥 ∣ 𝜇𝑗,𝜎𝑗) is a normal d istribution, 𝜇𝑗 is a given \\nmean, 𝜎𝑗 is standard deviation, 𝑥 is a given value, 𝜔𝑗 is a \\nweighting factor of each component (all weights sum to one). \\nIn order to achieve the objective, the researchers \\ncombined a deep convolutional network  with a mixture \\ndensity network. The evaluation was presented as Gaussian \\nmixture models as a representation of the probability density \\nfunctions in the redshift space.  As an addition to the \\nconventional estimation methods, the continuous ranked \\nprobability score (CRPS) and the probability integral \\ntransform (PIT) were implemented as performance criteria. \\nThe proposed method was capable of predicting redshift \\nprobability density functions regardless of the type of source \\n(e.g., galaxies, stars, quasars) and showed better results that \\nwere performed by reference techniques and that were \\ndescribed in the scientific literature. This deep learning \\nmethod is highly universal and is able to address any kind of \\nprobabilistic regression problem based on imaging data [10]. \\nThe reviewed example demonstrates the flexibility and \\nversatility of artificial intelligence systems in astrophysical \\nresearch related to the analysis of imaging data. \\nC. GALAXY EVOLUTION RESEARCH WITH \\nGENERATIVE MODELS \\nGenerative models demonstrate the potential for processing \\nastronomical intelligence in a way that focuses on a data -\\ndriven approach. Kevin Schawinski, M. Dennis Turp, and Ce \\nZhang described a method that applies generative models to \\nprobe and research hypotheses in astrophysics and other \\nscientific fields.  \\nDuring the experiment, using a latent space \\nrepresentation of the data, the Fader artificial neural network \\nhas been trained to produce synthes ized data for hypothesis \\nverification [11].  \\nThe architecture of the Fader network is the \\nimplementation of the encoder -decoder system with a \\ndomain adversarial training element that implies researching \\nand processing graphical samples in accordance with t heir \\nphysical features [12]. The Fader network tries to minimize \\nthe following objectives (given pairs of graphic samples and \\nlabels {x, y}) [12]: \\n \\nℒ𝑎𝑒 = −\\n1\\n𝑚 ∑ ∥ 𝐷(𝐸(𝑥),𝑦)−𝑥 ∥2\\n2−𝜆𝐸log\\u2061(𝑃(1−\\n\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061𝑦 ∣ 𝐸(𝑥))),                             (2) \\n \\n             ℒ𝑑𝑖𝑠 = −\\n1\\n𝑚 ∑log\\u2061(𝑃(𝑦 ∣ 𝐸(𝑥))),               (3) \\n \\nwhere 𝑥 represents images, 𝐸(𝑥) is the encoder (neural \\nnetwork) input, which is responsible for mapping from the \\nimage space to a latent representation of fixed dimension \\n[12], 𝐷(𝐸(𝑥),𝑦) is the decoder (a neural network as well), it \\ntakes attempts to rebuild 𝑥, {𝑥,𝑦} are binary labels, ℒ𝒶ℯ and \\nℒ𝒹𝒾𝓈 are two loss functions that interact with each other \\nthrough adversarial cooperation. \\nThe quenching of star formation in galaxies was used as \\nan illustration of the effectiveness of the method because this \\nprocess is well described  in astrophysical literature. In \\naddition to approaches that are based on simulations and \\nobservations, this method can be useful in exploring \\nimportant astronomical and other celestial phenomena from \\na different perspective [12]. The underlying reason why  \\nresearchers picked this architecture is that the Fader can \\ndistinguish two data distributions and learn and visualize \\nthese differences.  \\nAs for the limitations of this approach, it is important to'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le.pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349 \\n342 VOLUME 20(3), 2021 \\nstress that the described method is mainly applicable to test \\nhypotheses but not to prove them in a conventional way. \\nAlso, there is always room for mismatch when it comes to \\nthe collation of real data and imperfection of training sets and \\nnetwork design.  \\nThe proposed method requires domain knowledge \\nmanagement by the user since it is not completely \\nunmanned. Anyway, the proposed method of applying \\nFader-like generative models in testing hypotheses and \\nphysical processes modeling showed noteworthy potential in \\nastronomy and other scientific fields [12]. \\nD.4 SPATIAL-GANS AND SYNTHETIC IMAGING \\nMichael J. Smith and James E. Geach focused their research \\nendeavor on the problem of the small size of images \\nproduced by generative adversarial neural networks \\n(GAN/GANN, which are mentioned above) and on the \\nability of the  framework known as Spatial Generative \\nAdversarial Networks (SGANs, designed and described by \\nJetchev et al.) to generate large graphic images, if training \\nimage samples demonstrate a certain level of periodicity – \\nisotropy (cosmological principle) makes deep imaging \\nsurveys fit the criterion [13, 14]. \\nSGAN was trained to produce images resembling the \\neXtreme Deep Field (XDF) – the photo portrait of the \\nuniverse that was assembled by combining 10 years of \\nNASA Hubble Space Telescope photographs, which \\ncontains about 5,500 galaxies even within its smaller field of \\nview. As a result, generated images of fake  galaxies got a \\nhigh level of fidelity with real samples from the XDF in \\nterms of abundance, morphology, magnitude distributions, \\nand colors. In this particular example, researchers have \\ngenerated a 7.6-billion pixel ‘generative deep field’ spanning \\n1.45 degrees, showing that this approach can be extrapolated \\nto other training sets for producing realistic pseudo surveys \\nthat can be successfully applied in astrophysics and other \\nfields [13]. \\nDespite some limitations of the proposed method \\n(generated images a re dependent on the training set; \\nresearchers couldn’t reach stable learning output with more \\nthan three photometric bands) it also has undeniable \\nadvantages. The method is empirically driven because the \\ndata is used as the model, and it can be applied to generate \\nextremely realistic artificial images for the design, \\ndevelopment, and exploitation of new astronomical surveys. \\nFor instance, the technique allows assembling large training \\nsets for different fractionalization and classification tasks in \\nastrophysics [13]. The suggested generative technique \\nmakes it possible to expand small pieces of information \\nretrieved from the early phases of a new survey to a level that \\nwill be applicable for training deep learning models. \\nDescribed categorization and groupin g algorithms can be \\neffectively trained on the generated data and be implemented \\ntowards new data which can lead to expediting the \\nprocessing of data of new surveys. Equation 4 represents a \\nschematic architecture of the method [13]: \\n \\n𝐷𝑅(𝑥) = {\\n𝑆(𝐶(𝑥)−𝔼𝑥𝑓∼𝑄𝐶(𝑥𝑓)) for real 𝑥\\n𝑆(𝐶(𝑥)−𝔼𝑥𝑟∼ℙ𝐶(𝑥𝑟)) for generated 𝑥\\n,  (4) \\n \\nwhere 𝑆 is the activation function (sigmoid), 𝑥 is data, 𝐷𝑅(𝑥) \\nis the discriminator, 𝐶(𝑥) is the output of the final layer \\nwithout activation function, 𝑥𝑓 stands for faked images, 𝑥𝑟 – \\nreal images, 𝔼 is an expectation. \\nE. MORPHEUS – DL TOOL FOR ANALYSIS OF \\nASTRONOMICAL IMAGES \\nSurveying galaxies is a major instrument of observational \\nastronomy. Ryan Hausen and Brant Robertson designed and \\ndescribed a deep learning framework for pixel-level analysis \\nof astronomical image data – Morpheus [15]. This model \\nhelps astronomers to automatically classify galaxies by their \\nshape or morphology. It implements deep learning methods \\nin order to perform diverse astronomical tasks such as source \\ndetection, segmentation, and morphological classification \\nwhich is carried out pixel -by-pixel through  a semantic \\nsegmentation approach – a modified version of the computer \\nvision algorithm.  \\nTechnically speaking, Morpheus is implemented as a \\nconvolutional neural network (Figure 3) similar to the U-Net \\nframework and designed through a combination of Python 3 \\nas the main tool and TensorFlow as a machine learning \\nlibrary. It is constructed from a series of so -called “blocks” \\nthat unify multiple reusable operations [15, 16]. \\n \\n \\nFigure 3. Convolutional neural network. \\nRepresented by the Morpheus team pixel -by-pixel \\nclassification technique of astronomical images can be \\nconsidered as an effective method of data analysis with wide \\napplicability provided that suitable training datasets are \\navailable. The framework showed promising results with \\ndifferent datasets. A s a performance assessment tool, \\nCANDELS HLF and 3D-HST data were used, and Morpheus \\ndemonstrated a strong capability for morphological \\nclassification and object detection. \\nF. GAN & DEVELOPING NEW MOLECULES \\nZhavoronkov et al. described the process of developing new \\nmolecules that lasted only 21 days using artificial \\nintelligence and GAN; the molecules have been successfully \\ntested in mice. Customized and proprietary data were used'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le.pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349  \\nVOLUME 20(3), 2021 343 \\nas training and test datasets. The desc ribed approach has \\nsuccessfully passed experimental validation. The cost of the \\nmethod is only a small fraction of the cost associated with \\nthe traditional approach to conventional drug discovery [17]. \\nThe newly developed deep generative model GENTRL \\n(generative tensorial reinforcement learning) showed \\nsuccessful results in terms of de novo small-molecule design; \\nit was applied as a system for discovering potent inhibitors \\nof discoidin domain receptor 1 (DDR1), molecules that are \\ninvolved in the regulation of cell functions and related to \\nfibrosis and other diseases. It is anticipated that this method \\ncan be improved further as a perspective approach to identify \\ndrug candidates. \\nIt was also reported that GANs are able to successfully \\ndesign novel molecules for different inflammation-, fibrosis-\\n, and cancer-inducing protein targets [18]. During this work, \\ntwo conditional GANs were stacked as one functional deep \\nlearning chain ( conditional GANs  and Wasserstein GAN  \\nwith gradient penalty (WGAN -GP)) in order to ac hieve \\nexperimental expectations: the second network improved the \\nresults of the first one (stage 2 and stage 1). The following \\nequations represent loss functions for the two stages \\nmentioned above (stage 1 and stage 2 respectively, equations \\n5–8) [18]: \\n \\n        \\nℒ𝐷0 = 𝔼𝑥∼𝑝real [−𝐷0(𝑥)]\\n+𝔼𝑧∼𝑝𝑧,𝑐∼𝑝real [𝐷0(𝐺0(𝑧,𝑐))]\\n+𝜆𝔼𝑥̂∼𝑝𝑥[(∥∥∇𝑥̂𝐷0(𝑥̂)2 −1∥∥)2]\\n,          (5) \\n \\n \\nℒ𝐺0 = 𝔼𝑧∼𝑝𝑧,𝑐∼𝑝real \\n[−𝐷0(𝐺0(𝑧,𝑐))−𝛼log\\u2061(𝑓0(𝐺0(𝑧,𝑐),𝑐))],      (6) \\n \\nwhere the generator is conditioned by a variable 𝑐  – 𝐺0(𝑧,𝑐), \\n𝐺 and 𝐷 are generator and discriminator respectively, 𝑧 is a \\nrandom noise vector sampled from (𝑝𝑧), 𝑥 is data (𝑥 and 𝑐 \\nalso represent a molecule representation and a gene \\nexpression sig nature), 𝑝real\\u2061 – real data distribution, 𝑓0 is a \\nfunction that represents a neural network and measures the \\nprobability of a gene expression signature, 𝜆 and 𝛼 are \\nregularization parameters; \\n \\n                 \\nℒ𝐷1 = 𝔼𝑥∼𝑝real [−𝐷1(𝑥)]\\n+𝔼𝑠0∼𝑝𝐺0,𝑐∼𝑝real [𝐷1(𝐺1(𝑠0,𝑐))]\\n+𝜆𝔼𝑥̂∼𝑝𝑥̂ [(∥∥∇𝑥̂𝐷1(𝑥̂)∥∥2 −1)\\n2\\n]\\n,           (7) \\n \\nℒ𝐺1 = 𝔼𝑠0∼𝑝𝐺0,𝑐∼𝑝real  \\n[−𝐷1(𝐺1(𝑠0,𝑐))−𝛼log\\u2061(𝑓1(𝐺1(𝑠0,𝑐),𝑐))], (8) \\n \\nwhere (𝐺1(𝑠0,𝑐)) is the generator, (𝐷1(𝑥)) is the \\ndiscriminator, 𝐺1 takes the output of 𝐺0(𝑠0 = 𝐺0(𝑧,𝑐)) and \\nthe gene expression signature (𝑐) as an input (instead of \\nrandom noise). \\nG. GANS IN DISCOVERING NEW MATERIALS & \\nPREDICTING CRYSTAL STRUCTURE \\nThe combined techniques of machine learning and GAN \\nhave confirmed their applied effectiveness in discovering \\nnew stable materials and predicting their crystal structure, \\nwhich was described by Schmidt et al. [19]. A similar \\nmethod called CrystalGAN was proposed by Asma Nouira et \\nal., which made it possible to identify cross -domain \\nconnections in real data and to create new crystal structures. \\nThe proposed approach has demonstrated that it can \\nefficiently integrate knowledge sets provided by human \\nexperts [20]. The GAN model showed its capability to \\ngenerate new solid crystallographic structures.  \\nCrystalGAN was abl e to successfully identify cross -\\ndomain connections in real data and generate novel \\nstructures. The model can be considered as the first GAN that \\nhas been explicitly designed to generate scientific data in \\nmaterials science. CrystalGAN showed promising res ults \\nand coped with the challenging task of discovering novel \\nmaterials for hydrogen storage. Diverse GAN architectures \\nare currently being studied in order to receive data of even \\nhigher complexity (compounds that consist of four or five \\nchemical elements). It is important to stress that CrystalGAN \\nas a general model that can be effectively adapted to any \\nscientific task [20]. \\nH. DEEP LEARNING FOR DEEPFAKES CREATION AND \\nDETECTION \\nDeepfakes pose a serious threat to a person’s personal safety \\nsince fabricated media data can be used to discredit a person \\nby damaging their reputation (fake pornography, fake news, \\nfraud, hoaxes, etc); they can provoke political instability, \\ntrigger violence, or even a war conflict.  \\nDeepfakes are  usually created with a special type of \\nneural network that is called an autoencoder that studies \\neffective information codings in an unsupervised way. Using \\nthe encoder-decoder chain the method allows replacing the \\nface of one person with another (in video or photograph; for \\ninstance, Reface and DeepNude applications).  \\nOne of the powerful tools that enhance the capabilities of \\ndeepfakes is a generative adversarial network (GAN). A \\nGAN trains a decoder (generator) and a discriminator in an \\nadversarial inte rplay that makes fabricated data difficult to \\nidentify since two networks are consecutively evolving. As \\nsoon as the deepfake is identified, the system will \\nimmediately correct the defect and its further detection will \\nbe difficult. Due to the increasing q uality of deepfake \\nsamples, detection methods also need to be improved. It has \\nbeen suggested to make a benchmark data set of deepfakes \\nthat will help in developing effective detection methods [21].  \\nThis should simplify the process of training detection \\nalgorithms that require a massive training set. To better \\nunderstand the possible methods of countering fakes, it is \\nhighly important to thoroughly study GANs. One of the \\nperspective methods suggests detecting deepfakes by \\nanalyzing convolutional traces [22 ]. The approach is based \\non examining digital “fingerprints” to discriminate generated \\nimages and distinguish them from real photographic data.'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le.pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349 \\n344 VOLUME 20(3), 2021 \\nI. GANS IN BIOMEDICAL INFORMATICS \\nDeep learning and GAN methods are actively used in \\nmedical imaging (X -ray radi ography, magnetic resonance \\nimaging, positron emission tomography, etc.) as a powerful \\ntool that allows health-care experts and radiologists to detect \\nserious medical conditions and diseases at early stages with \\na high percentage of accuracy.  \\nSince image data for many diseases are scarce, there is an \\nurgent need for additional sources of information for training \\nmodels. When there are not enough images for model \\ntraining, GANs can generate high -quality samples that can \\nbe successfully applied as a solution  in medical image \\nanalysis [23]. CycleGAN utilizes a cycle sequence loss to \\nensure model learning without paired data.  \\nThus, it can map from one domain (A in Figure 4) to \\nanother (B in Figure 4) without pairwise alignments between \\nthe source and target domain [24]. \\nWolterink et al. [25] described an application of Cycle -\\nGAN in the radiotherapy treatment planning to CT and MRI \\nimages of patients with brain tumors. \\n \\n \\nFigure 4. CycleGAN schematics. \\nResults demonstrate that Cycle -GAN outperforms a \\nconventional single GAN trained with paired images. Li et \\nal. [26] applied the GAN approach to predicting the \\npossibility of whether a patient has a rare disease or not; the \\nprediction accuracy was 5% higher  compared to standard \\nmethods. Recent studies also suggest that GANs can be \\nengaged in solving the problem of lack of data in \\nbioinformatics due to the capability of the network to \\ngenerate high-quality data samples [27].  \\nModels trained on small datasets demonstrate high bias, \\ntend to overfit, and produce inaccurate predictions in terms \\nof classification tasks.  \\nJ. GANS FOR CREATING FONT EXAMPLES \\nGAN can be also effective as a tool for creating new fonts \\nand unique hand -written symbols like digits and lett ers. A \\nstandard GAN method in a combination with ADAM \\noptimizer was tested on the MNIST dataset. The experiment \\nconducted by the authors showed the potential of the said \\napproach to generate high -quality graphic samples of hand -\\nwritten symbols. The overwhe lming majority of obtained \\nsymbols looked similar to real digits, they were \\ndistinguishable by a sufficient level of clarity, structure, \\nshape, and there was no significant graphic noise (Fig. 5) \\n[28].  \\n \\n \\nFigure 5. Samples generated by a GAN based on the \\nMNIST dataset. \\nTo obtain results of even higher quality the GAN \\nparameters can be experimentally readjusted. \\nIt should also be noted that a similar architecture can be \\nused in typography to generate new font models, as well as \\nin applied forensics to generate new training handwritten \\nsamples that can be used as part of datasets for training \\nhandwriting recognition and identification systems.  \\nMore examples can be reviewed and analyzed, but it is \\nbarely possible to list them all. The approaches reviewed \\nabove and summarized in Table 1 are practical confirmation \\nof the high efficiency of using deep learning and gene rative \\nadversarial neural networks in a wide range of applied areas \\nin the context of sample and image data processing. \\nAlternatively, other generative models can be considered \\nfor the similar range of tasks: \\n• Variational Autoencoders (VAE) have density \\nestimation, invertible, stable training, better \\ndiversity, but (a) the quality of the synthesized \\nsamples is much lower than GANs can produce and \\n(b) slow learning speed. \\n• Autoregressive Model – more diverse samples, but \\nlearning requires supervision. \\n• Flow Models – lower quality samples. \\n• Hybrid Models – less stable. \\nGANs are still the best option when realistic generation \\nis the main goal.'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le.pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349  \\nVOLUME 20(3), 2021 345 \\nTable 1. Peculiarities and possible modifications of the reviewed heuristics. \\nApplication Area Technology/Models Specificity Possible Modification \\nMethods \\n \\n \\n \\nObservational Astronomy \\nGAN and DL for recovering \\nfeatures in galaxy images \\nand photometric redshift \\nestimation; Fader network, \\nSpatial GAN (SGAN), \\nMorpheus. \\nAn important component of \\ncosmological research. \\nLimited capacity of the \\ntraining sets; domain know -\\nledge oriented; generated \\nimage samples depend on \\nthe training datasets. \\nApplying synthesized \\nsamples for enlarging \\nexisting data -sets; dropout \\nand random forest methods \\nto avoid overfitting; \\narchitecture optimization. \\n \\n \\nHealth Care/Longevity \\nDL and GAN; Generative \\nTensorial Reinforcement \\nLearning (GENTRL); \\nWasserstein GAN (WGAN). \\nA highly effective approach \\nwith promising results; \\nfurther applications of the \\nmethod and its limitations \\nare being actively studied. \\nPotentially more effective \\ncombinations of models re -\\nquire further research. \\n \\n \\nMaterials Science \\nML/DL; GAN; Crystal -\\nGAN. \\nA successful approach for \\nthe design of new materials. \\nCross-domain knowledge \\nbases in physics and chem-\\nistry are needed for further \\noptimization and designing \\neven more productive \\nmodels. \\n \\n \\nDeep Fake \\nStandard GANs, autoenco -\\nder network. \\nA high degree of influence \\non the social factor and pub-\\nlic safety. \\nFurther improvement of \\ndetection methods; creation \\nof a benchmark data set of \\ndeepfakes; convolutional \\ntrace identification appro-\\naches. \\n \\n \\n \\n \\nBioinformatics \\nDL and GAN; CycleGAN. The method maps one \\ndomain to another without \\npairwise alignments be t-\\nween the source and target \\ndo-main. The suggested \\napproach can help to solve \\nthe problem of lack of data \\nin bioinformatics. \\nOptimizing the architecture, \\nexamining the possibility of \\nadding Gaussian noise bet -\\nween loops and analyzing its \\nimpact on system perfor-\\nmance.  \\n \\n \\nTypography and \\nHandwritten Samples \\nGAN; Deep Convolution \\nGANs. \\nRequires high -quality data -\\nsets and sophisticated algo -\\nrithmic optimization me -\\nthods. \\nNetwork parameters re -\\nadjustment; normalizing a \\nnumber of training epochs; \\napplying additional combi -\\nnations of optimization \\ntechniques. \\n \\nIII. ADVANCED APPROACHES FOR IMPROVING DEEP \\nLEARNING AND GANS APPLIED EFFICIENCY \\nGiven the above descriptions, there are several possible ways \\nthat could help to improve the effectiveness of all these \\napproaches for various scientific fields. \\nA. IMPROVING PERFORMANCE USING DATA \\nAs for improving performance with data, it needs to be \\nstressed that the more training data collected, the better the \\nperformance since this directly affects the quality of the deep \\nlearning models used. The efficiency of any algorithm loses \\nits value if the amount of data is insufficient for full -fledged \\ntraining. \\nIf there is not enough data in the training set, it is \\nadvisable to consider artificial data generation as an option. \\nAs it was mentioned by Schawinski et al. [7], the main \\nconstraint of their approach was the limited capacity of the \\navailable training set. Fabricated images generated by GANs \\nmay be considered as a possible solut ion for this issue. In \\nother words, special GAN architectures can assist other \\nGANs to improve their performance and results. When it \\ncomes to image data, we can either synthesize new images \\nor randomly modify samples of existing images; we can also \\nuse random rotation or shifting images or adding simulated \\nnoise. The principle of data augmentation applies to other \\ndata types as well (vectors of numbers, text, etc). \\nLack of training data can result in overfitting, thus the \\nbest way to avoid it is to provide a deep neural network with \\nincreased quantity of quality training data. Generative \\nmodeling with GANs can fill the gap by reinforcing smaller \\ndatasets with new synthesized high-quality images. The deep \\nconvolutional GAN architecture (DCGAN) is capable of \\ncreating photorealistic graphical samples that accurately'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le.pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349 \\n346 VOLUME 20(3), 2021 \\ncorrespond to the feature distributions of real galaxies in \\nterms of statistical estimation [29]. It is worth noticing, \\nthough, that these are limited by the accuracy of the \\nprobability density estimates. \\nLevi Fussell and Ben Moews experimentally proved that \\nStackGAN can be applied as a second-stage architecture and \\nform a combination system with DCGAN in order to \\nsynthesize fabricated galaxy images with higher resolutions, \\navoiding the obstructions that DCGAN models experience \\nwith such resolutions [30, 31].  \\nData rescaling in the context of the applied activation \\nfunctions also plays an important role. It’s sometimes useful \\nto normalize data values and rescale them; between 0 and 1 \\nif it comes to si gmoid activation functions, between 0 and \\ninfinity if it’s rectified linear unit (ReLU), -1 and 1 if it’s \\nhyperbolic tangent (tanh), for instance. This rescaling \\nprinciple can be applied to other activation functions as well. \\nB. ALGORITHMIC APPROACH \\n \\nFigure 6. Structure of interactions between Generator and \\nDiscriminator networks in GAN. \\nAs for the algorithmic approach, if GANs mainly use the \\noriginal algorithm designed by Goodfellow et al. (Figure 6) \\nand its modifications, other deep learning algorithms u se \\ndiverse models and approaches that vary from case to case \\n[9]. \\nEquation 9 represents a mathematical description of \\nGAN as a variation of the minimax two-player game [9]: \\n \\n𝑚𝑖𝑛\\u2061\\n𝐺\\n\\u200a𝑚𝑎𝑥\\n𝐷\\n\\u200a𝑉(𝐷,𝐺) = 𝔼𝒙∼𝑝data (𝒙)[log\\u2061𝐷(𝒙)]+\\n\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061\\u2061𝔼𝒛∼𝑝𝒛(𝒛)[log\\u2061(1−𝐷(𝐺(𝒛)))],              (9) \\n \\nwhere 𝐺 is the generator network; 𝐷 is the discriminator \\nnetwork; 𝑥 is a sample of real data; 𝑝𝑧(𝑧) – is a prior on input \\nnoise variables; 𝑧 is noise;  𝐷(𝑥) is the probabil ity that 𝑥 \\nactually is from the data rather than from the generator; 𝐺(𝑧) \\nis the generator output; 𝔼 is prediction (expectation), the first \\npart represents the discriminator ’s predictions on the real \\ndata, the second one – the discriminator’s predictions on the \\nfake/generated data; 𝑉(𝐷,𝐺) is the value function of \\ndiscriminator and generator in the two -player minimax \\ngame. \\nEach time proper algorithmic diagnostics should be \\napplied to the desired deep learning model. All examples \\nreviewed in the paper ca n be experimentally modified in \\nterms of weight configuration, network topology, types of \\nactivation functions, learning rate, batches, and number of \\nepochs. The network topology depends on the task at hand. \\nSince at the moment it is generally accepted that there are no \\nunified rules regarding how many layers or how many \\nneurons are needed for a particular configuration of a neural \\nnetwork, these parameters are selected experimentally. \\nC. HYBRID METHODS AND OPTIMIZATION \\nThe new technique proposed by Karras et al. allows training \\ngenerative adversarial networks with limited data. They have \\nsuggested methods of adaptive discriminator augmentation \\nin GANs and described the following overfitting heuristics \\n[32]: \\n \\n𝑟𝑣 =\\n𝔼[𝐷train ]−𝔼[𝐷validation ]\\n𝔼[𝐷train ]−𝔼[𝐷generated ] \\u2061\\u2061\\u2061\\u2061\\u2061\\u2061𝑟𝑡 = 𝔼[sign(𝐷train )], (10) \\n \\nwhere the discriminator outputs are denoted by 𝐷𝑡𝑟𝑎𝑖𝑛 for the \\ntraining set, 𝐷𝑣𝑎𝑙𝑖𝑑𝑎𝑡𝑖𝑜𝑛 for the validation set, and 𝐷𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑒𝑑 \\nfor the generated images,  𝔼 denotes their mean over 𝑁 \\nconsecutive minibatches. During the experiment, 𝑁 was \\nequal to 4, which corresponds to 4 × 64 = 256 images. For \\nboth parts of heuristics, 𝑟\\u2061 = \\u20610 stands for no overfitting and \\n𝑟\\u2061 = \\u20611 implies complete overfitting. The goal of the \\nexperiment was to adjust the augmentation probability 𝑝 so \\nthat the selected heuristic fits an appropriate target value. \\nTwo heuristic units:  𝑟𝑣 signifies the output for a validation \\nset relative to the training set and generated images, 𝑟𝑡 \\nevaluates the fraction of the training set that gets positive \\ndiscriminator outputs [32]. Additional augmentation and \\nregularization approaches in the GAN context have been \\nsuggested by Cubuk et al. (September 2019) and Zhang et al. \\n(February 2020) [33, 34]. \\nRecent research shows that a combination of multiple \\nGANs can create generated data with higher quality \\ncompared to a conventional single GAN [32]. It should also \\nbe noted that sometimes it happens that due to oversimplified \\nloss function GANs do not learn th e way they are expected \\nto (mode collapse, vanishing gradients, convergence). This \\nproblem remains one of the active areas of research at the \\nmoment. \\nSophisticated regularization methods, such as dropout \\n(equation 11), can also help to avoid overfitting in neural \\nnetworks [35]: \\n \\n                      ŵ𝑗 = {w𝑗,     with 𝑃(𝑐)\\n0,     otherwise ,              (11) \\n \\nwhere P(c) is the probability of c (stands for “keeping a \\nweight” factor) to keep a row in the weight matrix, w𝑗 is a \\nreal row in the weight matrix before dropout, ŵ𝑗 is a diluted \\nrow in the weight matrix. Dropout randomly omits (or “drops \\nout”) neurons of a neural network (both hidden and visible) \\nduring the training process. It should be not ed that zeroing \\nout the node does not impact the end result. \\nK-fold cross -validation is another method that can be \\napplied to a model in order to avoid overfitting. Data is'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le.pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349  \\nVOLUME 20(3), 2021 347 \\nseparated into K randomly -assigned fragments where one \\nfragment is earmarked as test data. The remaining combined \\nK-1 (minus one) fragments are used for training and after \\nthat, results need to be evaluated with the test set. This cycle \\nthen reiterates for each fragment and the average of the K r -\\nsquared scores is calculated, or the results displayed in a box \\nplot to give the median value and identify outliers. \\nAnother possible way to improve the results obtained \\nusing deep learning methods is algorithmic model rotation. \\nTo solve the same problem, completely different types of \\nneural netw orks and methods of their modification can be \\napplied. Restarts indicate the impact of local minima and \\nsaddle points in the loss function. \\nExperimenting with linear and non -linear methods \\n(logistic regression, polynomial regression, and multiple \\nregression) along with tree methods like gradient boosting, \\nclassification and regression trees, and random forest \\ndecreases variance and could give different results in terms \\nof productivity. For classification and regression analysis in \\nneural networks, support -vector networks and k -nearest \\nneighbors algorithms also demonstrate high-level efficiency. \\nSometimes, to obtain more efficient results, it makes sense \\nto resort to the hybridization of models, combining the \\nfollowing approaches with each other: learning vec tor \\nquantization, Boltzmann machines, multilayer perceptron, \\nconvolutional neural network, long short -term memory \\narchitecture, competitive networks such as GANs, \\nautoencoder networks, deep stacking networks. The list of \\narchitectures given is only illustrative since the number of all \\npossible combinations increases exponentially. \\nPerformance can be also improved by involving \\nalgorithms for gradient -based optimization such as \\nRMSProp, AdaGrad, Momentum, Adagrad, Adadelta, and \\nADAM. Upon experimental evaluat ion, ADAM \\ndemonstrated strong results with logistic regression, multi -\\nlayer neural networks, convolutional neural networks, and \\nperformed equal or better than RMSProp , regardless of \\nhyper-parameter settings [36]. Using ADAM as an example, \\nand considering it takes its name from “adaptive moment \\nestimation,” we can see that this method utilizes evaluations \\nof first and second moments of gradient adjusting the \\nlearning ra te for each weight parameter of the neural \\nnetwork. The moment is a numerical characteristic of the \\ndistribution of a given random variable (expected value of \\nthe variable to the n-th power, equation 12): \\n \\n                              𝑀𝑛 = 𝐸[𝑉𝑟𝑎𝑛.\\n𝑛],                            (12) \\n \\nwhere 𝑀 is the moment, 𝐸 is the expected value of the \\nvariable, 𝑉𝑟𝑎𝑛. is a random variable. \\nDeep learning optimization models are still an open \\ndomain and still require in -depth research, both in terms of \\nmathematical groundings and in terms of software and \\nhardware implementation. \\nIV. CONCLUSIONS \\nSince the amount of information is constantly increasing, \\nscientists need modern and efficient tools to examine and \\nanalyze the data they receive. Nowadays researchers have \\nunprecedented access to advanced AI tools for gathering, \\nretrieving, processing, and recovering images and statistical \\ndata [37, 38, 39]. It is certain that machine learning can \\nprocess and analyze information much faster than humans or \\nother computational methods, furthermore, it can \\ncomprehend data patterns and liaisons that we do not even \\nrecognize, e.g., it may detect diverse types of galaxies before \\nscientists know they exist.  \\nIn the article, the authors analyze the types of modern \\nGAN architectures an d existing approaches to their design, \\nas well as the main advantages and prospects for widespread \\nimplementation of GAN and deep learning (DL) for solving \\ntopical problems of artificial intelligence. The above analysis \\nof successful cases illustrates and confirms the high \\nefficiency of GANs and DL in astronomy, molecular \\nbiology, materials science, bioinformatics, handwriting \\nrecognition, and deepfake detection. The authors provide an \\nanalysis and offer proposals for the development of \\nadvanced approaches in terms of design and implementation \\nof GANs and DL. In the future, the authors plan to develop \\nsoftware for the implementation of GAN and DL based on \\nthe discussed advanced approaches. \\nProcessing instrumental images using artificial neural \\nnetworks can a ccelerate further research and help in \\nreconstructing imaging data even for nonstandard and \\nunstudied phenomena. GANs can effectively remove noise \\nand provide as clear of an image as possible due to their \\nability to recover graphical data that have damaged  or \\nmissing pixels, or unwanted instrumental artifacts. Thus, the \\nmachine learning methods and GANs should be considered \\nas the most promising assistive technologies for science as a \\nwhole. \\nGAN is one of the relatively new DL technologies \\n(2014), requires a thorough research and analysis, \\nsignificantly affects various aspects of scientific and \\ntechnological development (creation of new drugs, space \\nexploration) and socio -political life including Deep Fake \\nproblem (photo, video, audio). \\nAs for drawbacks and limitations, it is worth noticing that \\nin order to train a machine learning system we need a lot of \\nlabeled and preprocessed information. Moreover, until \\nrecently, the scientific community simply had no data about \\nsome substantial aspects that are  important for preparing \\neffective training sets. In addition, neural networks are being \\nconsidered as a kind of black box: researchers do not always \\nunderstand exactly how artificial neural networks operate, \\nespecially when it comes to complex architectur es with \\nmany hidden layers of neurons. Using tools without a proper \\nlevel of understanding of how they work is a matter of \\nconcern among scientists. \\nNevertheless, DL systems continue contributing to \\nprogress across a range of different scientific fields [40, 41], \\nand therefore, the prospects for the further use of machine \\nlearning in applied scientific research, as well as methods for \\nits improving and optimizing, should continue to be'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le.pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349 \\n348 VOLUME 20(3), 2021 \\ncomprehensively studied. Proper technological unification \\nand combination of research efforts can lead to revolutionary \\nresults. \\nThe main task of the article is an in -depth scientific \\nanalysis of current practical approaches in terms of \\napplication of deep learning methods, including generative \\nmodels, in the most important sp heres of human activity, \\nbecause today DL and GANs still need thorough study and \\nresearch: theoretical basis, areas and limits of application, \\nsecurity and safety. Awareness of the scientific community \\nin modern methods of artificial intelligence and the need for \\naccess to pervasive analytical materials are among the key \\naspects influencing the speed, intensity, and novelty of \\nresearch. The article analyzes innovative approaches in DL \\nand GANs, provides an integral assessment of their \\neffectiveness, and off ers practical and theoretical \\nsuggestions for improvement. \\nIt is expected that modern deep learning technologies will \\nmake a significant contribution to science and the \\ndevelopment of research methodology, provided that proper \\nconvergence between the vast majority of scientific fields is \\nachieved. \\nReferences \\n[1] S. Russell, P. Norvig, Artificial Intelligence: A Modern Approach, 3rd \\ned., Upper Saddle River, New Jersey: Prentice Hall, 2009, 1 p. \\n[2] P. Barmby, Astronomical observations: a guide for allied researchers, \\n2019, [Online]. Available at: https://arxiv.org/abs/1812.07963, \\nhttps://doi.org/10.21105/astro.1812.07963. \\n[3] E. C. Sutton, Observational Astronomy. Techniques and \\nInstrumentation, Cambridge University Press, 2011, 1 p. \\n[4] J. Lee, P. L. Freddolino, Y. Zhang, Ab initio protein structure \\nprediction, D.J. Rigden (ed.), From Protein Structure to Function with \\nBioinformatics, 2017, pp. 1 –33. https://doi.org/10.1007/978-94-024-\\n1069-3_1. \\n[5] DeepMind, “AlphaFold: Using AI for scientific discovery,” Nature, \\nvol. 577, pp. 706–710, 2020. \\n[6] S. Roberts, A. McQuillan, S. Reece, S. Aigrain, “Astrophysically \\nrobust systematics removal using variational inference: application to \\nthe first month of Kepler data, ” Monthly Notices of the Royal \\nAstronomical Society , vol. 435, pp. 36 39–3653, 2013.  \\nhttps://doi.org/10.1093/mnras/stt1555. \\n[7] K. Schawinski, C. Zhang, H. Zhang, L. Fowler, G. K. Santhanam, \\n“Generative adversarial networks recover features in astrophysical \\nimages of galaxies beyond the deconvolution limit,” Monthly Notices \\nof the Royal Astronomical Society: Letters, vol. 467, issue 1, pp. 110–\\n114, 2017. https://doi.org/10.1093/mnrasl/slx008. \\n[8] P. Magain, F. Courbin, S. Sohy, “Deconvolution with Correct \\nSampling,” The Astrophysical Journal , pp. 472 –477, 1998.  \\nhttps://doi.org/10.1086/305187. \\n[9] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, \\nS. Ozair, A. Courville, J. Bengio, “Generative Adversarial Networks,” \\nProceedings of the International Conference on Neural Informatio n \\nProcessing Systems (NIPS), 2014, pp. 2672–2680. \\n[10] A. D’Isanto, K. L. Polsterer, “Photometric redshift estimation via deep \\nlearning. Generalized and pre -classification-less, image based, fully \\nprobabilistic redshifts,” Astronomy & Astrophysics , vol. 609, A1 11, \\npp. 1–16, 2018. https://doi.org/10.1051/0004-6361/201731326. \\n[11] G. Lample, N. Zeghidour, N. Usunier, A. Bordes, L. Denoyer, M. \\nRanzato, “Fader networks: Manipulating images by sliding attributes,” \\nProceedings of 31st Conference on Neural Information Processing \\nSystems, USA, 2017, pp. 1–10.  \\n[12] K. Schawinski, M. D. Turp, C. Zhang, “Exploring galaxy evolution \\nwith generative models,” Astronomy & Astrophysics , vol. 616, L16, \\npp. 1–4, 2018. https://doi.org/10.1051/0004-6361/201833800. \\n[13] M. J. Smith, J. E. Geach, “Generative deep fields: arbitrarily sized, \\nrandom synthetic astronomical images through deep learning,” \\nMonthly Notices of the Royal Astronomical Society, vol. 490, issue 4, \\npp. 4985–4990, 2019. https://doi.org/10.1093/mnras/stz2886. \\n[14] N. Jetchev, U. Bergmann, R. Vollgraf, “Texture Synthesis with Spatial \\nGenerative Adversarial Networks,” Proceedings of Workshop on \\nAdversarial Training, NIPS’2016, Barcelona, Spain, 2016, pp. 1–11.  \\n[15] R. Hausen, B. Robertson, Morpheus: A Deep Learning Framework for \\nPixel-Level Analysis of Astronomical Image Data, 2019, [Onli ne]. \\nAvailable at: https://arxiv.org/abs/1906.11248 \\n[16] O. Ronneberger, P. Fischer, T. Brox, “U-Net: Convolutional networks \\nfor biomedical image segmentation ,” Proceedings of the 18th \\nInternational Conference Medical Image Computing and Computer -\\nAssisted Intervention, Munich, Germany, October 5-9, 2015, pp. 1–8. \\n[17] A. Zhavoronkov, Y. A. Ivanenkov, A. Aliper et al., “Deep learning \\nenables rapid identification of potent DDR1 kinase inhibitors,” Nature \\nBiotechnology, vol. 37, pp. 1038 –1040, 2019.  \\nhttps://doi.org/10.1038/s41587-019-0224-x. \\n[18] O. Méndez-Lucio, B. Baillif, D.-A. Clevert, D. Rouquié, J. Wichard , \\nDe novo generation of hit -like molecules from gene expression \\nsignatures using artificial intelligence, 2020. \\nhttps://doi.org/10.1038/s41467-019-13807-w. \\n[19] J. Schmidt, M. R. G. Marques, S. Botti et al., Recent advances and \\napplications of machine learning in solid-state materials science, 2019. \\nhttps://doi.org/10.1038/s41524-019-0221-0. \\n[20] A. Nouira, N. Sokolovska, J. -C. Crivello, “CrystalGAN: Learning to \\ndiscover crystallographic structures with generative adversarial \\nnetworks,” Proceedings of the AAAI Spring Symposium: Combining \\nMachine Learning with Knowledge Engineering, Stanford University, \\nUSA, March 25-27, 2019, pp. 1–9. \\n[21] T. T. Nguyen, C. M. Nguyen, D. T. Nguyen, D. T. Nguyen, S. \\nNahavandi, Deep Learning for Deepfakes Creation and Detection: A \\nSurvey, 2020, [Online]. Available at: https://arxiv.org/abs/1909.11573 \\n[22] L. Guarnera, O. Giudice, S. Battiato, “DeepFake detection b y \\nanalyzing convolutional traces,” Proceedings of the IEEE Conference \\non Computer Vision and Pattern Recognition Workshops , 2020, pp. \\n1–10. https://doi.org/10.1109/CVPRW50498.2020.00341. \\n[23] L. Lan, L. You, Z. Zhang, Z. Fan, W. Zhao, N. Zeng, Y. Chen, X. \\nZhou, Generative Adversarial Networks and its Applications in \\nBiomedical Informatics, 2020 . \\nhttps://doi.org/10.3389/fpubh.2020.00164. \\n[24] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, Unpaired Image-to-Image \\nTranslation using Cycle -Consistent Adversarial Networks , 2020, \\n[Online]. Available at: https://arxiv.org/abs/1703.10593 \\n[25] J. M. Wolterink et al., Deep MR to CT Synthesis Using Unpaired Data, \\nin Tsaftaris S., Gooya A., Frangi A., Prince J. (eds) Simulation and \\nSynthesis in Medical Imaging., SASHIMI 2017, Lecture Notes in \\nComputer Science, 2017, vol. 10557, Springer, Cham, pp. 14–23. \\n[26] W. Li, Y . Wang, Y. Cai, C. Arnold, E. Zhao, Y. Yuan, Semi -\\nsupervised Rare Disease Detection Using Generative Adversarial \\nNetwork, 2018, [Online]. Available at: \\nhttps://arxiv.org/abs/1812.00547 \\n[27] M. Marouf, P. Machart, V. Bansal, C. Kilian, D. S. Magruder, C. F. \\nKrebs, S. Bonn, Realistic in silico generation and augmentation of \\nsingle-cell RNA-seq data using generative adversarial networks, 2020. \\nhttps://doi.org/10.1038/s41467-019-14018-z. \\n[28] O. Striu k, Y. Kondratenko, I. Sidenko, A. Vorobyova, “Generative \\nadversarial neural network for creating photorealistic images ,” \\nProceedings of the 2020 IEEE 2nd International Conference on \\nAdvanced Trends in Information Theory , Kyiv, Ukraine, November \\n27, 2020, pp. 1–4.  \\n[29] M. C. Chan, J. P. Stott, “Deep-CEE I: Fishing for galaxy clusters with \\ndeep neural nets,” Monthly Notices of the Royal Astronomical Society, \\nvol. 490, pp. 5770 –5787, 2019.  \\nhttps://doi.org/10.1093/mnras/stz2936. \\n[30] Z. L. Wen, J. L. Han, F. S. Liu, “A Catalog of 132,684 clusters of \\ngalaxies identified from sloan digital sky survey III,” The \\nAstrophysical Journal Supplement, vol. 199, issue 2, article id. 34, pp. \\n1–12, 2012. https://doi.org/10.1088/0067-0049/199/2/34. \\n[31] L. Fussell, B. Moews, “Forging new worlds: high-resolution synthetic \\ngalaxies with chained generative adversarial networks,” Monthly'),\n",
       " Document(metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le.pdf'}, page_content='Oleksandr Striuk et al. / International Journal of Computing, 20(3) 2021, 339-349  \\nVOLUME 20(3), 2021 349 \\nNotices of the Royal Astronomical Society, vol. 485, issue 3, pp. 3203–\\n3214, 2019. https://doi.org/10.1093/mnras/stz602. \\n[32] T. Karras, M. Aittala, J. Hellsten, S. Laine, J. Lehtinen, T. Aila, \\n“Training generative adversarial networks with limited data ,” \\nProceedings of the 34th Conference on Neural Information \\nProcessing Systems (NeurIPS 2020), Vancouver, Canada, June 2020, \\npp. 1–37. \\n[33] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, D. Metaxas, \\n“StackGAN: Text to photo-realistic image synthesis with stacked \\ngenerative adversarial networks ,” Proceedings of the 2017 IEEE \\nInternational Conference on Computer Vision (ICCV) , Venice, Italy, \\n2017, pp. 5908-5916. https://doi.org/10.1109/ICCV.2017.629. \\n[34] E. D. Cubuk, B. Zoph, J. Shlens, Q. V. Le, RandAugment : Practical \\nautomated data augmentation with a reduced search space, 2019, \\n[Online]. Available at: https://arxiv.org/abs/1909.13719. \\nhttps://doi.org/10.1109/CVPRW50498.2020.00359. \\n[35] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. \\nSalakhutdinov, “Dropout: A Simple Way to Prevent Neural Networks \\nfrom Overfitting,” Journal of Machine Learning Research, vol. 15, pp. \\n1929–1958, 2014. \\n[36] D. P. Kingma, J. L. Ba, “Adam: A method for stochastic \\noptimization,” Proceedings of the 3rd International Conference for \\nLearning Representations, San Diego, USA, 2015, pp. 1–15. \\n[37] R. Leizerovych, G. Kondratenko, I. Sidenko, Y. Kondratenko, “IoT -\\ncomplex fo r monitoring and analysis of motor highway condition \\nusing artificial neural networks,” Proceedings of the 2020 IEEE 11th \\nInternational Conference on Dependable Systems, Services and \\nTechnologies, DESSERT 2020 , Kyiv; Ukraine; 14 -18 May, 2020; \\nArticle No. 9 125004, pp. 207 –212. \\nhttps://doi.org/10.1109/DESSERT50317.2020.9125004. \\n[38] K. Ivanova, G. Kondratenko, I. Sidenko, Y. Kondratenko, “Artificial \\nintelligence in automated system for w eb-interfaces visual testing,” \\nCEUR Workshop Proceedings , vol. 2604, 2020, 4th International \\nConference on Computational Linguistics and Intelligent Systems, \\nCOLINS 2020; Lviv; Ukraine; 2020; pp. 1019–1031. \\n[39] V. M. Kuntsevich et al. (Eds), Control Systems: Theory and \\nApplications. Series in Automation, Control and Robotics , River \\nPublishers, 2018, 146 p. \\n[40] Y. Kondratenko, D. Simon, Structural and parametric optimization of \\nfuzzy control and decision making systems, In: Zadeh L., Yager R., \\nShahbazova S ., Reformat M., Kreinovich V. (eds), Recent \\nDevelopments and the New Direction in Soft-Computing Foundations \\nand Applications. Studies in Fuzziness and Soft Computing, Springer, \\nCham., vol. 361, 2018, pp. 273–289. \\n[41] Z. Gomolka, E. Dudek -Dyduch, Y. P. Kondrat enko, “From \\nhomogeneous network to neural nets with fractional derivative \\nmechanism,” Proceedings of the International Conference on \\nArtificial Intelligence and Soft Computing, ICAISC-2017, Rutkowski, \\nL. et al. (Eds), Part I, Zakopane, Poland, 11 -15 June, 2017, LNAI \\n10245, Springer, Cham, 2017, pp. 52–63. https://doi.org/10.1007/978-\\n3-319-59063-9_5. \\n[42] J. Brownlee. 18 Impressive Applications of Generative Adversarial \\nNetworks (GANs), 2019,  [Online]. Available at: \\nhttps://machinelearningmastery.com/impressive-applications-of-\\ngenerative-adversarial-networks/ \\n \\n \\nOLEKSANDR STRIUK , Ph.D. student and \\nresearcher at Petro Mohyla  Black Sea National \\nUniversity (PMBSNU). Master of Science in \\nSystem Analysis. Research interests include AGI, \\nartificial neural networks, reasoning in AI, \\nstatistics, and probability theory. \\n \\n \\nYURIY KONDRATENKO , Doctor of Science, \\nProfessor, Honour Inventor of Ukraine (2008), \\nCorr. Academician of Royal Academy of Doctors \\n(Barce-lona, Spain), Head of the Department of \\nIntelligent  Information Systems at  Petro Mohyla \\nBlack Sea National University (PMBSNU), \\nUkraine. He has received (a) the Ph.D. (1983) \\nand Dr.Sc. (1994) in Elements and Devices of \\nComputer and Control Systems from Odessa National Polytechnic \\nUniversity, (b) several international grants and scholarships for \\nconducting research at Institute of Automation of Chongqing \\nUniversity, P.R.China (1988 -1989), Ruhr -University Bochum, \\nGermany (2000, 2010), Nazareth College and Cleveland State \\nUniversity, USA (2003), (c) Fulbright Scholarship for researching in \\nUSA (2015/2016) at the Dept. of Electrical Engineering and \\nComputer Science in Cleveland State University. Research interests \\ninclude robotics, automation, sensors and control systems, intelligent \\ndecision support systems, fuzzy logic. \\n \\n \\n \\nView publication stats'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Hands-On Generative AI with\\nTransformers and Diﬀusion Models\\nWith Early Release ebooks, you get books in their earliest form—the\\nauthors’ raw and unedited content as they write—so you can take advantage\\nof these technologies long before the official release of these titles.\\nOmar Sanseviero, Pedro Cuenca, Apolinario Passos, and\\nJonathan Whitaker'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Hands-On Generative AI with\\nTransformers and Diﬀusion Models\\nby Omar Sanseviero, Pedro Cuenca, Apolinario Passos, and\\nJonathan Whitaker\\nCopyright © 2025 Omar Sanseviero, Pedro Cuenca, Apolinario\\nPassos, and Jonathan Whitaker. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway\\nNorth, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or\\nsales promotional use. Online editions are also available for\\nmost titles (https://oreilly.com). For more information, contact\\nour corporate/institutional sales department: 800-998-9938 or\\ncorporate@oreilly.com.\\nAcquisitions Editor: Nicole Butterﬁeld\\nDevelopment Editor: Jill Leonard\\nProduction Editor: Gregory Hyman\\nInterior Designer: David Futato'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Cover Designer: Karen Montgomery\\nIllustrator: Kate Dullea\\nDecember 2024: First Edition\\nRevision History for the Early Release\\n2023-03-16: First Release\\n2024-01-19: Second Release\\n2024-03-20: Third Release\\n2024-05-20: Fourth Release\\n2024-07-16: Fifth Release\\n2024-09-06: Sixth Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098149246 for\\nrelease details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media,\\nInc. Hands-On Generative AI with Transformers and Diﬀusion\\nModels, the cover image, and related trade dress are\\ntrademarks of O’Reilly Media, Inc.\\nThe views expressed in this work are those of the authors and\\ndo not represent the publisher’s views. While the publisher and\\nthe authors have used good faith eﬀorts to ensure that the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='information and instructions contained in this work are\\naccurate, the publisher and the authors disclaim all\\nresponsibility for errors or omissions, including without\\nlimitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and\\ninstructions contained in this work is at your own risk. If any\\ncode samples or other technology this work contains or\\ndescribes is subject to open source licenses or the intellectual\\nproperty rights of others, it is your responsibility to ensure that\\nyour use thereof complies with such licenses and/or rights.\\n978-1-098-14924-6\\n[LSI]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Preface\\nGenerative AI is a revolutionary technology that has rapidly\\ntransitioned from lab demos to real-world applications,\\nimpacting billions. It can create new content—images, text,\\naudio, videos, and more—by learning patterns from existing\\ndata, thereby enhancing creativity, augmenting data, or\\nassisting in many tasks. For instance, a generative AI model\\ntrained on music can compose new melodies, while one trained\\non text can generate stories or even programming code.\\nThis book isn’t just for experts – it’s for anyone who wants to\\nlearn about this fascinating new ﬁeld. We won’t focus on\\nbuilding models from scratch or diving straight into\\ncomplicated mathematics. Instead, we’ll leverage existing\\nmodels to solve real-world problems, helping you to build a\\nsolid intuition around how these techniques work and\\nproviding the foundation for you to keep exploring.\\nThis hands-on approach, we hope, will help you get up and\\nrunning quickly and eﬃciently with generative AI. You’ll learn\\nhow to use pre-trained models, adapt them for your needs, and\\ngenerate new data with them. You’ll also learn how to evaluate\\nthe quality of generated data and explore ethical and social'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='issues that may arise from using generative AI. This exposure\\nwill allow you to stay up to date with new models and help you\\nidentify areas that you may want to explore more deeply.\\nWho Should Read This Book\\nGiven the impressive products and news you might have seen\\nabout generative AI, it’s normal to be excited, or worried, about\\nit! Whether you’re curious about how programs can generate\\nimages, want to train a model to tweet in your style, or are\\nlooking to gain a deeper understanding of products like\\nChatGPT, this book is for you. With generative AI, we can do all\\nof that and many other things, including:\\nWrite summaries of news articles\\nGenerate images based on a description\\nEnhance the quality of an image\\nTranscribe meetings\\nGenerate synthetic speech in your voice style\\nIncorporate new subjects or styles into image generation\\nmodels, like creating images of \"your cat dressed as\\nan astronaut“.\\nNo matter your reason, you’ve decided to learn about\\ngenerative AI, and this book will guide you through it.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Prerequisites\\nThis book assumes that you are comfortable programming in\\nPython and have a foundational understanding of what\\nMachine Learning is, including basic usage of frameworks like\\nPyTorch or TensorFlow. Having practical experience with\\ntraining models is not required, but it will be helpful to\\nunderstand the content with more depth. The following\\nresources provide a good foundation for the topics covered in\\nthis book:\\nHands-On Machine Learning with Scikit-Learn and\\nTensorFlow, by Aurélien Géron (O’Reilly)\\nDeep Learning for Coders with fastai and PyTorch, by\\nJeremy Howard and Sylvain Gugger (O’Reilly)\\nIf you feel intimidated by it, don’t worry! The book is designed\\nto enhance your intuition and provide a hands-on approach to\\nhelp you get started.\\nWhat You Will Learn\\nThis book is divided into three parts:'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='In Part 1, we’ll introduce the fundamental building blocks\\nof generative AI. You’ll learn how to use pre-trained models\\nto generate text and images. This part will help you\\nunderstand the basics of the ﬁeld and understand the big\\npicture.\\nPart 2 is all about ﬁne-tuning, showcasing ways to take\\nexisting models and adapt them to your needs. We’ll walk\\nyou through how to teach a diﬀusion model a new concept,\\ncustomize a transformer model to classify text and reply in\\nconversations, and explore advanced techniques for\\nworking with large models on limited hardware. Don’t\\nworry if this is the ﬁrst time you read about transformer or\\ndiﬀusion models, you’ll learn about them soon.\\nIn Part 3, we’ll extend the ideas from the previous parts,\\ngenerating new modalities such as audio and getting\\ncreative with new applications. After you’ve read this book,\\nyou’ll have a solid understanding of the methods and\\ntechniques on which generative applications are built.\\nHow to Read This Book\\nWe designed the book to be read in order, but we have kept the\\nchapters as self-contained as possible so you can jump around\\nto the parts that interest you most. Many of the ideas covered in'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='this book apply to multiple modalities, so even if you are only\\ninterested in one particular domain (such as image generation),\\nyou may still ﬁnd it valuable to skim through the other\\nchapters.\\nWe’ve included exercises and code examples throughout the\\nbook, designed to help you get hands-on with the material. Try\\nto complete these exercises as you go along, and where\\npossible, see if you can adapt the examples to your use cases.\\nTrying things out for yourself will help you build a much\\ndeeper understanding of the material.\\nFinally, each chapter includes some additional resources in\\ntheir summary for further reading. We encourage you to\\nexplore these resources to deepen your understanding of the\\ntopics covered in the book. You don’t need to do it before you\\nprogress to a new chapter; you can come back later whenever\\nyou are ready to go deeper into the subjects that interest you.\\nSoftware and Hardware\\nRequirements\\nTo get the most out of this book, we highly recommend running\\nthe code examples as you read along. Experimenting with the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='code by making changes and exploring diﬀerent scenarios will\\nenhance your understanding. Working with transformers and\\ndiﬀusion models can be computationally intensive, so having\\naccess to a computer with an NVIDIA GPU is beneﬁcial. While a\\nGPU is not mandatory, it will signiﬁcantly speed up training\\ntimes.\\nThere are multiple online options that you can use, such as\\nGoogle Colaboratory and Kaggle Notebooks. Follow these\\ninstructions to setup your environment and follow along:\\nUsing Google Colab\\nMost code should work on any Google Colab instance. We\\nrecommend you use GPU runtimes for chapters with training\\nloops.\\nRunning Code Locally\\nTo run the code in your computer, create a Python 3.10 virtual\\nenvironment using your preferred method. As an example, you\\ncan do it with conda like this:\\nconda create -n genaibook python=3.10\\nconda activate genaibook'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='For optimal performance, we recommend using a CUDA-\\ncompatible GPU.  If you don’t know what CUDA is, don’t worry,\\nwe’ll explain it in the book.\\nThere are many support utilities and helper functions used\\nthroughout the book. To access them, please install the\\ngenaibook package:\\npip install genaibook\\nThis will in turn install the libraries requried to run\\ntransformers and diﬀusion models, along with PyTorch,\\nMatplotlib, NumPy, and other essentials.\\nAll code examples and supplementary material can be found in\\nthe book’s GitHub repository. You can run all the examples\\ninteractively in Jupyter Notebooks, and the repository will be\\nregularly updated with the latest resources.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to\\nthe publisher:\\nO’Reilly Media, Inc.\\n1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata,\\nexamples, and any additional information. You can access this\\npage at https://learning.oreilly.com/library/view/hands-on-\\ngenerative-ai/9781098149239/.\\nState of the Art: A Moving Target\\nState of the Art (SOTA) is used to describe the highest level of\\nperformance currently achieved in a particular task or domain.\\nIn the ﬁeld of generative AI, the SOTA is constantly changing as\\nnew models are developed and new techniques are discovered.\\nThis book will provide you with a solid grounding in the\\nfundamentals of generative AI, but by the time you read it, new\\nmodels will have been released that outperform the ones we\\ndiscuss here.\\nRather than trying to chase the ever-shifting best, we’ve tried to\\nfocus on general principles that will help you to understand'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='how the models work in a way that will be useful even as the\\nﬁeld continues to evolve. New models rarely come out of\\nnowhere and often build on the ideas of previous models. By\\nunderstanding the fundamentals, you’ll be better equipped to\\nunderstand the latest developments as they happen.\\n Rather than GPU, you can also use the MPS device, which might work on Macs with\\nApple Silicon, but we have not tested this conﬁguration extensively.\\n1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Chapter 1. An Introduction to\\nGenerative Media\\nA NOTE FOR EARLY RELEASE READERS\\nWith Early Release ebooks, you get books in their earliest form\\n—the authors’ raw and unedited content as they write—so you\\ncan take advantage of these technologies long before the oﬃcial\\nrelease of these titles.\\nThis will be the ﬁrst chapter of the ﬁnal book. Please note that\\nthe GitHub repo will be made active later on.\\nIf you have comments about how we might improve the content\\nand/or examples in this book, or if you notice missing material\\nwithin this chapter, please reach out to the editor at\\njleonard@oreilly.com.\\nGenerative models have become widely popular in recent\\nyears. If you’re reading this book, you’ve probably interacted\\nwith a generative model at some point. Maybe you’ve used\\nChatGPT to generate text, used style transfer in apps like\\nInstagram, or seen the deepfake videos that have been making'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='headlines. These are all examples of generative models in\\naction!\\nIn this book, we’ll explore the world of generative models,\\nstarting with the basics of two families of generative models,\\ntransformers and diﬀusion, and working our way up to more\\nadvanced topics. We’ll cover the diﬀerent types of generative\\nmodels, how they work, and how to use them. In this chapter,\\nwe’ll cover some of the history of how we got here and take a\\nlook at the capabilities oﬀered by some of the models, which\\nwe’ll explore in more depth throughout the book.\\nSo, what exactly is generative modeling? At its core, it’s about\\nteaching a model to generate new data that resembles its\\ntraining data. For example, if I train a model on a dataset of\\nimages of cats, I can then use that model to generate new\\nimages of cats that look like they could have come from the\\noriginal dataset. This is a powerful idea, and it has a wide range\\nof applications, from creating novel images and videos to\\ngenerating text with a speciﬁc style.\\nThroughout this book, you’ll discover popular tools that make\\nusing existing generative models straightforward. The world of\\nMachine Learning oﬀers numerous open-access models, trained\\non large datasets, available for anyone to use. Training these'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='models from scratch can be costly and time-consuming, but\\nopen-access models provide a practical and eﬃcient alternative.\\nThese pre-trained models can generate new data, classify\\nexisting data, and be adapted for new applications. One of the\\nmost popular places to ﬁnd open-access models is Hugging Face,\\na platform with over two million models for many Machine\\nLearning tasks, including image generation.\\nGenerating Images\\nAs an example of an open-source library, we’ll kick oﬀ with\\ndiﬀusers. diﬀusers is a popular library that provides access to\\nstate-of-the-art diﬀusion models. It’s a powerful, simple toolbox\\nthat allows us to quickly load and train diﬀusion models.\\nBy going to the Hugging Face Hub and ﬁltering for models that\\ngenerate images based on a text prompt (text-to-image), we\\ncan ﬁnd some of the most popular models, such as Stable\\nDiﬀusion and SDXL. We’ll use Lykon’s Dreamshaper 8, a\\ncommunity-ﬁne tune of Stable Diﬀusion version 1.5, a diﬀusion\\nmodel capable of generating high-quality images. If you browse\\nthe model website, you can read the model card, an essential\\ndocument for discoverability and reproducibility. There, you'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='can read about the model, how it was trained, intended use\\ncases, and more.\\nGiven we have a model (Stable Diﬀusion) and a tool to use the\\nmodel (diﬀusers), we can now generate our ﬁrst image! When\\nwe load models, we’ll need to send them to a speciﬁc hardware\\ndevice, such as CPU (cpu), GPU (cuda or cuda:0), or Mac\\nhardware called Metal (mps). The genaibook library we\\nmentioned in the Preface has a utility function to select an\\nappropriate device depending on where you run the example\\ncode. For example, the following code will assign cuda to the\\ndevice variable if you have a GPU.\\nfrom genaibook.core import get_device\\ndevice = get_device()\\nprint(f\"Using device: {device}\")\\nUsing device: cuda\\nNext, we’ll load Stable Diﬀusion 1.5. diﬀusers oﬀers a\\nconvenient, high-level wrapper called\\nStableDiffusionPipeline, which is ideal for this use case.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Don’t worry about all the parameters for now - the highlights\\nare:\\nThere are many models with the Stable Diﬀusion\\narchitecture, so we need to specify the one we want to use,\\nwe are going to use Lykon/dreamshaper-8, a high quality\\nﬁne-tune of runwayml/stable-diffusion-v1-5, in this\\ncase.\\nWe need to specify the precision we’ll load the model with.\\nPrecision is something we’ll learn more about later. At a\\nhigh level, models are composed of many parameters\\n(millions or billions of them). Each parameter is a number\\nlearned during training, and we can store these parameters\\nwith diﬀerent levels of precision (in other words, we can\\nuse more bits to store the model). A larger precision allows\\nthe model to store more information, but it also requires\\nmore memory and computation. On the other hand, we can\\nuse a lower precision by setting torch_dtype=float16\\nand use less memory than the default float32. When\\ndoing inference (a fancy way of saying \"executing\" the\\nmodels), using float16 is usually ﬁne.\\nThe ﬁrst time you run this code, it can take a bit: the pipeline\\ndownloads a model of multiple gigabytes, after all! If you load\\nthe pipeline a second time, it will only re-download the model if\\n1 \\n2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='there has been a change in the remote repository  that hosts the\\nmodel on Hugging Face. Hugging Face libraries store the model\\nlocally in a cache, making things much faster for subsequent\\nloads.\\nimport torch\\nfrom diffusers import StableDiffusionPipeline\\npipe = \\nStableDiffusionPipeline.from_pretrained(\\n    \"Lykon/dreamshaper-8\",\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\",\\n).to(device)\\nNow that the model is loaded, we can deﬁne a prompt- the text\\ninput the model will receive. We can then pass the prompt\\nthrough the model and generate our ﬁrst image based on that\\ntext! Try inputting the following prompt:\\nprompt = \"a photograph of an astronaut riding \\na horse\"\\npipe(prompt).images[0]\\n  0%|          | 0/50 [00:00<?, ?it/s]\\n2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 1-1: Image generated in a few seconds with a Stable\\nDiﬀusion text-to-image model.\\nExciting! With a couple of lines of code, we generated a new\\nimage, shown in Figure 1-1. Play with the prompt and generate\\nnew images. You might notice two things. First, running the\\nsame code will generate diﬀerent images each time. This is\\nbecause the diﬀusion process is stochastic in nature, meaning it'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='uses randomness to generate images. We can control this\\nrandomness by setting a seed:\\nimport torch\\ntorch.manual_seed(0)\\nSecond, the generated images are not perfect. They might have\\nartifacts, be blurry, or not match the prompt at all. We’ll explore\\nthese limitations and how to improve the quality of the\\ngenerated images in later chapters. For instance:\\nChapters 4 and 5 dive into all the components behind\\ndiﬀusion models and how to get from text to new images.\\nThey rely on foundational methods, like AutoEncoders –\\nintroduced in Chapter 3-, that can learn eﬃcient\\nrepresentations from input data and reduce the compute\\nrequirements to build diﬀusion and other generative\\nmodels.\\nIn Chapter 7, we’ll learn how to teach new concepts to\\nStable Diﬀusion. For example, we can teach Stable Diﬀusion\\nthe concept of \"my dog\" to generate images of the author’s\\ndog in novel scenarios, such as \"my dog visiting the\\nmoon“.\\nChapter 8 shows how diﬀusion models can be used for\\nmore than just image generation, such as editing images'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='with a prompt or ﬁlling empty parts of an image.\\nGenerating Text\\nJust as diﬀusers is a very convenient library for diﬀusion\\nmodels, the popular transformers library is extremely useful for\\nrunning transformers-based models and adapting to new use\\ncases. It provides a standardized interface for a wide range of\\ntasks, such as generating text, detecting objects in images, and\\ntranscribing an audio ﬁle into text.\\nThe transformers library provides diﬀerent layers of\\nabstractions. For example, if you don’t care about all the\\ninternals, the easiest is to use pipeline, which abstracts all\\nthe processing required to get a prediction. We can instantiate a\\npipeline by calling the pipeline() function and specifying\\nwhich task we want to solve, such as text-classification.\\nfrom transformers import pipeline\\nclassifier = pipeline(\"text-classification\", \\ndevice=device)\\nclassifier(\"This movie is disgustingly good \\n!\")'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='[{\\'label\\': \\'POSITIVE\\', \\'score\\': \\n0.9998536109924316}]\\nThe model correctly predicted that the sentiment in the input\\ntext was positive. By default, the text classiﬁcation pipeline uses\\na sentiment analysis model under the hood, but we can also\\nspecify other transformers-based text classiﬁcation models.\\nSimilarly, we can switch the task to text generation (text-\\ngeneration), with which we can generate new text based on\\nan input prompt. By default, the pipeline uses the GPT-2 model.\\nThe transformer pipeline uses a default maximum number of\\nwords to generate, so don’t be surprised if th output is\\ntruncated. We’ll learn later how to change this.\\nfrom transformers import set_seed\\n# Setting the seed ensures we get the same \\nresults every time we run this code\\nset_seed(10)\\ngenerator = pipeline(\"text-generation\", \\ndevice=device)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='prompt = \"It was a dark and stormy\"\\ngenerator(prompt)[0][\"generated_text\"]\\nIt was a dark and stormy year, and my mind \\nwent blank,\" says the 27-ye\\nar-old, who has become obsessed with art, \\npoetry and music since movin\\ng to France. \"I don\\'t really know why, but \\nthere are things\\nAlthough GPT-2 is not a great model by today’s standards, it\\ngives us an initial example of transformers’ generation\\ncapabilities while using a small model. The same concepts we\\nlearn about GPT-2 can be applied to models such as Llama or\\nMistral, some of the most powerful open-access models (at the\\ntime of writing). Throughout the book, we’ll strike a balance\\nbetween the quality and size of the models. Usually, larger\\nmodels have higher-quality generations. At the same time, we\\nwant people with consumer computers or access to free\\nservices, such as Google Colab, to be able to create new\\ngenerations by running code.\\nChapter 2 will teach you how transformer models work\\nunder the hood. We’ll dive into diﬀerent types of'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='transformer models and how to use them for generating\\ntext.\\nChapter 6 will teach you how to continue training\\ntransformer models with our data for diﬀerent use cases.\\nThis will allow us to make conversational models like those\\nyou might have used with ChatGPT or Bard. We’ll also\\ndiscuss eﬃcient training approaches so you can run the\\ntraining on your computer.\\nGenerating Sound Clips\\nGenerative models are not limited to images and text. Models\\ncan generate videos, short songs, synthetic spoken speech,\\nprotein proposals, and more!\\nChapter 9 deepens into audio-related tasks that can be solved\\nwith Machine Learning, such as transcribing meetings and\\ngenerating sound eﬀects. For now, we can limit ourselves to the\\nnow familiar transformers pipeline and use the small\\nversion of MusicGen, a model released by Meta to generate\\nmusic conditioned on text.\\npipe = pipeline(\"text-to-audio\", \\nmodel=\"facebook/musicgen-small\",'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='device=device)\\ndata = pipe(\"electric rock solo, very \\nintense\")\\nprint(data)\\n{\\'audio\\': array([[[0.12342193, 0.11794732, \\n0.14775363, ..., 0.0265964 ,\\n         0.02168683, 0.03067675]]], \\ndtype=float32), \\'sampling_rate\\': 32000}\\nLater, we’ll learn how audio data is represented and what these\\nnumbers are. Of course, there’s no way for us to print the audio\\nﬁle directly in the book! The best alternative is to show a viewer\\nin our notebook or save the audio to a ﬁle we can play with our\\nfavorite audio application. For example, we can use\\nIPython.display() for this!\\nimport IPython.display as ipd\\ndisplay(ipd.Audio(data[\"audio\"][0], \\nrate=data[\"sampling_rate\"]))'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Ethical and Societal Implications\\nWhile generative models oﬀer remarkable capabilities, their\\nwidespread adoption raises important considerations around\\nethics and societal impact. It’s important to keep them in mind\\nas we explore the capabilities of generative models. Here are a\\nfew key areas to consider:\\nPrivacy and consent: The ability of generative models to\\ngenerate realistic images and videos based on very little\\ndata poses signiﬁcant challenges to privacy. For example,\\ncreating synthetic images from a small set of real images\\nfrom an individual raises questions about using personal\\ndata without consent. It also increases the risk of creating\\ndeepfakes, which can be used to spread misinformation or\\nharm individuals.\\nBias and fairness: Generative models are trained on large\\ndatasets that contain biases. These biases can be inherited\\nand ampliﬁed by the generative models, as we’ll explore in\\nChapter 2. For example, biased datasets used to train image\\ngeneration models may generate stereotypical or\\ndiscriminatory images. It’s important to consider mitigating\\nthese biases and ensure that generative models are used\\nfairly and ethically.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Regulation: Given the potential risks associated with\\ngenerative models, there is a growing call for regulatory\\noversight and accountability mechanisms to ensure\\nresponsible development. This includes transparency\\nrequirements, ethical guidelines, and legal frameworks to\\naddress the misuse of generative models.\\nIt’s important to approach generative models with a thoughtful\\nand ethical mindset. As we explore the capabilities of these\\nmodels, we’ll also consider the ethical implications and how to\\nuse them responsibly.\\nWhere We’ve Been and Where\\nThings Stand\\nGenerative models began decades ago with eﬀorts focused on\\nrule-based systems. As computing power and data availability\\nincreased, generative models evolved to use statistical methods\\nand Machine Learning. With the emergence of deep learning as\\na powerful paradigm in Machine Learning and breakthroughs\\nin the ﬁelds of image and speech recognition, generative\\nmodels have advanced signiﬁcantly. Although invented decades\\nago, Convolutional Neural Networks (CNNs) and Recurrent\\nNeural Networks (RNNs) have become widely popular in the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='last decade. CNNs revolutionized image processing tasks, and\\nRNNs brought sequential data modeling capabilities, enabling\\ntasks like translating text and text generation.\\nThe introduction of Generative Adversarial Networks (GANs) by\\nIan Goodfellow in 2014, and variants such as DCGAN and\\nconditional GANs, brought a new era of generative models.\\nGANs have been used to generate high-quality images and\\napplied to tasks like style transfer, enabling users to apply\\nartistic styles to their images with astonishing realism.\\nAlthough quite powerful, the quality of GANs has been\\nsurpassed by diﬀusion models in recent years.\\nSimilarly, although RNNs were the to-go tool for language\\nmodeling, transformer models, including architectures like GPT,\\nachieved state-of-the-art performance in Natural Language\\nProcessing (NLP). These models have demonstrated remarkable\\ncapabilities in tasks such as language understanding, text\\ngeneration, and machine translation. GPT, in particular, became\\nextremely popular due to its ability to generate coherent and\\ncontextually relevant text. Not long afterward, a huge wave of\\ngenerative language models emerged.\\nThe ﬁeld of generative AI is more accessible than ever due to\\nthe rapid expansion of research, resources, and development in'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='recent years. A growing community interested in the area, a\\nrich open-source ecosystem, and research facilitating\\ndeployment have led to a wide range of applications and use\\ncases. Between 2023 and 2024, a new generation of models that\\ncan generate high-quality images, text, code, videos, and more\\nhas emerged; examples include ChatGPT, DALL·E, Imagen,\\nStable Diﬀusion, Llama, Mistral, and many others.\\nHow Are Generative AI Models\\nCreated?\\nTypically, it comes down to big budgets or open-source.\\nSeveral of the most impressive generative models in the past\\ncouple of years were created by inﬂuential research labs in big,\\nprivate companies. OpenAI developed ChatGPT, DALL·E, and\\nSora; Google built Imagen, Bard, and Gemini; and Meta created\\nLlama and Code Llama. There’s a varying degree of openness in\\nthe way these models are released. Some can be used via\\nspeciﬁc UIs, some have access through developer APIs, and\\nsome are just announced as research reports with no public\\naccess at all. In some cases, code and model weights are\\nreleased as well: these are usually called open-source releases\\nbecause those are the essential artifacts necessary to run the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='model on your hardware. Frequently, however, they are kept\\nhidden for strategic reasons.\\nAt the same time, an ever-increasing, energetic, and\\nenthusiastic community uses open-source models as the clay for\\ntheir creativity. All types of practitioners, including researchers,\\nengineers, tinkerers, and amateurs, build on top of each other’s\\nwork and come up with novel solutions and clever ideas that\\npush the ﬁeld forward, one commit at a time. Some of these\\nideas make their way into the theoretical corpus of knowledge\\nwhere researchers draw from, and new impressive models that\\nuse them come out after a while.\\nBig models, even when hidden, serve as inspiration for the\\ncommunity, whose work yields fruits that serve the ﬁeld as a\\nwhole.\\nThis cycle can only work because some of the models are open-\\nsourced and can be used by the community. Companies that\\nrelease open-source models don’t do it for altruistic reasons but\\nbecause they discover economic value in this strategy. By\\nproviding code and models that are adopted by the community,\\nthey receive public scrutiny with bug ﬁxes, new ideas, derived\\nmodel architectures, or even new datasets that work well with\\nthe models released. Because all these contributions are based'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='on the assets they published, these companies can quickly\\nadopt them and thus move faster than they would on their own.\\nWhen Meta released Llama, one of the most popular language\\nmodels, a thriving ecosystem organically grew around it. Both\\nestablished and new companies alike, including Meta, Stability\\nAI (Stable Diﬀusion), or Mistral AI, have embraced varying\\ndegrees of open-source as part of their business strategy. This is\\nas legitimate as the strategy of competing companies that prefer\\nto keep their trade secrets behind closed doors (even if those\\ncompanies can also draw from the open-source community).\\nAt this point, we’d like to clarify that model releases are rarely\\ntruly open-source. Unlike in the software world, source code is\\nnot enough to fully understand a Machine Learning system.\\nModel weights are not enough either: they are just the ﬁnal\\noutput of the model training process. Being able to exactly\\nreplicate an existing model would require the source code used\\nto train the model (not just the modeling code or the inference\\ncode), the training regime and parameters, and, crucially, all the\\ndata used for training. None of these, and particularly the data,\\nare usually released. If there were access to these details, it\\nwould be possible for the community and the public to\\nunderstand how the model works, explore the biases that may\\naﬄict it, and better assess its strengths and limitations. Access\\nto the weights and model code provides an imperfect'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='estimation of all this knowledge, but the actual hard data would\\nbe much better. On top of it, even when the models are publicly\\nreleased, they often come out with a special license that does\\nnot adhere to the Open Source Initiative’s deﬁnition of open-\\nsource. This is not to say that the models are not useful or that\\nthe companies are not doing a good thing by releasing them,\\nbut it’s an important context to keep in mind and one of the\\nreasons we’ll often say open access instead of open-source.\\nBe it as it may, there has never been a better time to build\\ngenerative models or with generative models. You don’t need to\\nbe an engineer in a top-notch research lab to come up with\\nideas to solve the problems that interest you or to contribute to\\nthe ﬁeld. We hope you ﬁnd these pages helpful in your journey!\\nSummary\\nHopefully, after generating your ﬁrst images, text, and audios,\\nyou’ll be excited to learn how diﬀusion and transformers work\\nunder the hood, how to adapt them for new use cases, and how\\nto use them for diﬀerent creative applications. Although this\\nchapter focused on high-level tools, we’ll build solid\\nfoundations and intuition on how these models work as we'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='embark on our generative journey. Let’s go ahead and learn\\nabout the principles of generative models!\\n You might wonder about the variant parameter. For some models, you might ﬁnd\\nmultiple checkpoints with diﬀerent precision. When specifying\\ntorch_dtype=float16, we download the default model (float32) and convert it\\nto float16. By also specifying the fp16 variant, we’re downloading a smaller\\ncheckpoint already stored in float16 precision, which requires half the bandwidth\\nand storage to download it. Check the model you want to use to ﬁnd out if there are\\nmultiple precision variants.\\n Hugging Face repositories are git-based repositories under the hood\\n1 \\n2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Chapter 2. Transformers\\nA NOTE FOR EARLY RELEASE READERS\\nWith Early Release ebooks, you get books in their earliest form\\n—the authors’ raw and unedited content as they write—so you\\ncan take advantage of these technologies long before the oﬃcial\\nrelease of these titles.\\nThis will be the second chapter of the ﬁnal book. Please note\\nthat the GitHub repo will be made active later on.\\nIf you have comments about how we might improve the content\\nand/or examples in this book, or if you notice missing material\\nwithin this chapter, please reach out to the editor at\\njleonard@oreilly.com.\\nMany trace the most recent wave of advances in generative AI\\nto the introduction of a class of models called transformers in\\n2017. Their most well-known application is the powerful Large\\nLanguage Model (LLM), such as Llama and GPT-4, used by\\nhundreds of millions daily. Transformers have become a\\nbackbone for modern AI applications, powering everything\\nfrom chatbots and search systems to machine translation and'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='content summarization. They’ve even branched out beyond\\ntext, making waves in ﬁelds like computer vision, music\\ngeneration, and protein folding. In this chapter, we’ll explore\\nthe core ideas behind transformers and how they work, with a\\nfocus on one of the most common applications: language\\nmodeling.\\nBefore we dive into the details of transformers, let’s take a step\\nback and understand what language modeling is. At its core, a\\nlanguage model (LM) is a probabilistic model that learns to\\npredict the next word (or token) in a sequence based on the\\npreceding or surrounding words. Doing so captures language’s\\nunderlying structure and patterns, allowing it to generate\\nrealistic and coherent text. For example, given the sentence \"I\\nbegan my day eating“, a language model might predict the\\nnext word as \"breakfast\" with a high probability.\\nSo, how do transformers ﬁt into this picture? Transformers are\\ndesigned to handle long-range dependencies and complex\\nrelationships between words eﬃciently and expressively. For\\nexample, imagine that you want to use an LM to summarize a\\nnews article, which might contain hundreds or even thousands\\nof words. Traditional LMs, such as Recurrent Neural Networks,\\nstruggle with long contexts, so the summary might skip critical\\ndetails from the beginning of the article. Transformer-based'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='LMs, however, show strong results in this task. Besides high-\\nquality generations, transformers have other properties, such\\nas eﬃcient parallelization of training, scalability, and\\nknowledge transfer, making them popular and well-suited for\\nmultiple tasks. At the heart of this innovation lies a mechanism\\ncalled self-attention, which allows the model to weigh the\\nimportance of each word in the context of the entire sequence.\\nTo help us build intuition about how language models work,\\nwe’ll use code examples that interact with existing models, and\\nwe’ll describe the relevant pieces as we ﬁnd them. Let’s get to it.\\nA Language Model in Action\\nIn this section, we will load and interact with an existing small\\n(pre-trained) transformer model to get a high-level\\nunderstanding of how they work. In recent years, companies,\\nresearch labs, and open communities have released thousands\\nof open models that you can use. These models are composed\\nfrom\\nWe’ll pick a small model you can run directly in your hardware,\\nbut consider that the same principles apply to the larger (over\\n100 times larger!) and more powerful models that have since\\nbeen released. Some good examples of small models are:'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='GPT-2 (137M), which made headlines in 2019 for its (then)\\nimpressive text-generation capabilities. Although small and\\nalmost quaint by today’s standards, GPT-2 illustrates how\\nthese language models work.\\nQwen2 (494M) is an Alibaba model and part of the Qwen\\nfamily. The Qwen family has models with 500 million to\\nover 100 billion parameters.\\nSmolLM (135M) is a model by Hugging Face trained with\\nvery high-quality data. The authors released models with\\n135 million, 360 million, and 1.7 billion parameters.\\nChapter 6 will provide more insights into picking the suitable\\nmodel for your use case. For now, we suggest exploring one of\\nthe models above (or all of them!).\\nTokenizing Text\\nLet’s begin our journey to generate some text based on an initial\\ninput. For example, given the phrase \"it was a dark and\\nstormy“, we want the model to generate some words to\\ncontinue it. Models can’t receive text directly as input; their\\ninput must be data represented as numbers. To feed text into a\\nmodel, we must ﬁrst ﬁnd a way to turn sequences into\\nnumbers. This process is called tokenization, a crucial step in\\nany Natural Language Processing (NLP) pipeline.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='An easy option would be to split the text into individual\\ncharacters and assign each a unique numerical ID. This scheme\\ncould be helpful for languages such as Chinese, where each\\ncharacter carries much information. In languages like English,\\nthis creates a very small token vocabulary, and there will be\\nvery few unknown tokens (characters not found during\\ntraining) when running inference. However, this method\\nrequires many tokens to represent a string, which is bad for\\nperformance and erases some of the structure and meaning of\\nthe text – a downside for accuracy. Each character carries very\\nlittle information, making it hard for the model to learn the\\nunderlying structure of the text.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 2-1. Figure 2-1. An example of character-level tokenization. Each letter has its\\nown ID, with all instances of the same letter having the same ID. In this example, the\\nIDs correspond to their position in the alphabet.\\nAnother approach could be to split the text into individual\\nwords. While this lets us capture more meaning per token, it\\nhas the downsides that we need to deal with more unknown\\nwords (e.g., typos or slang), we need to deal with diﬀerent\\nforms of the same word (e.g., \"run“, \"runs“, and \"running“),\\nand we might end with a very large vocabulary, which could\\neasily be over half a million words for languages such as\\nEnglish.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 2-2. Figure 2-2. An example of word-level tokenization. The same word always\\nhas the same ID.\\nModern tokenization strategies strike a balance between these\\ntwo extremes, splitting the text into subwords that capture both\\nthe structure and meaning of the text while still being able to\\nhandle unknown words and diﬀerent forms of the same word.\\nCharacters that are usually found together (like most frequent\\nwords) can be assigned a single token that represents the whole\\nword or group. Long or complicated words, or words with\\nmany inﬂections, may be split into multiple tokens, where each\\none usually represents a meaningful section of the word. There\\nis no single best tokenizer; each language model comes with its\\nown one. The diﬀerences between tokenizers reside in the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='number of tokens supported and the tokenization strategy. For\\nexample, the GPT-2 tokenizer averages 1.3 tokens per word.\\nFigure 2-3. Figure 2-3. An example of sub-word tokenization. Llama is split in two\\ntokens because it was likely not a common word in the data used to create the\\ntokenizer.\\nLet’s ﬁnd out how the Qwen tokenizer handles a sentence. We’ll\\nﬁrst use the transformers library to load the tokenizer\\ncorresponding to Qwen. Then, we’ll run the input text (also\\ncalled prompt) through the tokenizer to encode the string into\\nnumbers representing the tokens. We’ll use the decode()\\nmethod to convert each ID back into its corresponding token for\\ndemonstration purposes.\\n1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from transformers import AutoTokenizer\\n# Use the id of the model you want to use\\n# GPT-2 \"openai-community/gpt2\"\\n# Qwen \"Qwen/Qwen2-0.5B\"\\n# SmolLM \"HuggingFaceTB/SmolLM-135M\"\\nprompt = \"It was a dark and stormy\"\\ntokenizer = \\nAutoTokenizer.from_pretrained(\"Qwen/Qwen2-\\n0.5B\")\\ninput_ids = tokenizer(prompt).input_ids\\ninput_ids\\n[2132, 572, 264, 6319, 323, 13458, 88]\\nfor t in input_ids:\\n    print(t, \"\\\\t:\", tokenizer.decode(t))\\n2132    : It\\n572     :  was\\n264     :  a\\n6319    :  dark\\n323     :  and'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='13458   :  storm\\n88  : y\\nAs shown, the tokenizer splits the input string into a series of\\ntokens and assigns a unique ID to each. Most words are\\nrepresented by a single token, but when using Qwen and GPT-2\\ntokenizers, \"stormy\" is represented by two tokens: one for ”\\nstorm” (including the space before the word) and one for the\\nsuﬃx \"y“. This allows the model to learn that \"stormy\" is\\nrelated to \"storm\" and that the suﬃx \"y\" is often used to turn\\nnouns into adjectives. On the other hand, the SmolLM tokenizer\\ndoes not split any of the words in this particular sentence. Each\\nmodel is usually paired with its own tokenizer, so always use\\nthe proper tokenizer when using a model. The three models of\\nthis section have vocabularies that go from 50,000 to 150,000\\ntokens, which allows them to represent almost any input text.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='NOTE\\nEven though we usually talk about training tokenizers, this has nothing to do with\\ntraining a model. Model training is stochastic (non-deterministic) by nature, whereas\\nwe train a tokenizer using a statistical process that identiﬁes which subwords are the\\nbest to pick for a given dataset. How to choose the subwords is a design decision of\\nthe tokenization algorithm. Therefore, tokenization training is deterministic. We\\nwon’t dive into diﬀerent tokenization strategies, but some of the most popular\\nsubword approaches are Byte-level BPE, used in GPT-2, WordPiece, and\\nSentencePiece.\\nPredicting Probabilities\\nGPT-2, Qwen, and SmolLM were trained as causal language\\nmodels (also known as auto-regressive), which means they were\\ntrained to predict the next token in a sequence given the\\npreceding tokens. The transformers library has high-level tools\\nthat enable us to use such a model to generate text or perform\\nother tasks quickly. It is helpful to understand how the model\\nmakes its predictions by directly inspecting them on this\\nlanguage-modeling task. We begin by loading the model.\\nfrom transformers import AutoModelForCausalLM\\nmodel ='),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-\\n0.5B\")\\nNote the use of AutoTokenizer and\\nAutoModelForCausalLM. The transformers library supports\\nhundreds of models and their corresponding tokenizers. Rather\\nthan having to learn the name of each tokenizer and model\\nclass, we will use AutoTokenizer and AutoModelFor*. For\\nthe automatic model class, we need to specify for which task\\nwe’re using the model, such as classiﬁcation\\n(AutoModelForSequenceClassification) or object\\ndetection (AutoModelForObjectDetection). In the case of\\nQwen2, we’ll use the class corresponding to the causal language\\nmodeling task. When using the automatic classes, transformers\\nwill pick an adequate default class based on the conﬁguration\\nof a model. For example, under the hood, transformers will use\\nQwen2Tokenizer and Qwen2ForCausalLM.\\nIf we feed the tokenized sentence from the previous section\\nthrough the model, we get a result with 151,936 values for each\\ntoken in the input string:\\n# We tokenize again but specifying the \\ntokenizer that we want it to\\n# return a PyTorch tensor, which is what the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='model expects,\\n# rather than a list of integers\\ninput_ids = tokenizer(prompt, \\nreturn_tensors=\"pt\").input_ids\\noutputs = model(input_ids)\\noutputs.logits.shape  # An output for each \\ninput token\\ntorch.Size([1, 7, 151936])\\nThe ﬁrst dimension of the output is the number of batches (1\\nbecause we just ran a single sequence through the model). The\\nsecond dimension is the sequence length, or the number of\\ntokens in the input sequence (7 in this case). The third\\ndimension is the vocabulary size. We get a list of ~150 thousand\\nnumbers for each token in the original sequence. These are the\\nraw model outputs, or logits, that correspond to the tokens in\\nthe vocabulary. For every input token, the model predicts how\\nlikely each token in the vocabulary is to continue the sequence\\nup to that point. With our example sentence, the model will\\npredict logits for \"It“, \"It was“, \"It was a“, and so on.\\nHigher logits’ values mean the model considers the\\ncorresponding token a more likely continuation of the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='sequence. Table 2-1 shows the input sequences, the most likely\\ntoken ID, and its corresponding token.\\nNOTE\\nLogits are the raw output of the model (a list of numbers such as [0.1, 0.2, 0.01, …]).\\nWe can use the logits to select the most likely token to continue the sequence.\\nHowever, we can also convert the logits into probabilities, as we’ll do soon.\\nInput Sequence ID of most likely\\nnext token\\nCorresponding\\ntoken\\nIt 374 is\\nIt was 264 a\\nIt was a 2244 great\\nIt was a dark 323 and\\nIt was a dark and 13458 storm\\nIt was a dark and 88 y\\nIt was a dark and\\nstormy\\n3729 (let’s ﬁgure this\\none)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Table 2-1. The most likely token to continue input sequences\\naccording to the Qwen2 model.\\nLet’s focus on the logits for the entire prompt and learn how to\\npredict the next word of the sequence. We can ﬁnd the index of\\nthe token with the highest value using the argmax() method:\\nfinal_logits = model(input_ids).logits[0, -1]  \\n# The last set of logits\\nfinal_logits.argmax()  # The position of the \\nmaximum\\ntensor(3729)\\n3729 corresponds to the ID of the token the model considers\\nmost likely to follow the input string \"It was a dark and\\nstormy“. Decoding this token, we can ﬁnd out that this model\\nknows a few story tropes:\\ntokenizer.decode(final_logits.argmax())\\n\\' night\\''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='So ” night” is the most likely token. This makes sense\\nconsidering the beginning of the sentence we provided as input.\\nThe model learns how to pay attention to other tokens using a\\nmechanism called self-attention, which is the fundamental\\nbuilding block of transformers. Intuitively, self-attention allows\\nthe model to identify how much each token contributes to the\\nmeaning of the sequence.\\nNOTE\\nTransformer models contain many of these attention layers, each one specializing in\\nsome aspect of the input. Contrary to heuristics systems, these aspects or features are\\nlearned during training, instead of being speciﬁed beforehand.\\nLet’s now ﬁnd out which other tokens were potential candidates\\nby selecting the top 10 values with topk():\\nimport torch\\ntop10_logits = torch.topk(final_logits, 10)\\nfor index in top10_logits.indices:\\n    print(tokenizer.decode(index))\\n night\\n evening'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='day\\n morning\\n winter\\n afternoon\\n Saturday\\n Sunday\\n Friday\\n October\\nWe need to convert logits into probabilities to better\\nunderstand how conﬁdent the model is about each prediction.\\nWe can do that by comparing each value with all the other\\npredicted values and normalizing so all the numbers sum up to\\n1. That’s precisely what the softmax() operation does. The\\nfollowing code uses softmax() to print out the top 10 most\\nlikely tokens and their associated probabilities according to the\\nmodel:\\ntop10 = \\ntorch.topk(final_logits.softmax(dim=0), 10)\\nfor value, index in zip(top10.values, \\ntop10.indices):\\n    print(f\"{tokenizer.decode(index):<10} \\n{value.item():.2%}\")\\n2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='night     88.71%\\n evening   4.30%\\n day       2.19%\\n morning   0.49%\\n winter    0.45%\\n afternoon 0.27%\\n Saturday  0.25%\\n Sunday    0.19%\\n Friday    0.17%\\n October   0.16%\\nBefore going further, we suggest you to experiment with the\\ncode above. Here are some ideas for you to try:\\nChange a few words: Try changing the adjectives (e.g.,\\n\"dark\" and \"stormy“) in the input string and ﬁnd out\\nhow the model’s predictions change. Is the predicted word\\nstill \"night“? How do the probabilities change?\\nChange the input string: Try diﬀerent input strings and\\nanalyze how the model’s predictions change. Do you agree\\nwith the model’s predictions?\\nGrammar: What happens if you provide a string that is not\\na grammatically correct sequence? How does the model\\nhandle it? Look at the probabilities of the top predictions.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Generating Text\\nOnce we know how to get the model’s predictions for the next\\ntoken in a sequence, it is easy to generate text by repeatedly\\nfeeding the model’s predictions back into itself. We can call\\nmodel(ids), generate a new token ID, add it to the list, and\\ncall the function again. To make it more convenient to generate\\nmultiple words, transformers auto-regressive models have a\\ngenerate() method ideal for this case. Let’s explore an\\nexample.\\noutput_ids = model.generate(input_ids, \\nmax_new_tokens=20)\\ndecoded_text = \\ntokenizer.decode(output_ids[0])\\nprint(\"Input IDs\", input_ids[0])\\nprint(\"Output IDs\", output_ids)\\nprint(f\"Generated text: {decoded_text}\")\\nInput IDs tensor([ 2132,   572,   264,  6319,   \\n323, 13458,    88])\\nOutput IDs tensor([ 2132,   572,   264,  \\n6319,   323, 13458,    88,  3729,    13,\\n          576, 12884,   572,  6319,   323,   \\n279,  9956,   572,  1246,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='2718,    13,   576, 11174,   572, \\n50413,  1495,   323,   279])\\nGenerated text: It was a dark and stormy \\nnight. The sky was dark and t\\nhe wind was howling. The rain was pouring \\ndown and the\\nWhen we ran the model() forward method in the previous\\nsection, it returned a list of logits for each token in the\\nvocabulary (151936). Then, we had to calculate the probabilities\\nand pick the most likely token. generate() abstracts this logic\\naway. It makes multiple forward passes, predicts the next token\\nrepeatedly, and appends it to the input sequence. generate()\\nprovides us with the token IDs of the ﬁnal sequence, including\\nboth the input and new tokens. Then, with the tokenizer\\ndecode() method, we can convert the token IDs back to text.\\nThere are many possible strategies to perform generation. The\\none we just did, picking the most likely token, is called greedy\\ndecoding. Although this approach is straightforward, it can\\nsometimes lead to suboptimal outcomes, especially in\\ngenerating longer text sequences. Greedy decoding can be\\nproblematic because it doesn’t consider the overall probability\\nof a sentence, focusing only on the immediate next word. For\\ninstance, given the starting word Sky and the choices blue'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='and rockets for the next word, greedy decoding might favor\\nSky blue since blue initially appears more likely following\\nSky. However, this approach might overlook a more coherent\\nand probable overall sequence like Sky rockets soar.\\nTherefore, greedy decoding can sometimes miss out on the most\\nlikely overall sequence, leading to less optimal text generation.\\nFigure 2-4. Figure 2-4. Another greedy decoding example. The dog barks is\\ngenerated because dog is the most likely second token with a probability of 0.5, and\\nbarks is the most likely token following The dog\\nRather than one token at a time, techniques such as beam\\nsearch explore multiple possible continuations of the sequence\\nand return the most likely sequence of continuations. It keeps'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='the most likely num_beams of hypotheses during generation\\nand chooses the most likely one.\\nFigure 2-5. Figure 2-5. A beam search example which ﬁnds a more likely sequence.\\nThe dog barks has a total probability of 0.50.4=0.2, while The horse runs has a\\nprobability of 0.40.9=0.36\\nbeam_output = model.generate(\\n    input_ids,\\n    num_beams=5,\\n    max_new_tokens=30,\\n)\\nprint(tokenizer.decode(beam_output[0]))'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='It was a dark and stormy night. The wind was \\nhowling, and the rain was\\n pouring down. The sky was dark and gloomy, \\nand the air was filled wit\\nh the\\nDepending on the model, you might get somewhat repetitive\\nresults. Although not frequently used, there are multiple\\nparameters one can control to perform less-repetitive\\ngenerations. Let’s consider two examples:\\nrepetition_penalty - how much to penalize already\\ngenerated tokens, avoiding repetition. A good default value\\nis 1.2.\\nbad_words_ids - a list of tokens that should not be\\ngenerated (e.g., to avoid generating oﬀensive words).\\nLet’s explore what we can achieve by penalizing repetition:\\nbeam_output = model.generate(\\n    input_ids,\\n    num_beams=5,\\n    repetition_penalty=2.0,\\n    max_new_tokens=38,\\n)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='print(tokenizer.decode(beam_output[0]))\\nIt was a dark and stormy night. The sky was \\nfilled with thunder and li\\nghtning, and the wind howled in the distance. \\nIt was raining cats and\\ndogs, and the streets were covered in puddles \\nof water.\\nWhich generation strategy to use? As often is true in Machine\\nLearning… it depends. Beam search works well when the\\ndesired length of the text is somewhat predictable. This is the\\ncase for tasks such as summarization or translation but not for\\nopen-ended generation, where the output length can vary\\ngreatly, leading to repetition. Although we can inhibit the model\\nto avoid repeating itself, doing so can also lead to it performing\\nworse. Also note that beam search will be slower than greedy\\nsearch as it needs to run inference for multiple beams\\nsimultaneously, which can be an issue for large models.\\nWhen we generate with greedy search and beam search, we\\npush the model to generate text with a distribution of high-\\nprobability next words, as can be seen in Figure 2-6 .\\nInterestingly, high-quality human language does not follow a\\n3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='similar distribution. Human text tends to be more\\nunpredictable. An excellent paper about this counter-intuitive\\nobservation is The Curious Case of Neural Text Degeneration.\\nThe authors conjecture that human language disfavors\\npredictable words - people optimize against stating the obvious.\\nThe paper proposes a method called nucleus sampling.\\nBefore discussing nucleus sampling, let’s discuss sampling. With\\nsampling, we pick the next token by sampling from the\\nprobability distribution of the next tokens. This means that\\nsampling is not a deterministic generation process. If the next\\npossible tokens are night (60%), day (35%), and apple (5%),\\nrather than choosing night (with greedy search), we will sample\\nfrom the distribution. In other words, there will be a 5% chance\\nof picking \"apple\" even if it’s a low-probability token and\\nleads to a nonsensical generation. Sampling avoids creating\\nrepetitive text, hence leading to more diverse generations.\\nSampling is done in transformers using the do_sample\\nparameter.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"Figure 2-6. Figure 2-6. Greedy generation will always pick the most likely next token,\\nwhile sampling will pick the next token by sampling from the probability\\ndistribution.\\nfrom transformers import set_seed\\n# Setting the seed ensures we get the same \\nresults every time we run this code\\nset_seed(70)\\nsampling_output = model.generate(\\n    input_ids,\\n    do_sample=True,\\n    max_new_tokens=34,\\n    top_k=0,  # We'll come back to this \\nparameter\\n)\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='print(tokenizer.decode(sampling_output[0]))\\nIt was a dark and stormy night. Kevin said he \\nwas going to stay up all\\n night, staring at the cloudless stars, \\nwondering, what if I lost my d\\nream.He’d been teasing her about\\nWe can manipulate the probability distribution before we\\nsample from it, making it sharper or ﬂatter using a\\ntemperature parameter. A temperature higher than one will\\nincrease the randomness of the distribution, which we can use\\nto encourage generation of less probable tokens. A temperature\\nbetween 0 and 1 will reduce the randomness, increasing the\\nprobability of the more likely tokens and avoiding predictions\\nthat might be too unexpected. A temperature of 0 will move all\\nthe probability to the most likely next token, which is\\nequivalent to greedy decoding, as can be seen in Figure 2-7.\\nCompare the eﬀect of this temperature parameter on the\\ngenerated text in the following example.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 2-7. Figure 2-7. Eﬀect of temperature on the token probability distribution.\\nsampling_output = model.generate(\\n    input_ids,\\n    do_sample=True,\\n    temperature=0.4,\\n    max_new_tokens=40,\\n    top_k=0,\\n)\\nprint(tokenizer.decode(sampling_output[0]))\\nIt was a dark and stormy night in 1878. The \\nonly light was the moon, a\\nnd the only sound was the distant roar of the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='thunder. The only thing\\nthat could be heard was the sound of the \\nstorm\\nsampling_output = model.generate(\\n    input_ids,\\n    do_sample=True,\\n    temperature=0.001,\\n    max_new_tokens=40,\\n    top_k=0,\\n)\\nprint(tokenizer.decode(sampling_output[0]))\\nIt was a dark and stormy night. The sky was \\ndark and the wind was howl\\ning. The rain was pouring down and the \\nlightning was flashing. The sky\\n was dark and the wind was howling. The rain \\nwas pouring down\\nsampling_output = model.generate(\\n    input_ids,\\n    do_sample=True,\\n    temperature=3.0,\\n    max_new_tokens=40,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='top_k=0,\\n)\\nprint(tokenizer.decode(sampling_output[0]))\\nIt was a dark and stormy 清晨一步\\n            女人 们 都 BL 咻 任 何 时 候  ﺗﺺ ﺑﻦ \\nattendees*sinruitment โ ล ก uresindi ambassa\\ndors eventData 原来是 ехалENCE hemisphere \\nworldsω.Anyar 久 �  Sous dapat HVışís\\ndía.inventory emptiedfuncpping {\\\\Sex\\nWell, the ﬁrst test is much more coherent than the second one.\\nThe second, which uses a very low temperature, is repetitive\\n(similar to greedy decoding). Finally, the third sample, with an\\nextremely high temperature, gives gibberish text.\\nOne parameter you likely noticed is top_k. What is it? Top-K\\nsampling is a simple sampling approach in which only the K\\nmost likely next tokens are considered. For example, using\\ntop_k=5, the generation method will ﬁrst ﬁlter the most likely\\nﬁve tokens and redistribute the probabilities so they add to one.\\nsampling_output = model.generate(\\n    input_ids,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='do_sample=True,\\n    max_new_tokens=40,\\n    top_k=10,\\n)\\nprint(tokenizer.decode(sampling_output[0]))\\nIt was a dark and stormy night in New York. \\nThe city was on the brink\\nof a violent storm. The sky above was painted \\nwith a mix of bright red\\n and orange. It was a sign, but the storm had \\narrived\\nHmm…this could be better. An issue with Top-K Sampling is that\\nthe number of relevant candidates in practice could vary\\ngreatly. If we deﬁne top_k=5, some distributions will still\\ninclude tokens with very low probability, while others will\\nconsist of only high-probability tokens.\\nThe ﬁnal generation strategy we’ll visit is Top-p sampling (also\\nknown as nucleus sampling). Rather than sampling the K words\\nwith the highest probability, we will use all the most likely\\nwords whose cumulative probability exceeds a given value. If\\nwe use a top_p=0.94, we’ll ﬁrst ﬁlter only to keep the most'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='likely words that cumulatively have a probability of 0.94 or\\nhigher, as can be observed in Figure 2-8. We then redistribute\\nthe probability and do regular sampling. Let’s check this out in\\naction.\\nFigure 2-8. Figure 2-8. Eﬀect of top_k and top_p on the token probability distribution.\\nsampling_output = model.generate(\\n    input_ids,\\n    do_sample=True,\\n    max_new_tokens=40,\\n    top_p=0.94,\\n    top_k=0,\\n)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='print(tokenizer.decode(sampling_output[0]))\\nIt was a dark and stormy night in the skies \\nof Morrowind, and a partic\\nularly ruthless fighter had decided that \\nthese careless tourists at Ca\\nrrabine should be dealt with with maximum \\ncruelty. The chief of this i\\nmportant operation was appointed by\\nBoth Top-K and Top-p are commonly used in practice. They can\\neven be combined to ﬁlter out low-probability words but have\\nmore generation control. The issue with the stochastic\\ngeneration methods is that the generated text doesn’t\\nnecessarily contain coherence.\\nWe’ve explored three diﬀerent generation methods: greedy\\nsearch, beam-search decoding, and sampling (with\\ntemperature, Top-K, and Top-p providing further control).\\nThose are lots of approaches! If you feel underwhelmed by the\\nmodel generations, consider that it’s a model with few hundred\\nmillion parameters - the fact that it can generate coherent text\\nis already impressive! New models spawn billions or even\\nhundreds of billions of parameters and are trained with higher-'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='quality data, so switching to a more modern or larger model\\nwill lead to better results. In the public resources of the book,\\nyou can ﬁnd an online interactive demo to visualize how\\ntemperature, top_p, and top_k impact a generation\\ndistribution.\\nIf you want to experiment with generation further, here are\\nsome suggestions:\\nExperiment with diﬀerent parameter values. How does\\nincreasing the number of beams impact the quality of your\\ngeneration? What happens if you reduce or increase your\\ntop_p value?\\nOne approach to reduce repetition in Beam Search is\\nintroducing penalties for n-grams (word sequence of n\\nwords). This can be conﬁgured using\\nno_repeat_ngram_size, which avoids repeating the\\nsame n-gram. For example, if you use\\nno_repeat_ngram_size=4, the generation will never\\ncontain the exact four consecutive words.\\nTop-K can lead to discarding high-quality tokens and Top-p\\ncan lead to including some low-probability tokens. For a\\nmore dynamic appoach, you can use Min-P (min_p), which\\nmultiplies min_p times the top token’s probability, and\\nthen only include tokens above that percentage. In other'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='words, Min-P deﬁnes a dynamic threshold based on the top\\ntoken’s probability\\nA newer method, contrastive search, can generate long,\\ncoherent output while avoiding repetition. This is achieved\\nby considering both the probabilities predicted by the\\nmodel and the similarity with the context. This can be\\ncontrolled via penalty_alpha and top_k.\\nIf all of this sounds too empirical, it’s because it is. Generation is\\nan active area of research, with new papers coming up with\\ndiﬀerent proposals, such as more sophisticated ﬁltering. We’ll\\nbrieﬂy discuss these in the ﬁnal chapter. No single rule works\\nfor all models, so it’s always important to experiment with\\ndiﬀerent techniques.\\nZero-Shot Generalization\\nGenerating language is a fun and exciting application of\\ntransformers, but writing fake articles about unicorns  is not\\nthe reason why they are so popular. To predict the next token\\nwell, these models must learn a fair amount about the world.\\nWe can take advantage of this to perform various tasks. For\\nexample, instead of training a model dedicated to translation,\\nwe can prompt a suﬃciently powerful language model with an\\ninput like:\\n4 \\n5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Translate the following sentence from English \\nto French:\\nInput: The cat sat on the mat.\\nTranslation:\\nI typed this example with GitHub Copilot active, and it helpfully\\nsuggested \"Le chat était assis sur le tapis\" as a\\ncontinuation of the above prompt - a perfect illustration of how\\na language model can perform tasks not explicitly trained for.\\nThe more powerful the model, the more tasks it can perform\\nwithout additional training. This ﬂexibility makes transformers\\nquite powerful and has made them so popular in recent years.\\nTo check this in action for ourselves, let’s use Qwen as a\\nclassiﬁcation model. Speciﬁcally, we’ll classify movie reviews as\\npositive or negative - a classic benchmark task in the NLP ﬁeld.\\nWe’ll use a zero-shot approach to make things interesting,\\nwhich means we won’t provide the model with any labeled\\ndata. Instead, we’ll prompt the model with the text of a review\\nand ask it to predict the sentiment.\\nThere are multiple ways we can use a generative model as a\\nclassiﬁer. Usually, we begin by inserting the movie review into a\\nprompt template that provides context for the model. This\\nprompt template could instruct the model to simply return the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='sentiment of the review (and ask to limit as positive or\\nnegative). An alternative trick, specially needed with a small\\nmodel like GPT-2 or the small Qwen, is to look at its prediction\\nfor the next token and ﬁnd out which possible token is assigned\\na higher probability: positive or negative? Let’s go with\\nthis approach and ﬁnd the IDs corresponding to those tokens.\\n# Check the token IDs for the words \\' \\npositive\\' and \\' negative\\'\\n# (note the space before the words)\\ntokenizer.encode(\" positive\"), \\ntokenizer.encode(\" negative\")\\n([6785], [8225])\\nOnce we have the IDs, we can now run inference with the\\nmodel and generate a label based on the probabilities:\\ndef score(review):\\n    \"\"\"Predict whether it is positive or \\nnegative\\n    This function predicts whether a review \\nis positive or negative\\n    using a bit of clever prompting. It looks'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='at the logits for the\\n    tokens \\' positive\\' and \\' negative\\', and \\nreturns the label\\n    with the highest score.\\n    \"\"\"\\n    prompt = f\"\"\"Question: Is the following \\nreview positive or\\nnegative about the movie?\\nReview: {review} Answer:\"\"\"\\n    input_ids = tokenizer(prompt, \\nreturn_tensors=\"pt\").input_ids  \\n    final_logits = model(input_ids).logits[0, \\n-1]  \\n    if final_logits[6785] > \\nfinal_logits[8225]:  \\n        print(\"Positive\")\\n    else:\\n        print(\"Negative\")\\nTokenize the prompt\\nGet the logits for each token in the vocabulary. Note that\\nwe’re using model() rather than model.generate(), as\\nmodel() returns the logits for each token in the vocabulary,\\nwhile model.generate() returns only the chosen token.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Check if the logit for the positive token is higher than the\\nlogit for the negative token.\\nWe can try out this zero-shot classiﬁer on a few fake reviews to\\nevaluate how it does:\\nscore(\"This movie was terrible!\")\\nNegative\\nscore(\"That movie was great!\")\\nPositive\\nscore(\"A complex yet wonderful film about the \\ndepravity of man\")  # A mistake\\nNegative\\nIn the GitHub repo for this book, you’ll ﬁnd a dataset of labeled\\nreviews and code to assess the accuracy of this zero-shot\\napproach. Can you tweak the prompt template to improve the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='model’s performance? Can you think of other tasks that could\\nbe performed using a similar approach?\\nThe zero-shot capabilities of recent models have been a game-\\nchanger. As the models improve, they can perform more tasks\\nout-of-the-box, making them more accessible and easier to use\\nand reducing the need for specialized models for each task.\\nFew-Shot Generalization\\nDespite the release of ChatGPT and the quest for the perfect\\nprompts, zero-shot generalization (or prompt tuning) is not the\\nonly way to bend powerful language models to perform\\narbitrary tasks.\\nZero-shot is the extreme application of a technique called few-\\nshot generalization, in which we provide the language model a\\nfew examples about the task we want it to perform and then\\nask it to provide similar answers for us. Instead of training the\\nmodel, we show some examples to inﬂuence generation by\\nincreasing the probability that the continuation text follows the\\nsame structure and pattern as our prompt. Let’s try an example.\\nApart from providing examples, providing a short description\\nof what the model should do, e.g., \"Translate English to\\nFrench“, will help with higher-quality generations.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='prompt = \"\"\"\\\\\\nTranslate English to Spanish:\\nEnglish: I do not speak Spanish.\\nSpanish: No hablo español.\\nEnglish: See you later!\\nSpanish: ¡Hasta luego!\\nEnglish: Where is a good restaurant?\\nSpanish: ¿Dónde hay un buen restaurante?\\nEnglish: What rooms do you have available?\\nSpanish: ¿Qué habitaciones tiene disponibles?\\nEnglish: I like soccer\\nSpanish:\"\"\"\\ninputs = tokenizer(prompt, \\nreturn_tensors=\"pt\").input_ids\\noutput = model.generate(\\n    inputs,\\n    max_new_tokens=10,\\n)\\nprint(tokenizer.decode(output[0]))'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Translate English to Spanish:\\nEnglish: I do not speak Spanish.\\nSpanish: No hablo español.\\nEnglish: See you later!\\nSpanish: ¡Hasta luego!\\nEnglish: Where is a good restaurant?\\nSpanish: ¿Dónde hay un buen restaurante?\\nEnglish: What rooms do you have available?\\nSpanish: ¿Qué habitaciones tiene disponibles?\\nEnglish: I like soccer\\nSpanish: Me gusta el fútbol\\nEnglish:\\nWe state the task we want to achieve and provide four\\nexamples to set the context for the model. Hence, this is a 4-shot\\ngeneralization task. Then, we ask the model to generate more\\ntext to follow the pattern and provide the requested translation.\\nSome ideas to explore:\\nWould this work with fewer examples?\\nWould it work without the task description?'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='How about other tasks?\\nHow does GPT-2 and SmolLM score in this setting?\\nNOTE\\nGPT-2, given its size and training process, is not very good at few-shot tasks, and it’s\\neven worse at zero-shot generalization. How is it possible you could have used it for\\nsentiment classiﬁcation as in the previous section? We cheated a bit: we didn’t look at\\nthe text generated by the model, but just checked whether the probability for ”\\npositive” was larger than the ” negative ” probability. Understanding how models\\nwork under the hood can unlock powerful applications even with small models.\\nRemember to think about your problem; don’t be afraid to explore.\\nGPT-2 and Qwen 0.5B are examples of base models. Some base\\nmodels in the style of Qwen have zero-shot and few-shot\\ncapabilities that we can use at inference time. Another approach\\nis to ﬁne-tune a model: we take the base model and keep\\ntraining it a bit longer on domain or task-speciﬁc data. We don’t\\nalways need the extreme generalization capabilities showcased\\nby the most powerful models in the world; if you only want to\\nsolve a particular task, it will usually be cheaper and better to\\nﬁne-tune and deploy a smaller model specialized on a single\\ntask.\\nIt’s also important to note that base models are not\\nconversational; although you can write a very nice prompt'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='that will help make a chatbot with a base model, it’s often more\\nconvenient to ﬁne-tune the base model with conversational\\ndata, hence improving the conversational capabilities of the\\nmodel. That’s precisely what we’ll do in Chapter 6. Recent LLM\\nreleases tend to include both a base model and an oﬃcial model\\nwith conversational capabilities. In the case of the small Qwen\\nmodel, this would be Qwen2-0.5B-Instruct.\\nA Transformer Block\\nAfter our brief experiments using language models, we are\\nready to introduce an architecture diagram for transformer-\\nbased language generation models shoen in Figure 2-9.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 2-9. Figure 2-9. Architecture of a transformer-based language model.\\nThe high-level pieces involved include:'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Tokenization. The input text is broken down into\\nindividual tokens. Each token has a corresponding ID used\\nto index the token embeddings.\\nInput Token Embedding. The tokens are represented as\\nvectors called embeddings. These embeddings serve as\\nnumerical representations that capture basic information\\nof each token. You can think of vectors as (a long) list of\\nnumbers, where each number corresponds to a particular\\naspect of the token’s meaning. During training, a model\\nlearns how to map each token to its corresponding\\nembedding. The token embedding will always be the same\\nfor each token, regardless of its position in the input\\nsequence.\\nPositional Encoding. The transformer model has no notion\\nof order, so we need to enrich the token embeddings with\\npositional information. This is done by adding a positional\\nencoding to the token embeddings. The positional encoding\\nis a set of vectors that encode the position of each token in\\nthe input sequence. This allows the model to diﬀerentiate\\nbetween tokens based on their position in the sequence,\\nwhich can be useful as the same token appearing in\\ndiﬀerent places can have diﬀerent meanings.\\nTransformer blocks: The core of the transformer model is\\nthe transformer block. The power of transformers comes'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from stacking multiple blocks, allowing the model to learn\\nincreasingly complex and abstract relationships between\\nthe input tokens. It consists of two main components:\\nSelf-Attention Mechanism. This mechanism allows\\nthe model to weigh the importance of each token in the\\ncontext of the entire sequence. It helps the model\\nunderstand the relationships between diﬀerent tokens\\nin the input. The self-attention mechanism is the key to\\nthe transformer’s ability to handle long-range\\ndependencies and complex relationships between\\nwords, and it helps generate coherent and contextually\\nappropriate text.\\nFeed-Forward Neural Network. The self-attention\\noutput is passed through a feed-forward neural\\nnetwork, which further reﬁnes the representation of\\nthe input sequence.\\nContextual Embeddings. The output of the transformer\\nblock is a set of contextual embeddings that capture the\\nrelationships between tokens in the input sequence. Unlike\\nthe input embeddings, which are ﬁxed for each token, the\\ncontextual embeddings are updated at each layer of the\\ntransformer model based on the relationships between\\ntokens. The embeddings capture rich and complex'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='semantic information about the token in the context in\\nwhich it appears\\nPrediction. An additional layer processes the ﬁnal\\nrepresentation into a task-dependent ﬁnal output. In the\\ncase of text generation, this involves having a linear layer\\nthat maps the contextual embeddings to the vocabulary\\nspace, followed by a softmax operation to predict the next\\ntoken in the sequence.\\nOf course, this is a simpliﬁcation of the transformer\\narchitecture. Diving into the internals of how self-attention\\nworks or the internals of the transformer block is beyond the\\nscope of this book. However, understanding the high-level\\narchitecture of a transformer model can be helpful to grasp\\nhow these models work and how they can be applied to various\\ntasks. This architecture has enabled transformers to achieve\\nunprecedented performance in various tasks and domains, and\\nyou’ll ﬁnd them cropping up again and again –not only in the\\nrest of this book, but also in the discipline as a whole.\\nTransformer Models Genealogy\\nAt the beginning of the chapter, we experimented with Qwen to\\nauto-regressively generate text. Qwen, an example of a decoder-'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='based transformer, has a single stack of transformer blocks that\\nprocess an input sequence. This is a popular approach today,\\nbut there are other architectures that have been developed over\\nthe years. This section will provide a brief overview of the\\ngenealogy of transformer models.\\nSequence-to-Sequence Tasks\\nThe original transformer paper, Attention is all you need, ,\\nshown in Figure 2-10, used a seemingly more complicated\\narchitecture called the encoder-decoder architecture. Although\\nthe encoder-decoder architecture was very popular until 2023,\\nthey have been superseded by decoders in most research labs\\n6'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 2-10. Figure 2-10. A diagram of the encoder-decoder transformer (inspired on\\nhttps://arxiv.org/abs/1706.03762)\\nThe transformer paper focused on machine translation as the\\nexample sequence-to-sequence task. The best results in\\nmachine translation at the time were achieved by RNNs, such as\\nLSTM and GRU (don’t worry if you’re unfamiliar with them).\\nThe paper demonstrated better results by focusing solely on the\\nattention method and showed that scalability and training were\\nmuch easier. These factors –excellent performance, stable\\ntraining, and easy scalability– are why transformers took oﬀ\\nand were adapted to multiple tasks, as the next section explores\\nin more depth.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='In encoder-decoder models, like the original transformer model\\ndescribed in the paper, one stack of transformer blocks, called\\nencoder, processes an input sequence into a set of rich\\nrepresentations, which are then fed into another stack of\\ntransformer blocks, called decoder, that decodes them into an\\noutput sequence. This approach to convert one sequence into a\\ndiﬀerent one is called sequence-to-sequence or seq2seq and is\\nnaturally well suited for tasks such as translation,\\nsummarization, or question-answering.\\nFor example, you feed an English sentence through the encoder\\nof a translation model, which generates a rich embedding that\\ncaptures the meaning of the input. Then, the decoder generates\\nthe corresponding French sentence using this embedding. The\\ngeneration happens in the decoder one token at a time, as we\\nsaw when generating sequences earlier in the chapter.\\nHowever, the predictions for each successive token are\\ninformed not just by the previous tokens in the sequence being\\ngenerated but also by the output from the encoder.\\nThe mechanism by which the output from the encoder side is\\nincorporated into the decoder stack is called cross-attention. It\\nresembles self-attention, except that each token in the input\\n(the sequence being processed by the decoder) attends to the\\ncontext from the encoder rather than other tokens in its'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='sequence. The cross-attention layers are interleaved with self-\\nattention, allowing the decoder to use both contexts within its\\nsequence and the information from the encoder.\\nAfter the transformer paper, existing sequence-to-sequence\\nmodels, such as Marian NMT, incorporated these techniques as\\na central part of their architecture. New models were developed\\nusing these ideas. A notable one is BART (short for Bidirectional\\nand Auto-Regressive Transformers ). During pre-training, BART\\ncorrupts input sequences and attempts to reconstruct them in\\nthe decoder output. Afterward, BART is ﬁne-tuned for other\\ngeneration tasks, such as translation or summarization,\\nleveraging the rich sequence representations achieved during\\npre-training. Input corruption, by the way, is one of the key\\nideas behind diﬀusion models, as we’ll discuss in Chapter 4.\\nEncoder-only Models\\nAs we’ve discussed, the original transformer model was based\\non an encoder-decoder architecture that has been further\\nexplored in models such as BART or T5. In addition, the encoder\\nor the decoder can be trained and used independently, giving\\nrise to distinct transformer families. The ﬁrst sections of this\\nchapter explored decoder-only, or autoregressive models. These\\nmodels are specialized in text generation using the techniques\\n7'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='we described and have shown impressive performance, as\\ndemonstrated by ChatGPT, Claude, Llama, or Gemma.\\nEncoder models, on the other hand, are specialized in obtaining\\nrich representations from text sequences and can be used for\\ntasks such as classiﬁcation or to prepare semantic embeddings\\nfor a multitude of documents that can be used in retrieval\\nsystems. The best-known transformer encoder model is\\nprobably BERT , which introduced the masked language model\\nobjective that was later picked up and further explored by\\nBART.\\nCausal language modeling predicts the next token given the\\nprevious ones - it’s what we did with Qwen. The model can only\\nattend to the context on the left of a given token. A diﬀerent\\napproach used in encoder models is called masked language\\nmodeling (MLM). Masked language modeling, proposed in the\\nfamous BERT paper, pre-trains a model to learn to \"fill in\\nthe blanks“. Given an input text, we randomly mask some\\ntokens, and the model must predict the hidden tokens, as shown\\nin Figure 2-11. Unlike causal language modeling, MLM uses\\nboth the sequence at the masked token’s left and right, hence\\nthe B of bidirectional in BERT’s name. This helps create strong\\nrepresentations of the given text. Under the hood, these models\\nuse the encoder part of the transformer’s architecture.\\n8'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='We just discussed encoder-decoder and decoder-only\\narchitectures. A common question is why one might need an\\nencoder-decoder model for tasks like translation if decoder-only\\nmodels like Qwen and Llama can show good results. Encoder-\\ndecoder models are designed to translate an entire input\\nsequence to an output sequence, making them well-suited for\\ntranslation. In contrast, decoder-only models focus on\\npredicting the next token in a sequence. Initially, decoder-only\\nmodels like GPT-2 were less capable in zero-shot learning\\nscenarios than more recent models like GPT-4, but this was due\\nto more than just the absence of an encoder. The improvement\\nin zero-shot capabilities in advanced models like GPT-4 is also\\ndue to larger training data, better training techniques, and\\nincreased model sizes. While encoders in seq2seq models play a\\ncrucial role in understanding the full context of input\\nsequences, advancements in decoder-only models have made\\nthem more eﬀective and versatile, even for tasks traditionally\\nrelying on seq2seq models.\\nLet’s look at some code. Rather than using the AutoModel and\\nAutoTokenizer classes we used before, let’s introduce a\\nhigher-level transformers API called pipeline. This API allows\\nyou to easily load a model for a given task. pipeline takes\\ncare of all the pre- and post-processing, and hence, it’s a great\\nway to quickly try out models.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from transformers import pipeline\\nfill_masker = pipeline(\"fill-mask\", \\nmodel=\"bert-base-uncased\")\\nfill_masker(\"The [MASK] is made of milk.\")\\n[{\\'score\\': 0.19546695053577423,\\n  \\'token\\': 9841,\\n  \\'token_str\\': \\'dish\\',\\n  \\'sequence\\': \\'the dish is made of milk.\\'},\\n {\\'score\\': 0.1290755718946457,\\n  \\'token\\': 8808,\\n  \\'token_str\\': \\'cheese\\',\\n  \\'sequence\\': \\'the cheese is made of milk.\\'},\\n {\\'score\\': 0.10590697824954987,\\n  \\'token\\': 6501,\\n  \\'token_str\\': \\'milk\\',\\n  \\'sequence\\': \\'the milk is made of milk.\\'},\\n {\\'score\\': 0.04112089052796364,\\n  \\'token\\': 4392,\\n  \\'token_str\\': \\'drink\\',\\n  \\'sequence\\': \\'the drink is made of milk.\\'},\\n {\\'score\\': 0.03712352365255356,\\n  \\'token\\': 7852,\\n  \\'token_str\\': \\'bread\\',\\n  \\'sequence\\': \\'the bread is made of milk.\\'}]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Good to know that the milk is made of milk! What happens\\nunder the hood? The encoder receives the input sequence and\\ngenerates a contextualized representation for each token. This\\nrepresentation is a vector of numbers that captures the\\nmeaning of the token in the context of the entire sequence. The\\nencoder is usually followed by a task-speciﬁc layer that uses the\\nrepresentations to perform tasks such as classiﬁcation, question\\nanswering, or masked language modeling, as shown in Figure 2-\\n11. The encoder is trained to generate representations that are\\nuseful for understanding-heavy tasks.\\nFigure 2-11. Figure 2-11. Encoder models output semantic embeddings that can be\\nused to solve tasks such as predicting a token in the middle of a sequence.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Between encoder-only, decoder-only, and encoder-decoder\\nmodels, companies and research labs have released a large\\nnumber of new open and closed language models, such as GPT-\\n4, Mistral, Falcon, Llama, Qwen, Yi, Claude, Bloom, Gemma, and\\nhundreds more. Yann LeCun posted this delightful genealogy\\ndiagram in Twitter, taken from a survey paper  shows\\ntransformers’ rich and fruitful impact on the NLP landscape as\\nof 2024.\\nThe Power of Pre-training\\nHaving access to existing models is quite powerful.\\nTransformer models have shown state-of-the-art performance\\nacross many other language tasks, such as text classiﬁcation,\\nmachine translation, and answering questions based on an\\ninput text. Why do transformers work so well?\\nThe ﬁrst insight is the usage of the attention mechanism, as\\nhinted across the chapter. Attention mechanisms allow the\\ntransformers model to attend to long sequences and learn long-\\nrange relationships. In other words, transformers can estimate\\nhow relevant some tokens are to other tokens.\\n9'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The second key aspect is their ability to scale. The transformer\\narchitecture has an implementation optimized for\\nparallelization, and research has shown that these models can\\nscale to handle high-complexity and high-scale datasets.\\nAlthough initially designed for text data, the transformer\\narchitecture can be ﬂexible enough to support diﬀerent data\\ntypes and handle irregular inputs.\\nThe third key insight is the ability to do pre-training and ﬁne-\\ntuning. Traditional approaches to a task, such as movie review\\nclassiﬁcation, were limited by the availability of labeled data. A\\nmodel would be trained from scratch on a large corpus of\\nlabeled examples, attempting to predict the label from the input\\ntext directly. This approach is often referred to as supervised\\nlearning. However, it has a signiﬁcant drawback: it requires a\\nlarge amount of labeled data to train eﬀectively. This is a\\nproblem because labeled data is expensive to obtain and time-\\nconsuming to label. There might not even be any available data\\nin many domains.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 2-12. Figure 2-12. Popular model releases\\nTo address this, researchers began looking for a way to pre-\\ntrain models on existing data that could then be ﬁne-tuned (or\\nadjusted) for a speciﬁc task. This approach is known as transfer\\nlearning and is the foundation of modern ML in many ﬁelds,\\nsuch as Natural Language Processing and Computer Vision.\\nInitial works in NLP focused on ﬁnding domain-speciﬁc corpora\\nfor the language model pre-training phase, but papers such as\\nULMFiT  showed that even pre-training on generic text such as\\nWikipedia could yield impressive results when the models were\\nﬁne-tuned on downstream tasks, such as sentiment analysis or\\nquestion answering. This set the stage for the rise of\\n1 0'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='transformers, which turned out to be highly well-suited to\\nlearning rich representations of language.\\nThe idea of pre-training is to train a model on a large unlabeled\\ndataset and then ﬁne-tune it to a new target task, for which one\\nwould require much less labeled data. Before graduating to\\nNLP, transfer learning had already been very successful with\\nthe Convolutional Neural Networks that form the backbone of\\nmodern Computer Vision. In this scenario, one ﬁrst trains a\\nlarge model with a massive amount of labeled images in a\\nclassiﬁcation task. Through this process, the model learns\\ncommon features that can be leveraged on a diﬀerent but\\nrelated problem. For example, we can pre-train a model on\\nthousands of classes and then ﬁne-tune it to classify whether a\\npicture is of a hot dog.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 2-13. Figure 2-13. While pre-training a base model can require a signiﬁcant\\namount of resources, ﬁne-tuning an existing model on a new task or domain is\\nsigniﬁcantly cheaper.\\nWith transformers, things are taken further with self-\\nsupervised pre-training (training with unlabeled data). We can\\npre-train a model on large, unlabeled text data. How? Let’s\\nthink about causal models such as GPT. The model predicts\\nwhich is the next token. Well, we don’t need any labels to obtain\\ntraining data! Given a corpus of text, we can mask the tokens\\nafter a sequence and train the model to learn to predict them.\\nLike in the Computer Vision case, pre-training gives the model a\\nmeaningful representation of the underlying text. We can then\\nﬁne-tune the model to perform another task, such as generating\\ntext in the style of our Tweets or a speciﬁc domain (e.g., your'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='company chat). Given the model has already learned a\\nrepresentation of language, ﬁne-tuning will require much less\\ndata than if we trained from scratch.\\nFor many tasks, a rich representation of the input is more\\nimportant than being able to predict the next token. For\\nexample, if you want to ﬁne-tune a model to predict the\\nsentiment of a movie review, masked language models would\\nbe more powerful. Models such as GPT-2 are designed to\\noptimize for text generation rather than for building powerful\\nrepresentations of the text. On the other hand, models such as\\nBERT are ideal for this task. As brieﬂy mentioned before, the\\nlast layer of an encoder model outputs a dense representation\\nof the input sequence, called embedding. This embedding can\\nthen be leveraged by adding a small, simple network on top of\\nthe encoder and ﬁne-tuning the model for the speciﬁc task. As a\\nconcrete example, we can add a simple linear layer on top of\\nthe BERT encoder output to predict the sentiment of a\\ndocument. We can take this approach to tackle a wide range of\\ntasks:\\nToken classiﬁcation. Identify each entity in a sentence,\\nsuch as a person, location, or organization.\\nExtractive question answering. Given a paragraph,\\nanswer a speciﬁc question and extract the answer from the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='input.\\nSemantic search. The features generated by the encoder\\ncan be handy to build a search system. Given a database of\\na hundred documents, we can compute the embeddings for\\neach. Then, we can compare the input embeddings with the\\ndocuments’ ones at inference time, hence identifying the\\nmost similar document in the database.\\nAnd many others, including text similarity, anomaly\\ndetection, named entity linking, recommendation systems,\\nand document classiﬁcation.\\nLet’s use a BERT-based model that was ﬁne-tuned to perform\\nsequence classiﬁcation to determine if the sentiment of a text is\\npositive or negative. Once again, we’ll use the pipeline API to\\nload the model and perform the classiﬁcation.\\nfrom transformers import pipeline\\nclassifier = pipeline(\\n    \"text-classification\",\\n    model=\"distilbert/distilbert-base-\\nuncased-finetuned-sst-2-english\",\\n)\\nclassifier(\"This movie is disgustingly good \\n!\")\\n1 1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"[{'label': 'POSITIVE', 'score': \\n0.9998536109924316}]\\nThis classiﬁcation model can analyze reviews and do the same\\nas in the zero-shot classiﬁcation section. The challenge section\\nof this chapter shows how to evaluate classiﬁcation models and\\ncompare between a zero-shot setup and this ﬁne-tuned model.\\nTransformers Recap\\nWe’ve discussed three types of architectures.\\nEncoder-based architectures such as BERT, DistilBERT,\\nand RoBERTa , are ideal for tasks that require\\nunderstanding the entire input. These models output\\ncontextualized embeddings that capture the meaning of the\\ninput sequence. We can then add a small network on top of\\nthese embeddings and train it for a new speciﬁc task that\\nrelies on the semantic information.\\nDecoder-based architectures, such as GPT-2, Qwen,\\nGemma, and Llama, are ideal for new text generation.\\nEncoder-decoder architectures, or seq2seq, such as BART\\nand T5, are great for tasks that require generating new\\n1 2\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='sentences based on a given input, such as summarization\\nor translation.\\n\"Wait,\" you might say, \"I can handle all these tasks\\nwith ChatGPT or Llama.\" That’s true. Given the vast and\\ngrowing amount of training data, computing power, and\\ntraining optimizations, the quality of generative models has\\nsigniﬁcantly improved. Their zero-shot capabilities have come a\\nlong way compared to a few years ago.\\nThere are two main schools of thought here. One perspective is\\nthat, given the resources, ﬁne-tuning a model for your speciﬁc\\ntask and domain will yield better results than using a generalist\\npre-trained model. For example, if you want to use a GPT-like\\nmodel to generate character dialogues in real-time within a\\ngame, ﬁne-tuning it with similar data beforehand might\\nimprove performance. Similarly, if you need a model to extract\\nentities from a dataset of chemistry papers, it might make sense\\nto ﬁne-tune an encoder-based model with relevant chemistry\\ntexts.\\nOn the other hand, with the rise of high-quality, low-cost\\ngeneralist models that perform well across a variety of tasks,\\nsome argue that ﬁne-tuning might be unnecessary for most use'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='cases. Instead, prompt engineering could be a more eﬀective\\nand cheaper approach.\\nSeq2seq models were initially successful because they can\\nencode variable-length input sequences into embeddings that\\nsummarize the input information. The decoder then uses this\\ncontext to generate output. Recently, decoder-only models have\\ngained popularity due to their simplicity, scalability, eﬃciency,\\nand parallelization. In practice, diﬀerent types of models are\\nemployed depending on the task—there’s no single \"golden\"\\nmodel that works for everything.\\nWith over half a million open models, you might wonder which\\none to use. Chapter 6 will help you navigate this landscape,\\nproviding guidelines on how to choose the right model for your\\ntask and requirements as well as how to ﬁne-tune a model for\\nyour speciﬁc needs.\\nLimitations\\nAt this point, you might wonder what the issues are with\\ntransformers. Let’s brieﬂy go over some of the limitations:\\nTransformers are very large. Research has consistently\\nshown that larger models perform better. Although that’s\\nquite exciting, it also brings concerns. First, some of the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='most powerful models require tens of millions of U.S.\\ndollars to train - just in computing power. That means that\\nonly a small set of institutions can train very large base\\nmodels, limiting the kind of research that institutions\\nwithout those resources can do. Second, using such\\namounts of computing power can also have ecological\\nimplications - those millions of GPU hours are, of course,\\npowered by lots of electricity. Third, even if some of these\\nmodels are open-sourced, running them might require\\nmany GPUs. Chapter 6 will explore some techniques to use\\nthese LLMs even if you don’t have multiple GPUs at home.\\nEven then, deploying them in resource-constrained\\nenvironments is a frequent challenge.\\nSequential processing: If you recall the decoder section,\\nwe had to process all the previous tokens for each new\\ntoken. That means generating the 10,000th token in a\\nsequence will take considerably longer than generating the\\ninitial one.  In computer science terms, transformers have\\nquadratic time complexity with respect to the input length.\\nThis means that as the length of the input increases, the\\ntime taken for processing grows quadratically, making it\\nchallenging to scale them to very long documents or use\\nthese models in some real-time scenarios. While\\ntransformers excel in many tasks, their computational\\n1 3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='demands require careful consideration and optimization\\nwhen being used in production. That said, there has been a\\nlot of research on making transformers more eﬃcient for\\nextremely long sequences, such as clever caching\\ntechniques, ring attention, and Inﬁni-attention.\\nFixed input size: Transformer models can handle a\\nmaximum number of tokens, which depends on the base\\nmodel. The number of tokens the model can attend is called\\nthe context window and is an essential thign to look into\\nwhen picking a pre-trained model. You cannot simply pass\\nentire encyclopedias to transformers, expecting they will be\\nable to summarize them, but this is changing quickly. While\\nsome pre-trained models can only handle up to 512 tokens,\\nhaving models that can handle up to 32 thousand tokens is\\nmore common nowadays. New techniques allow to scale to\\nhundreds of thousands or even millions of tokens! For\\nexample, Llama 3.1 can handle 131 thousand tokens, which\\nis close to the length of this book! This is an essential thing\\nto look into when picking a pre-trained model.\\nLimited interpretability: Transformers are often\\ncriticized for their lack of interpretability. .\\nAll of the above are very active research areas - people have\\nbeen exploring how to train and run models with less\\ncomputing power (e.g., QLoRA, which we’ll explore in Chapter\\n1 4'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='6), make generation faster (e.g., ﬂash attention and assisted\\ngeneration), enable unconstrained input sizes (e.g., RoPE and\\nattention sinks), and interpret the attention mechanisms.\\nOne big concern is the presence of biases in models. If the\\ntraining data used to pre-train transformers contains biases, the\\nmodel can learn and perpetuate them. This is a broader issue in\\nMachine Learning but is specially relevant to transformers.\\nLet’s revisit the fill-mask pipeline. Let’s say we want to\\npredict the most likely profession. As you can check out below,\\nthe results are very diﬀerent if we use the word \"man\"\\nvs.\\xa0\"woman“.\\nunmasker = pipeline(\"fill-mask\", model=\"bert-\\nbase-uncased\")\\nresult = unmasker(\"This man works as a [MASK] \\nduring summer.\")\\nprint([r[\"token_str\"] for r in result])\\nresult = unmasker(\"This woman works as a \\n[MASK] during summer.\")\\nprint([r[\"token_str\"] for r in result])\\n[\\'farmer\\', \\'carpenter\\', \\'gardener\\', \\n\\'fisherman\\', \\'miner\\']'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"['maid', 'nurse', 'servant', 'waitress', \\n'cook']\\nWhy does this happen? To enable pre-training, researchers\\nusually require large amounts of data, leading to scraping all\\nthe content they can ﬁnd. This content might be of all kinds of\\nquality, including toxic content (which can be, to some extent,\\nﬁltered out). The base model might end up engraining and\\nperpetuating these biases when being ﬁne-tuned. Similar\\nconcerns exist for conversational models, where the ﬁnal model\\nmight generate toxic content learned from the pre-training\\ndataset.\\nBeyond Text\\nTransformers have been used for many tasks representing data\\nas text. A clear example is code generation – rather than\\ntraining a language model with English data, we can use lots of\\ncode, and, by the same principles we just learned, it will learn\\nhow to auto-complete code. Another example is using\\ntransformers to answer questions from a table, such as a\\nspreadsheet.\\nAs transformer models have been so successful in the text\\ndomain, considerable interest has sparked in other\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='communities to adapt these techniques to other modalities. This\\nhas led to transformer models being used for tasks such as\\nimage recognition, segmentation, object detection, video\\nunderstanding, and more.\\nCNN have been widely used as the go-to state-of-the-art models\\nfor most Computer Vision techniques. With the introduction of\\nVision Transformers (ViT) , there has been a switch in recent\\nyears to explore how to tackle vision tasks with attention and\\ntransformers-based techniques. ViTs don’t discard CNNs\\nentirely: In the image processing pipeline, CNNs extract feature\\nmaps of the image to detect high-level edges, textures, and other\\npatterns. The feature maps obtained from the CNNs are then\\ndivided into ﬁxed-size, non-overlapping patches. These patches\\ncan be treated similarly to a sequence of tokens, so the attention\\nmechanism can learn the relationships between patches in\\ndiﬀerent places.\\nUnfortunately, ViTs required more data (300 million images!)\\nand compute than CNNs to get good results. Further work has\\nhappened in recent years; for example, DeiT was able to use\\ntransformer-based models with mid-sized datasets (1.2M\\nimages) thanks to using augmentation and regularization\\ntechniques common in CNNs. Other models such as DETR,\\nSegFormer, and Swin Transformer have pushed the ﬁeld\\n1 5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='further, supporting many tasks such as image classiﬁcation,\\nobject detection, image segmentation, video classiﬁcation,\\ndocument understanding, image restoration, super-resolution,\\nand others.\\nA powerful example of transformer-based image models is\\nzero-shot image classiﬁcation. Unlike traditional image\\nclassiﬁers that are trained on a ﬁxed set of classes, zero-shot\\nimage classiﬁcation allows the speciﬁcation of classes at\\ninference time. This provides the ﬂexibility to use a single\\nmodel for various image classiﬁcation tasks, even those it was\\nnot explicitly trained for. To demonstrate, let’s start by loading\\nan image using the PIL library, a widely-used tool for vision-\\nrelated pre-processing.\\nimport requests\\nfrom PIL import Image\\nfrom genaibook.core import SampleURL\\n# Download an image and load it with the PIL \\nlibrary\\nurl = SampleURL.CatExample\\nimage = Image.open(requests.get(url, \\nstream=True).raw)\\nimage'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 2-14. Figure 2-14. A cat picture we’ll pass to a zero-shot image classiﬁcation\\npipeline.\\nNow, let’s use the high-level pipeline to use a model for the\\ngiven task.\\npipe = pipeline(\\n    \"zero-shot-image-classification\", \\nmodel=\"openai/clip-vit-base-patch32\"\\n)  \\nlabels = [\"cat\", \"dog\", \"zebra\"]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"pipe(image, candidate_labels=labels)  \\nimage\\nLoad the openai/clip-vit-base-patch32 model.\\nDeﬁne the classes we want to use at inference time.\\nPass the image and labels through the pipeline to get the\\nmodel predictions.\\n[{'score': 0.9936687350273132, 'label': \\n'cat'},\\n {'score': 0.006043245084583759, 'label': \\n'dog'},\\n {'score': 0.0002880473621189594, 'label': \\n'zebra'}]\\nAs we’ll explore in Chapter 9, transformer models can also be\\nused for audio tasks, such as transcribing audio or generating\\nsynthetic speech or music. Under the hood, the same\\nfundamental principles of pre-training and attention\\nmechanisms persist, but each modality has diﬀerent data types,\\nrequiring diﬀerent approaches and modiﬁcations.\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 2-15. Figure 2-15. Transformers models can be used for diﬀerent tasks, such as\\nclassifying images, detecting objects, and segmenting images.\\nOther modalities where transformers are being explored are:\\nGraphs: An excellent introductory read is Introduction to\\nGraph Machine Learning by Fourrier. Using transformers\\nfor graphs is still very exploratory, but there are some\\nexciting early results. Some examples of tasks that involve\\ngraph data are predicting the toxicity of molecules,\\npredicting the evolution of systems, or generating new\\nplausible molecules.\\n3D data: For example, perform segmentation of data that\\ncan be represented in 3D, such as LiDAR point clouds in\\nautonomous driving or CT scans for organ segmentation.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Another example is estimating an object’s 6 degrees of\\nfreedom, which can be helpful in robotics applications.\\nTime series: Analyzing stock price or performing weather\\nforecasting.\\nMultimodal: Some transformer models are designed to\\nprocess or output multiple types of data (such as text,\\nimages, and audio) together. This opens new possibilities,\\nsuch as multimodal systems where you can speak, write, or\\nprovide pictures and have a single model to process them.\\nAnother example is visual question answering, where a\\nmodel can answer questions about provided images.\\nProject Time: Using LMs to Generate\\nText\\nWe used the generate() method in the generation section to\\nperform diﬀerent decoding techniques. To better understand\\nhow it works under the hood, it’s time to implement it\\nourselves. We’ll use the generate() method as a reference\\nbut implement it from scratch. We’ll also explore using the\\ngenerate() method to perform diﬀerent decoding\\ntechniques.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Your goal is to ﬁll the code in the following function. Rather\\nthan use model.generate(), the idea is to iteratively call\\nmodel(), passing the previous tokens as input. You have to\\nimplement greedy search when do_sample=False, sampling\\nwhen do_sample=True, and Top-K sampling when\\ndo_sample=True and top_k is not None. This will be a\\nchallenging task, so do not worry if you don’t come up with a\\nsolution quickly. We suggest you begin implementing greedy\\nsearch and then build on top of it.\\ndef generate(\\n    model, tokenizer, input_ids, \\nmax_length=50, do_sample=False, top_k=None\\n):\\n    \"\"\"Generate a sequence without using \\nmodel.generate()\\n    Args:\\n        model: The model to use for \\ngeneration.\\n        tokenizer: The tokenizer to use for \\ngeneration.\\n        input_ids: The input IDs\\n        max_length: The maximum length of the \\nsequence.\\n        do_sample: Whether to use sampling.\\n        top_k: The number of tokens to sample'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from.\\n    \"\"\"\\n    # Write your code here\\n    # Begin by the simplest approach, greedy \\ndecoding.\\n    # Then add sampling and finally top-k \\nsampling.\\nSummary\\nYou now have learned to load and use transformers for various\\ntasks. This chapter also covered how transformer models\\nsequence data such as text and how this property lets them\\nlearn valuable representations that we can use to generate or\\nclassify new sequences. As the scale of these models increases,\\nso do their capabilities - to the point where massive models\\nwith hundreds of billions of parameters can now perform many\\ntasks previously thought impossible for computers.\\nWe can pick powerful existing pre-trained models and modify\\nthem for speciﬁc domains and use cases thanks to ﬁne-tuning.\\nThe trend towards larger and more capable models has caused\\na shift in how people use them. Task-speciﬁc models are often\\nout-competed by general-purpose LLMs, and most people now\\ninteract with these models via APIs, hosted solutions, local'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='deployments or directly via slick chat-based user interfaces. At\\nthe same time, thanks to the release of large and powerful\\nopen-access models, such as Llama, there is a strong wave in\\nthe researchers’ and practitioners’ ecosystems aiming to run\\nhigh-quality models directly in consumer computers, resulting\\nin privacy-ﬁrst solutions. This trend extends beyond inference:\\nnovel training approaches that allow individuals to ﬁne-tune\\nthese models without many computational resources have\\nemerged in recent years. Chapter 6 explores this further and\\ndives into both traditional and novel ﬁne-tuning techniques.\\nAlthough we covered how transformers work and we’ll dive\\ninto their training, diving into the internals of these models (for\\nexample, the math behind attention mechanisms) or how to\\npre-train a model from scratch is outside the scope of this book.\\nLuckily for us, there are excellent resources to learn about this:\\nThe Illustrated Transformer by Jay Alammar is a beautiful\\nvisual guide that explains transformers in a detailed and\\nintuitive way.\\nWe recommend reading the Natural Language Processing\\nwith Transformers book if you want to dive deeper into the\\ninternals of ﬁne-tuning these models for multiple speciﬁc\\ntasks.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Hugging Face has a free, open-source course which teaches\\nhow to solve diﬀerent NLP tasks.\\nIf you want to dive more into the GPT family of models, we\\nsuggest to review the following papers:\\nImproving Language Understanding by Generative Pre-\\nTraining. This is the original GPT paper, published in 2018\\nby Alec Radford, Karthik Narasimhan, Tim Salimans, and\\nIlya Sutskever. It introduced the idea of using a\\ntransformer-based model pre-trained on a large corpus of\\ntext to learn general language representations and then\\nﬁne-tuning it on speciﬁc downstream tasks. The paper also\\nshowed that the GPT model achieved state-of-the-art results\\non several natural language understanding benchmarks at\\nthe time.\\nLanguage Models are Unsupervised Multitask Learners,\\npublished in 2019 by Alec Radford, Jeﬀrey Wu, Rewon\\nChild, David Luan, Dario Amodei, and Ilya Sutskever. It\\npresented GPT-2, a transformer-based model with 1.5\\nbillion parameters pre-trained on a large corpus of web\\ntext called WebText. The paper also demonstrated that GPT-\\n2 could perform well on various natural language tasks\\nwithout ﬁne-tuning, such as text generation,\\nsummarization, translation, reading comprehension, and'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='commonsense reasoning. Finally, it discussed large-scale\\nlanguage models’ potential ethical and social implications.\\nLanguage Models are Few-Shot Learners, published in\\n2020 by Tom B. Brown and others. This paper shows that\\nscaling up language models dramatically improves their\\nability to perform new language tasks from only a few\\nexamples or simple instructions without ﬁne-tuning or\\ngradient updates. The paper also presents GPT-3, an\\nautoregressive language model with 175 billion\\nparameters, which achieves strong performance on many\\nNLP datasets and tasks.\\nExercises\\n1. What’s the role of the attention mechanism in text\\ngeneration?\\n2. In which cases would a character-based tokenizer be\\npreferred?\\n3. What happens if you use a tokenizer diﬀerent from the one\\nused with the model?\\n4. What’s the risk of using no_repeat_ngram_size when\\ndoing generation? (hint: think of city names)\\n5. What would happen if you combine Beam-search and\\nsampling?'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='6. Imagine you’re using a LLM that generates code in a code\\neditor by doing sampling. What would be more convenient?\\nA low temperature or a high temperature?\\n7. What’s the importance of ﬁne-tuning, and why is it\\ndiﬀerent than zero-shot generation?\\n8. Explain the diﬀerence and application of encoder, decoder,\\nand encoder-decoder transformers.\\nYou can ﬁnd the solutions to these exercises in the GitHub\\nrepository of the book.\\nChallenges\\n9. Summarization. Use a summarization model (you can do\\npipeline(\"summarization\")) to generate summaries\\nof a paragraph. How does it compare with the results of\\nusing zero-shot? Can it be beaten by providing few-shot\\nexamples?\\n10. Sentiment Analysis. In the zero-shot supplementary\\nmaterial, we calculate some metrics using zero-shot\\nclassiﬁcation. Explore using the distilbert-base-\\nuncased-finetuned-sst-2-english encoder model that\\ncan do sentiment analysis. What results do you get?'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='11. Semantic Search. Let’s build an FAQ system! Sentence\\ntransformers are powerful models that can measure\\nsemantic text similarity. While the transformer encoder\\nusually outputs an embedding for each token, sentence\\ntransformers output an embedding for the whole input\\ntext, allowing us to determine if two texts have similar\\nmeanings based on their similarity score. Let’s look at a\\nsimple example using the sentence_transformers library.\\nfrom sentence_transformers import \\nSentenceTransformer, util\\nsentences = [\"I\\'m happy\", \"I\\'m full of \\nhappiness\"]\\nmodel = SentenceTransformer(\"sentence-\\ntransformers/all-MiniLM-L6-v2\")\\n# Compute embedding for both lists\\nembedding_1 = model.encode(sentences[0], \\nconvert_to_tensor=True)\\nembedding_2 = model.encode(sentences[1], \\nconvert_to_tensor=True)\\nutil.pytorch_cos_sim(embedding_1, \\nembedding_2)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"tensor([[0.6003]], device='cuda:0')\\nWrite a list of ﬁve questions and answers about a topic. Your\\ngoal will be to build a system that, given a new question, can\\ngive the user the most likely answer. How can we use sentence\\ntransformers to solve this? The supplementary material\\ncontains the solution, but although challenging, we suggest\\ntrying it ﬁrst before looking there.\\nNOTE\\nA powerful technique called Retrieval Augmented Generation (RAG) combines text\\ngeneration and embeddings to retrieve relevant documents. Appendix C shows an\\nend-to-end example of how to build a minimal RAG pipeline. Before that, we suggest\\nreading Chapter 6, which introduces ﬁne-tuning and how to use it to adapt models to\\nspeciﬁc tasks.\\nReferences\\n1. Brown, Tom B., et al.\\xa0Language Models Are Few-Shot\\nLearners. arXiv, 22 July 2020. arXiv.org,\\nhttp://arxiv.org/abs/2005.14165\\n2. Devlin, Jacob, et al.\\xa0BERT: Pre-Training of Deep Bidirectional\\nTransformers for Language Understanding. arXiv, 24 May\\n2019. arXiv.org, http://arxiv.org/abs/1810.04805\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='3. Dosovitskiy, Alexey, et al.\\xa0An Image Is Worth 16x16 Words:\\nTransformers for Image Recognition at Scale. arXiv, 3 June\\n2021. arXiv.org, http://arxiv.org/abs/2010.11929\\n4. Fourrier, Clémentine. Introduction to Graph Machine\\nLearning, Hugging Face Blog,\\nhttps://huggingface.co/blog/intro-graphml\\n5. Gao, Leo, et al.\\xa0Scaling and evaluating sparse autoencoders.\\narXiv, 6 June 2024. arXiv.org,\\nhttps://arxiv.org/abs/2406.04093\\n6. Holtzman, Ari, et al.\\xa0The Curious Case of Neural Text\\nDegeneration. arXiv, 14 Feb.\\xa02020. arXiv.org,\\nhttp://arxiv.org/abs/1904.09751\\n7. Howard, Jeremy, and Sebastian Ruder. Universal Language\\nModel Fine-Tuning for Text Classiﬁcation. arXiv, 23 May\\n2018. arXiv.org, http://arxiv.org/abs/1801.06146\\n8. Lewis, Mike, et al.\\xa0BART: Denoising Sequence-to-Sequence\\nPre-Training for Natural Language Generation, Translation,\\nand Comprehension. arXiv, 29 Oct.\\xa02019. arXiv.org,\\nhttp://arxiv.org/abs/1910.13461\\n9. Radford, Alec, et al.\\xa0Language models are unsupervised\\nmultitask learners. OpenAI blog 1, no. 8 (2019): 9.\\n10. Radford, Alec, et al.\\xa0Improving language understanding by\\ngenerative pre-training. (2018).'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='11. Raﬀel, Colin, et al.\\xa0Exploring the Limits of Transfer Learning\\nwith a Uniﬁed Text-to-Text Transformer. arXiv, 28 July 2020.\\narXiv.org, http://arxiv.org/abs/1910.10683\\n12. T. Lan. Generating human-level text with contrastive search\\nin Transformers 🤗  Hugging Face Blog,\\nhttps://huggingface.co/blog/introducing-csearch\\n13. Vaswani, Ashish, et al.\\xa0Attention is all you need. Advances in\\nneural information processing systems 30 (2017). arXiv\\npreprint\\n14. Yang, Jingfeng, et al.\\xa0Harnessing the Power of LLMs in\\nPractice: A Survey on ChatGPT and Beyond. arXiv, 27\\nApr.\\xa02023. arXiv.org, http://arxiv.org/abs/2304.13712\\n You can explore diﬀerent tokenizers interactively in the Tokenizer Playground at\\nhttps://huggingface.co/spaces/Xenova/the-tokenizer-playground.\\n We use < to align the token to the left, 10 to specify the width of the ﬁeld, and .2%\\nto format the probability as a percentage with two decimal places.\\n In statistics, a distribution is a way of describing how the values of a variable are\\nspread out. It tells us how often diﬀerent values of the variable occur.\\n An excellent deep dive into contrastive search is the Generating Human-level Text\\nwith Contrastive Search blog post (https://huggingface.co/blog/introducing-csearch).\\n The ﬁrst example in the GPT-2 release blog post was famously a news story about\\nunicorns (https://openai.com/research/better-language-models).\\n1 \\n2 \\n3 \\n4 \\n5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Vaswani et al.\\xa0Attention is all you need. Advances in neural information processing\\nsystems 30 (2017). arXiv preprint\\n Lewis, Mike, et al.\\xa0BART: Denoising Sequence-to-Sequence Pre-Training for Natural\\nLanguage Generation, Translation, and Comprehension. arXiv, 29 Oct.\\xa02019. arXiv.org,\\nhttp://arxiv.org/abs/1910.13461\\n Devlin, Jacob, et al.\\xa0BERT: Pre-Training of Deep Bidirectional Transformers for\\nLanguage Understanding. arXiv, 24 May 2019. arXiv.org,\\nhttp://arxiv.org/abs/1810.04805.\\n Yang, Jingfeng, et al.\\xa0Harnessing the Power of LLMs in Practice: A Survey on ChatGPT\\nand Beyond. arXiv, 27 Apr.\\xa02023. arXiv.org, http://arxiv.org/abs/2304.13712\\n Howard, Jeremy, and Sebastian Ruder. Universal Language Model Fine-Tuning for\\nText Classiﬁcation. arXiv, 23 May 2018. arXiv.org, http://arxiv.org/abs/1801.06146\\n This oversimpliﬁes how semantic search works, but we’ll get a chance to build a\\nsimple search system using semantic embeddings in the challenge section of this\\nchapter. This is the core of Retrieval Augmented Generation.\\n DistilBERT is a smaller model that preserves 95% of the original BERT performance\\nwhile having 40% less parameters. RoBERTa is a very powerful BERT-based model\\ntrained for longer with diﬀerent hyperparameters.\\n One can cache things to avoid processing tokens from scratch. Even then, for each\\nnew token, the model must attend to all the previous tokens.\\n Among the diﬀerent lines of research on this, using sparse AutoEncoders to extract\\ninterpretable features from transformers is becoming increasingly popular. You can\\nﬁnd the original paper at https://arxiv.org/abs/2406.04093\\n6 \\n7 \\n8 \\n9 \\n 0 \\n 1 \\n 2 \\n 3 \\n 4'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Dosovitskiy, Alexey, et al.\\xa0An Image Is Worth 16x16 Words: Transformers for Image\\nRecognition at Scale. arXiv, 3 June 2021. arXiv.org, http://arxiv.org/abs/2010.11929\\n 5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Chapter 3. Compressing and\\nRepresenting Information\\nA NOTE FOR EARLY RELEASE READERS\\nWith Early Release ebooks, you get books in their earliest form\\n—the authors’ raw and unedited content as they write—so you\\ncan take advantage of these technologies long before the oﬃcial\\nrelease of these titles.\\nThis will be the third chapter of the ﬁnal book. Please note that\\nthe GitHub repo will be made active later on.\\nIf you have comments about how we might improve the content\\nand/or examples in this book, or if you notice missing material\\nwithin this chapter, please reach out to the editor at\\njleonard@oreilly.com.\\nThis chapter introduces Machine Learning models and\\ntechniques to learn eﬃcient data representations for tasks\\ninvolving images, videos, or text. Why are eﬃcient\\nrepresentations important? We want to reduce the amount of\\ninformation we need to store and process while keeping the\\nessential characteristics of the data. Rich representations'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='enable training models specialized on particular tasks, and\\nmaking the representations compact reduces the computational\\nrequirements to train and work with data-intensive models. For\\nexample, training on a vector embedding of an image can be\\nmore eﬃcient and expressive than doing it directly on its pixels.\\nTraditional compression methods like zip or JPEG focus on\\nspeciﬁc data types and use hand-crafted algorithms to reduce\\nﬁle sizes. While these methods are eﬀective for their intended\\npurposes, they lack the ﬂexibility and adaptability of learned\\ncompression techniques. Zip, for instance, excels at lossless\\ncompression of general data by identifying and encoding\\nrepetitive patterns. On the other hand, JPEG is designed\\nspeciﬁcally for image compression and achieves signiﬁcant size\\nreduction by discarding less noticeable visual information.\\nHowever, these traditional methods don’t learn from the data\\nthey compress and can’t automatically adapt to diﬀerent types\\nof content or optimize for speciﬁc tasks beyond size reduction.\\nThis is where Machine Learning models can be useful.\\nWe’ll begin by exploring AutoEncoders, a family of Machine\\nLearning models that consist of an encoder that \"compresses\"\\ndata and a decoder that reconstructs it just using the\\nrepresentation. The encoder learns the essential features of the\\ndata it needs to focus on, which allows the decoder to reverse'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='the transformations, as shown in the left panel of Figure 3-1.\\nThis training approach is a way to build compressors\\nautomatically without relying on hand-crafted algorithms.\\nCompressing information (even in a lossy way) is helpful in\\nitself, but there are a few other interesting things we can do\\nonce we have a compact dataset representation.\\nIf our system is correctly trained and the decoder can recover\\nthe original from the representations, it means that the learned\\nrepresentations have captured the essential information.\\nTherefore, operating on the representations is equivalent to\\nworking with the originals but requires much less memory and\\ncomputing. This is one of the key design aspects of models such\\nas Stable Diﬀusion – as we’ll see in Chapter 5, we can generate\\nand manipulate large images, but most of the computation\\nhappens in the smaller latent space where representations\\nreside.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 3-1. Figure 3-1. Eﬃcient data representation methods (left) can be used for\\nother tasks, such as classiﬁcation (middle), or to generate new content (right).\\nBecause the learned representations capture the essential\\ninformation, we can split the encoder and decoder once the\\nAutoEncoder is trained and use the encoder as a feature\\nextraction component. Adding a small network on top of the\\nencoder’s outputs, as represented in the center panel of Figure\\n3-1, allows us to train the model for diﬀerent tasks, such as text\\nor image classiﬁcation. These small networks don’t operate on\\nthe whole input image but on the essential characteristics\\nobtained by the encoder.\\nWe can also encode diﬀerent data types to the same latent space\\nrepresentations. As we saw in Chapter 2, sequence-to-sequence'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='language models use an encoder-decoder architecture to\\nperform a wide variety of tasks, such as translation or\\nsummarization. Although there are more details to consider\\nwhen designing such systems, one key insight is that the\\nencoder’s job is to capture the essential features that carry\\nenough semantic information about the input text. This works\\nacross modalities, too: the job of an image captioning model, for\\nexample, is to translate image representations to textual\\ndescriptions, using a latent space as the internal working data.\\nA ﬁnal example of the use of AutoEncoders is for generative\\nmodeling (right panel of Figure 3-1). After we have trained the\\nencoder-decoder pair, we can throw away the encoder and\\ngenerate new data by sampling from a random distribution in\\nthe latent space. This is the base of Variational AutoEncoders\\n(or VAEs). We’ll see an example of how it’s done in the second\\nsection of this chapter.\\nWe’ll use image data to showcase how AutoEncoders and VAEs\\nwork, but the techniques can be applied to any data, not just\\nimages. The ﬁnal section of this chapter will examine how\\nmultimodal representation learning systems, such as CLIP,\\nbridge the gap between text and images and can be used for\\nvery interesting use cases such as semantic search, data\\nﬁltering, text-to-image generation, and more.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='AutoEncoders\\nAutoEncoders consist of two models stitched together: an\\nencoder and a decoder, schematically represented in Figure 3-2.\\nBoth models are trained together with the objective that the\\nencoder produces intermediate representations, which the\\ndecoder then uses to regenerate the input data. If training\\nsucceeds, the AutoEncoder learns to extract key features from\\nthe input data.\\nFigure 3-2. AutoEncoder: conceptual architecture diagram'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Preparing the Data\\nIn this section, we’ll build a simple AutoEncoder using the\\nMNIST dataset. MNIST is a classical dataset consisting of 70,000\\nlow-resolution (28x28) black-and-white images of handwritten\\nnumerical digits. We’ll download it from the redistribution of\\nthe dataset hosted in Hugging Face. To download it, we’ll use a\\nlibrary called datasets, which provides a uniﬁed API to access\\nthousands of datasets for any type of data. Details about how it\\nworks are not important now; just note that it will take care of\\ndownloading and caching for subsequent use. It provides two\\ndataset splits: a train set with 60,000 images and a test\\ndataset with the remaining 10,000 images.\\nfrom datasets import load_dataset\\nmnist = load_dataset(\"mnist\")\\nmnist\\nDatasetDict({\\n    train: Dataset({\\n        features: [\\'image\\', \\'label\\'],\\n        num_rows: 60000'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='})\\n    test: Dataset({\\n        features: [\\'image\\', \\'label\\'],\\n        num_rows: 10000\\n    })\\n})\\nAs you can see, the dataset contains a column called image,\\nwith the handwritten images, and label, which contains the\\nnumber represented by the image. As we’re going to train an\\nAutoEncoder to compress and reconstruct the image, we don’t\\nreally need the label data at all: we’ll feed batches of random\\nsamples to the encoder, and the decoder’s job will be to\\nregenerate images that resemble the inputs. Because the input\\ndata has everything required to train without relying on\\nexternal annotated information, AutoEncoder training is an\\nexample of a\\xa0self-supervised\\xa0learning process.\\nWe will ignore the labels for training, but we’ll use them later\\nfor visualization purposes. As always, before training a model,\\nlet’s explore the dataset.\\nmnist[\"train\"][\"image\"][1]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 3-3. A sample of the MNIST dataset.\\nImages with a resolution of just 28x28 are very small by today’s\\nstandards. We’ll use a helper function, show_images(), to\\nshow them with higher resolution. show_images() is based\\non Python’s matplotlib library, which by default uses a high-\\ncontrast color palette to represent monochrome image data,\\nshown in Figure 3-4.\\nfrom genaibook.core import show_images\\nshow_images(mnist[\"train\"][\"image\"][:4])'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 3-4. Some samples from the MNIST dataset, rendered by\\nmatplotlib.\\nBecause the originals are black and white, we’ll conﬁgure\\nmatplotlib to just use gray colors instead. We choose \"reversed\\ngray\" (gray_r) to get black numbers on a white background,\\nshown in Figure 3-5. Note that the originals are the other way\\naround (pixels with number data are white while the\\nbackground is all zeros, which means black color).'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='import matplotlib as mpl\\nmpl.rcParams[\"image.cmap\"] = \"gray_r\"\\nshow_images(mnist[\"train\"][\"image\"][:4])\\nFigure 3-5. MNIST samples drawn using a monochrome palette.\\nWe convert the images to Pytorch tensors and shuﬄe the\\ntraining dataset. We’ll use ToTensor(), from the torchvision\\nlibrary , to convert the input pixels, in the [0, 255] range, to1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='PyTorch tensors between 0 and 1. We don’t apply any other\\nmanipulations. For convenience, show_images can also draw\\ntensors representing images, as shown in Figure 3-6.\\nfrom torchvision import transforms\\ndef mnist_to_tensor(samples):\\n    t = transforms.ToTensor()\\n    samples[\"image\"] = [t(image) for image in \\nsamples[\"image\"]]\\n    return samples\\nmnist = mnist.with_transform(mnist_to_tensor)\\nmnist[\"train\"] = \\nmnist[\"train\"].shuffle(seed=1337)\\nLet’s check out a single image from the dataset and conﬁrm that\\nthe input pixels range from 0 to 1.\\nx = mnist[\"train\"][\"image\"][0]\\nx.min(), x.max()\\n(tensor(0.), tensor(1.))'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='show_images(mnist[\"train\"][\"image\"][0])\\nFigure 3-6. show_images() can also display tensors.\\nWe now create a PyTorch DataLoader to prepare the training\\ndata. Because training an AutoEncoder is a self-supervised\\nprocess, we’ll just work with the image column of the dataset\\nand ignore the labels. Later, we’ll return and use the labels to\\nvisualize results. A DataLoader is an abstraction whose\\nprimary mission is to collate inputs, which means gathering'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='and combining individual samples into training batches with\\nthe same shape. In our case, all images have the same size, and\\ntherefore, all tensors have the same shape, so the dataloader\\nwill just concatenate them together. In more complicated cases,\\ndataloaders may need to deal with irregular input shapes using\\nstrategies such as padding or truncation.\\nfrom torch.utils.data import DataLoader\\nbs = 64\\ntrain_dataloader = DataLoader(mnist[\"train\"]\\n[\"image\"], batch_size=bs)\\nModeling the Encoder\\nFirst, we’ll create a model deﬁnition for the encoder part of the\\nAutoEncoder. Because we are working with image data, a\\nnatural choice is to use convolutional layers, which are good at\\ncapturing image features. We could consider many other\\nalternatives for this problem: linear layers, transformer blocks,\\nuse of residual skip connections, etc. We’ll use a simple\\nconvolutional encoder that is based on a convolutional\\nAutoEncoder implementation from the excellent pythae library.\\nThis is a great starting point for exploration!'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='WHAT ARE CONVOLUTIONAL LAYERS?\\nConvolutional layers are a collection of small 2D ﬁlters applied repeatedly to\\ndiﬀerent regions of the input image. These ﬁlters can detect patterns such as lines or\\ncircular areas. Traditionally, 2D ﬁlters have been used in digital image processing,\\nwhere they are carefully hand-crafted to match speciﬁc features in input images. The\\nmain diﬀerence with convolutional layers is that the ﬁlters are not prepared\\nbeforehand – instead, convolutional layers learn them as part of the training process\\nof the network. By stacking multiple convolutional layers, the model can\\nprogressively extract more abstract features from the input image, learning ﬁlters\\nthat eﬀectively resolve the task. There’s a thrilling ﬁeld of interpretation and\\nexplainability that aims to visualize and understand the internal workings of model\\nlayers, and it’s been shown that ﬁlters learned by neural networks sometimes\\nresemble classic ﬁlters designed manually to detect edges, colors, or contours.\\nFor an in-depth look at Convolutional Neural Networks, we recommend Chapter 13\\nof Deep Learning for Coders with fastai & PyTorch, by Jeremy Howard & Sylvain\\nGugger.\\nBecause we will stack several convolutional layers, we’ll write a\\nsimple helper function to create them. Our conv_block()\\nhelper makes a 2D convolution, then appends a batch\\nnormalization layer and a non-linearity (we’ll use the ReLU\\nactivation function for this example). During training, batch\\nnormalization uses the mean and standard deviation of the\\ncurrent batch to normalize input data so it remains within a\\npredictable range, which most of the time results in smoother\\nand faster training.2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from torch import nn\\ndef conv_block(in_channels, out_channels, \\nkernel_size=4, stride=2, padding=1):\\n    return nn.Sequential(\\n        nn.Conv2d(\\n            in_channels,\\n            out_channels,\\n            kernel_size=kernel_size,\\n            stride=stride,\\n            padding=padding,\\n        ),\\n        nn.BatchNorm2d(out_channels),\\n        nn.ReLU(),\\n    )\\nAs described, the encoder implementation will be a sequence of\\nconvolutional layers. Each layer progressively reduces the\\nimage resolution while it increases the number of channels of\\nthe representations to 1024. Finally, we’ll append a linear layer\\nat the end to create 16-dimensional vector representations. The\\ncomments in the forward() method show how the shape of\\nthe input data is transformed as it travels through the layers.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='class Encoder(nn.Module):\\n    def __init__(self, in_channels):\\n        super().__init__()\\n        self.conv1 = conv_block(in_channels, \\n128)\\n        self.conv2 = conv_block(128, 256)\\n        self.conv3 = conv_block(256, 512)\\n        self.conv4 = conv_block(512, 1024)\\n        self.linear = nn.Linear(1024, 16)\\n    def forward(self, x):\\n        x = self.conv1(x)  # (batch size, \\n128, 14, 14)\\n        x = self.conv2(x)  # (bs, 256, 7, 7)\\n        x = self.conv3(x)  # (bs, 512, 3, 3)\\n        x = self.conv4(x)  # (bs, 1024, 1, 1)\\n        # Keep batch dimension when \\nflattening\\n        x = \\nself.linear(x.flatten(start_dim=1))  # (bs, \\n16)\\n        return x\\nLet’s verify that we can run our input images through the\\nencoder. They have a [1, 28, 28] shape because they only\\ncontain one channel of (black or white) pixel data. However,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='note that we coded our encoder in a general way: we could use\\nit for 3-channel images as well.\\nmnist[\"train\"][\"image\"][0].shape\\ntorch.Size([1, 28, 28])\\nLet’s select and put a single image inside a batch, creating a new\\ndimension with PyTorch’s None indexing. We also need to set\\nthe encoder in eval mode. eval mode conﬁgures the model\\nfor inference instead of training. If we don’t do this, the last\\nBatchNorm2d layer will fail because it will receive a tensor\\nwith shape [1, 1024, 1, 1], and it can’t compute the mean\\nand standard deviation of a single sample .\\nin_channels = 1\\nx = mnist[\"train\"][\"image\"][0][None, :]\\nencoder = Encoder(in_channels).eval()\\nencoded = encoder(x)\\nencoded.shape\\ntorch.Size([1, 16])\\n3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The encoder model works! It converts 28x28 images (784 pixels\\neach) into vectors with just 16 numbers. If we can train it\\neﬀectively, the representations computed by the encoder will\\nhave much lower dimensionality than the original pixel data.\\nOf course, the representations are currently meaningless, as the\\nmodel has yet to be trained.\\nencoded\\ntensor([[-0.0145, -0.0318, -0.0109,  0.0080, \\n-0.0218,  0.0305,  0.0183, -0.0294,\\n          0.0075,  0.0178, -0.0161, -0.0018,  \\n0.0208, -0.0079,  0.0215,  0.0101]],\\n       grad_fn=<AddmmBackward0>)\\nLet’s see if it can handle a batch of 64 images.\\nbatch = next(iter(train_dataloader))\\nencoded = Encoder(in_channels=1)(batch)\\nbatch.shape, encoded.shape\\n(torch.Size([64, 1, 28, 28]), torch.Size([64, \\n16]))'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='This completes our Encoder model, which transforms images\\nto intermediate representations. Let’s move to the Decoder\\nnow.\\nDecoder\\nThe decoder begins with the latent representation obtained by\\nthe encoder (vectors with 16 dimensions) and turns them into\\nimages of the original size.\\nThe decoder architecture does not have to be the reverse of the\\nencoder’s – it can be anything that \"understands\" the encoder\\nrepresentations and is able to translate them to images. In our\\ncase, we’ll create a more or less symmetrical network to the\\nencoder: we’ll apply transposed convolutions  to increase the\\nresolution while decreasing the number of channels until we\\nreach our desired output resolution of 28x28 pixels. We’ll\\nprepend the transposed convolutions with a linear layer to\\ncreate tensors of 16384 (1024x4x4) pixels. This layer will be\\nreshaped to a 4x4 resolution ([1024,4,4])which will be the\\ninput to the ﬁrst transposed convolution. From there, we\\nprogressively reduce the channels and increase the resolution\\nuntil we reach the original image shape. There are other ways\\nto achieve the same; remember that the input is a ﬂat vector\\n4'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='with 16 channels, and the output must consist of 1 channel and\\n28x28 pixels.\\ndef conv_transpose_block(\\n    in_channels,\\n    out_channels,\\n    kernel_size=3,\\n    stride=2,\\n    padding=1,\\n    output_padding=0,\\n    with_act=True,\\n):\\n    modules = [\\n        nn.ConvTranspose2d(\\n            in_channels,\\n            out_channels,\\n            kernel_size=kernel_size,\\n            stride=stride,\\n            padding=padding,\\n            output_padding=output_padding,\\n        ),\\n    ]\\n    if with_act:  # Controling this will be \\nhandy later\\n        \\nmodules.append(nn.BatchNorm2d(out_channels))'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"modules.append(nn.ReLU())\\n    return nn.Sequential(*modules)\\nclass Decoder(nn.Module):\\n    def __init__(self, out_channels):\\n        super().__init__()\\n        self.linear = nn.Linear(\\n            16, 1024 * 4 * 4\\n        )  # note it's reshaped in forward\\n        self.t_conv1 = \\nconv_transpose_block(1024, 512)\\n        self.t_conv2 = \\nconv_transpose_block(512, 256, \\noutput_padding=1)\\n        self.t_conv3 = \\nconv_transpose_block(256, out_channels, \\noutput_padding=1)\\n    def forward(self, x):\\n        bs = x.shape[0]\\n        x = self.linear(x)  # (bs, 1024*4*4)\\n        x = x.reshape((bs, 1024, 4, 4))  # \\n(bs, 1024, 4, 4)\\n        x = self.t_conv1(x)  # (bs, 512, 7, \\n7)\\n        x = self.t_conv2(x)  # (bs, 256, 14,\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='14)\\n        x = self.t_conv3(x)  # (bs, 1, 28, \\n28)\\n        return x\\ndecoded_batch = Decoder(x.shape[0])(encoded)\\ndecoded_batch.shape\\ntorch.Size([64, 1, 28, 28])\\nTraining\\nSo far, we have created the Encoder, which reduces the\\ndimensionality of the input images, and the Decoder, which\\nexpands low-dimensional latent to the original image\\nresolution. In addition to being initialized with random weights,\\nthese two components are completely unconnected at the\\nmoment. We need to train them together so they both\\nunderstand the same latent representations.\\nTo do so, we’ll create an AutoEncoder model that passes the\\ninput data through the encoder and the decoder in sequence.\\nWe’ll train it to minimize the diﬀerence between the decoded\\nimage at the output and the original image we supplied as'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='input. If we are successful, the output images will resemble the\\ninputs. This process is useful as it allows data compression, but\\nit becomes even more interesting when we realize that we can\\nuse the two components separately after training. This will\\nenable us to do many exciting things in the following chapters.\\nFor example, we can use the encoder to convert arbitrary\\nimages to more compressed representations, which can be used\\nas inputs by other models. We can also use the decoder to\\ngenerate new images that resemble the ones from the training\\ndataset. Excited? Let’s get started on training our AutoEncoder.\\nclass AutoEncoder(nn.Module):\\n    def __init__(self, in_channels):\\n        super().__init__()\\n        self.encoder = Encoder(in_channels)\\n        self.decoder = Decoder(in_channels)\\n    def encode(self, x):\\n        return self.encoder(x)\\n    def decode(self, x):\\n        return self.decoder(x)\\n    def forward(self, x):\\n        return self.decode(self.encode(x))'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='model = AutoEncoder(1)\\nWe can use the torchsummary library to print a summary of the\\nmodel, which shows the number of parameters and the output\\nshape of each layer. This is a useful tool to check if the model is\\ncorrectly deﬁned and to understand the model’s architecture.\\nimport torchsummary\\ntorchsummary.summary(model, input_size=(1, \\n28, 28), device=\"cpu\")\\n---------------------------------------------\\n-------------------\\n        Layer (type)               Output \\nShape         Param #\\n=================================================\\n            Conv2d-1          [-1, 128, 14, \\n14]           2,176\\n       BatchNorm2d-2          [-1, 128, 14, \\n14]             256\\n              ReLU-3          [-1, 128, 14, \\n14]               0\\n            Conv2d-4            [-1, 256, 7, \\n7]         524,544\\n       BatchNorm2d-5            [-1, 256, 7,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='7]             512\\n              ReLU-6            [-1, 256, 7, \\n7]               0\\n            Conv2d-7            [-1, 512, 3, \\n3]       2,097,664\\n       BatchNorm2d-8            [-1, 512, 3, \\n3]           1,024\\n              ReLU-9            [-1, 512, 3, \\n3]               0\\n           Conv2d-10           [-1, 1024, 1, \\n1]       8,389,632\\n      BatchNorm2d-11           [-1, 1024, 1, \\n1]           2,048\\n             ReLU-12           [-1, 1024, 1, \\n1]               0\\n           Linear-13                   [-1, \\n16]          16,400\\n          Encoder-14                   [-1, \\n16]               0\\n           Linear-15                [-1, \\n16384]         278,528\\n  ConvTranspose2d-16            [-1, 512, 7, \\n7]       4,719,104\\n      BatchNorm2d-17            [-1, 512, 7, \\n7]           1,024\\n             ReLU-18            [-1, 512, 7, \\n7]               0\\n  ConvTranspose2d-19          [-1, 256, 14,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='14]       1,179,904\\n      BatchNorm2d-20          [-1, 256, 14, \\n14]             512\\n             ReLU-21          [-1, 256, 14, \\n14]               0\\n  ConvTranspose2d-22            [-1, 1, 28, \\n28]           2,305\\n      BatchNorm2d-23            [-1, 1, 28, \\n28]               2\\n             ReLU-24            [-1, 1, 28, \\n28]               0\\n          Decoder-25            [-1, 1, 28, \\n28]               0\\n=================================================\\nTotal params: 17,215,635\\nTrainable params: 17,215,635\\nNon-trainable params: 0\\n---------------------------------------------\\n-------------------\\nInput size (MB): 0.00\\nForward/backward pass size (MB): 2.86\\nParams size (MB): 65.67\\nEstimated Total Size (MB): 68.54\\n---------------------------------------------\\n-------------------'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='We’ll create a simple training loop that repeatedly goes through\\nthe training data and uses a constant learning rate. To focus on\\nthe essentials, we won’t bother running validations on the test\\nset (but you are encouraged to do it to practice!). We use the\\npopular tqdm library for progress display, but we won’t\\ndescribe it here for the sake of brevity – we’ll see more\\nexamples in other chapters. It’s not necessary to understand\\neverything that’s going on; just pay attention to the high-level\\noperations:\\nLoad a batch from the dataloader\\nGet the model predictions\\nCalculate the loss with respect to the original images\\nPerform an optimizer step to update the model weights\\nimport torch\\nfrom matplotlib import pyplot as plt\\nfrom torch.nn import functional as F\\nfrom tqdm.notebook import tqdm, trange\\nfrom genaibook.core import get_device\\nnum_epochs = 10\\nlr = 1e-4\\ndevice = get_device()'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='model = model.to(device)\\noptimizer = \\ntorch.optim.AdamW(model.parameters(), lr=lr, \\neps=1e-5)\\nlosses = []  # List to store the loss values \\nfor plotting\\nfor _ in (progress := trange(num_epochs, \\ndesc=\"Training\")):\\n    for _, batch in (\\n        inner := \\ntqdm(enumerate(train_dataloader), \\ntotal=len(train_dataloader))\\n    ):\\n        batch = batch.to(device)\\n        # Pass through the model and obtain \\nreconstructed images\\n        preds = model(batch)\\n        # Compare the prediction with the \\noriginal images\\n        loss = F.mse_loss(preds, batch)\\n        # Display loss and store for plotting\\n        inner.set_postfix(loss=f\"\\n{loss.cpu().item():.3f}\")\\n        losses.append(loss.item())'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='# Update the model parameters with \\nthe optimizer based on this loss\\n        loss.backward()\\n        optimizer.step()\\n        optimizer.zero_grad()\\n    progress.set_postfix(loss=f\"\\n{loss.cpu().item():.3f}\", lr=f\"{lr:.0e}\")\\nLet’s plot the loss curve in Figure 3-7 to see how the training\\nwent.\\nplt.plot(losses)\\nplt.xlabel(\"Step\")\\nplt.ylabel(\"Loss\")\\nplt.title(\"AutoEncoder – Training Loss \\nCurve\")\\nplt.show()'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 3-7. Training loss of the AutoEncoder.\\nWe didn’t perform validation during the training loop, but we\\ncan see how the AutoEncoder fares with the test set. Training\\nwith visual data is a great way to learn and iterate, as we can\\nsee the ﬁnal results and judge for ourselves.\\nWe’ll create a batch with 16 samples from the test set, pass\\nthem through the trained encoder and decoder, and display the\\nreconstructions. Let’s begin creating the evaluation dataloader.\\neval_bs = 16\\neval_dataloader = DataLoader(mnist[\"test\"]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='[\"image\"], batch_size=eval_bs)\\nWe’ll use model.eval() to put the model in evaluation mode\\n(in our case, it will disable BatchNorm updates) and the\\ninference_mode context manager to turn oﬀ gradient\\ncomputation.\\nmodel.eval()\\nwith torch.inference_mode():\\n    eval_batch = next(iter(eval_dataloader))\\n    predicted = \\nmodel(eval_batch.to(device)).cpu()\\nNow that we have the predictions, let’s display the original\\nimages and their reconstructions.\\nbatch_vs_preds = torch.cat((eval_batch, \\npredicted))\\nshow_images(batch_vs_preds, imsize=1, \\nnrows=2)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 3-8. Original MNIST images (top) and reconstructions\\nfrom our AutoEncoder (bottom)\\nThe results shown in Figure 3-8 look pretty good! Remember\\nthat the numbers in the second row are approximations of the\\noriginals obtained by the decoder from concise vector\\nrepresentations.\\nAt this point, we suggest ensuring you understand the diﬀerent\\nconcepts just introduced. We also recommend trying to get\\nbetter reconstruction results. Some experiment ideas:\\nProgressively decrease the learning rate\\nTry diﬀerent batch sizes\\nUse a sigmoid function at the decoder’s end to encourage\\nthe ﬁnal pixel values to be either black or white. Note that\\nour input data is between 0 and 1, so the sigmoid output\\nmust match that range\\nPlay with the network depth or topology'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='We also suggest experimenting with the training loop by adding\\nand logging the evaluation loss during training.\\nExploring the Latent Space\\nOne important hyperparameter of the AutoEncoder is the\\nnumber of dimensions we use to represent the encoded inputs.\\nWe arbitrarily chose 16, and our results show that this seems\\nenough to represent the wide variety of hand-drawn numbers\\nin the MNIST dataset.\\nFor our next experiment, we will use just two dimensions to\\nrepresent the vectors in the latent space. We’ll force the\\nencoder to squeeze as much information as possible about\\ninput images into just two ﬂoat numbers, and we’ll ﬁgure out if\\nthis is enough to recover the inputs. In addition, using two\\ndimensions is very convenient for visualization. After training\\nour new model, we can draw some interesting plots in 2D space\\nthat will help us gain additional intuition.\\nWe’ll slightly refactor our code with the following changes:\\nWe include the dimensionality of the latent space as a\\nhyperparameter.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='We use a container (nn.Sequential) for the convolution\\nlayers to make it easier to adjust the network depth if we\\nwant to experiment with that later.\\nWe replace the activation after the ﬁnal decoder\\nconvolution with a sigmoid function. We want to encourage\\nthe decoder to produce pixels that are either black or white,\\nand a sigmoid is better suited than a ReLU for that purpose.\\nThis is because the sigmoid function squashes the output to\\nthe range (0, 1), the same range as pixels in the images\\nThis can also be an excellent time to put our training loop\\ninside a function.\\nTIP\\nDon’t try to create code with many options and parameters from the start. It’s better\\nto start with the simplest working code you can write and progressively make it\\nricher as needed (and if required).\\nFigure 3-9 shows a plot of the ReLU vs.\\xa0the Sigmoid function.\\nBy using Sigmoid as the activation function, we ensure that the\\noutput after each layer lies within the range (0, 1), the same\\nrange input images use. This is not strictly necessary for the\\nnetwork to learn; we are trying to help it because we know our\\ndesired output range.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 3-9. Activation functions: ReLU vs Sigmoid\\nclass Encoder(nn.Module):\\n    def __init__(self, in_channels, \\nlatent_dims):\\n        super().__init__()\\n        self.conv_layers = nn.Sequential(\\n            conv_block(in_channels, 128),\\n            conv_block(128, 256),\\n            conv_block(256, 512),\\n            conv_block(512, 1024),'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=')\\n        self.linear = nn.Linear(1024, \\nlatent_dims)\\n    def forward(self, x):\\n        bs = x.shape[0]\\n        x = self.conv_layers(x)\\n        x = self.linear(x.reshape(bs, -1))\\n        return x\\nclass Decoder(nn.Module):\\n    def __init__(self, out_channels, \\nlatent_dims):\\n        super().__init__()\\n        self.linear = nn.Linear(latent_dims, \\n1024 * 4 * 4)\\n        self.t_conv_layers = nn.Sequential(\\n            conv_transpose_block(1024, 512),\\n            conv_transpose_block(512, 256, \\noutput_padding=1),\\n            conv_transpose_block(\\n                256, out_channels, \\noutput_padding=1, with_act=False\\n            ),\\n        )\\n        self.sigmoid = nn.Sigmoid()'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='def forward(self, x):\\n        bs = x.shape[0]\\n        x = self.linear(x)\\n        x = x.reshape((bs, 1024, 4, 4))\\n        x = self.t_conv_layers(x)\\n        x = self.sigmoid(x)\\n        return x\\nThe new version of the AutoEncoder class is almost identical\\nto the previous one: it calls the decoder on the encoder’s\\noutputs. It simply accepts an additional argument,\\nlatent_dims, so that we can specify the desired\\ndimensionality of the representations.\\nclass AutoEncoder(nn.Module):\\n    def __init__(self, in_channels, \\nlatent_dims):\\n        super().__init__()\\n        self.encoder = Encoder(in_channels, \\nlatent_dims)\\n        self.decoder = Decoder(in_channels, \\nlatent_dims)\\n    def encode(self, x):\\n        return self.encoder(x)\\n    def decode(self, x):'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='return self.decoder(x)\\n    def forward(self, x):\\n        return self.decode(self.encode(x))\\nThe training loop is the same as before, but we put it inside a\\nfunction to reuse it and call it whenever needed.\\ndef train(model, num_epochs=10, lr=1e-4):\\n    optimizer = \\ntorch.optim.AdamW(model.parameters(), lr=lr, \\neps=1e-5)\\n    model.train()  # Put model in training \\nmode\\n    losses = []\\n    for _ in (progress := trange(num_epochs, \\ndesc=\"Training\")):\\n        for _, batch in (\\n            inner := tqdm(\\n                enumerate(train_dataloader), \\ntotal=len(train_dataloader)\\n            )\\n        ):\\n            batch = batch.to(device)\\n            # Pass through the model and'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='obtain another set of images\\n            preds = model(batch)\\n            # Compare the prediction with the \\noriginal images\\n            loss = F.mse_loss(preds, batch)\\n            # Display loss and store for \\nplotting\\n            inner.set_postfix(loss=f\"\\n{loss.cpu().item():.3f}\")\\n            losses.append(loss.item())\\n            # Update the model parameters \\nwith the optimizer based on this loss\\n            loss.backward()\\n            optimizer.step()\\n            optimizer.zero_grad()\\n        progress.set_postfix(loss=f\"\\n{loss.cpu().item():.3f}\", lr=f\"{lr:.0e}\")\\n    return losses\\nWe create and train an AutoEncoder with just two latent\\nvariables. The training curve is shown in Figure 3-10.\\nae_model = AutoEncoder(in_channels=1, \\nlatent_dims=2)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='ae_model.to(device)\\nlosses = train(ae_model)\\nplt.plot(losses)\\nplt.xlabel(\"Step\")\\nplt.ylabel(\"Loss\")\\nplt.title(\"Training Loss Curve (two latent \\ndimensions)\")\\nplt.show()'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 3-10. Training loss of an AutoEncoder with just two\\nlatent variables.\\nLet’s once again load the trained model and look at some\\nreconstructions.\\nae_model.eval()\\nwith torch.inference_mode():\\n    eval_batch = next(iter(eval_dataloader))\\n    predicted = \\nae_model(eval_batch.to(device)).cpu()\\nbatch_vs_preds = torch.cat((eval_batch, \\npredicted))\\nshow_images(batch_vs_preds, imsize=1, \\nnrows=2)\\nFigure 3-11. Reconstructions from an AutoEncoder that uses\\ntwo latent variables'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The results, shown in Figure 3-11, are not quite as good as\\nbefore, but remember, we now use just two ﬂoats to represent a\\n28x28 handwritten image. We see some confusion with the 4s,\\nthe 5s, and the 9s, but overall, the recovered images are very\\nsimilar to the input ones!\\nVisualizing the Latent Space\\nWe used just two dimensions for the latent space to visualize its\\nstructure easily. In Figure 3-12, we represent all the encoded\\nvectors from the test dataset, using the label column to assign\\ndiﬀerent colors to each class. The ﬁrst value of the encoded\\nvectors will be displayed on the X-axis, and the second value\\nwill be represented on the Y-axis.\\nimages_labels_dataloader = \\nDataLoader(mnist[\"test\"], batch_size=512)\\nimport pandas as pd\\ndf = pd.DataFrame(\\n    {\\n        \"x\": [],\\n        \"y\": [],\\n        \"label\": [],'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='}\\n)\\nfor batch in tqdm(\\n    iter(images_labels_dataloader), \\ntotal=len(images_labels_dataloader)\\n):\\n    encoded = \\nae_model.encode(batch[\"image\"].to(device)).cpu()\\n    new_items = {\\n        \"x\": [t.item() for t in encoded[:, \\n0]],\\n        \"y\": [t.item() for t in encoded[:, \\n1]],\\n        \"label\": batch[\"label\"],\\n    }\\n    df = pd.concat([df, \\npd.DataFrame(new_items)], ignore_index=True)\\n  0%|          | 0/20 [00:00<?, ?it/s]\\nplt.figure(figsize=(10, 8))\\nfor label in range(10):\\n    points = df[df[\"label\"] == label]\\n    plt.scatter(points[\"x\"], points[\"y\"],'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='label=label, marker=\".\")\\nplt.legend();'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 3-12. Visualization of the MNIST AutoEncoder 2D latent\\nspace.\\nThe AutoEncoder has done a good job at separating diﬀerent\\nareas of the latent space for the various images in our dataset.\\nNote, for example, how images representing the number 0\\n(dark blue dots) are distant from the representations of ones\\n(orange dots). Remember that no information about the image\\nlabels was used during training, but even so, data points were\\nautomatically grouped in diﬀerent regions according to their\\nvisual features. However, this process took place in an entirely\\nunconstrained way, so there is no guarantee about the shape or\\nstructure of the latent space.\\nTherefore, the latent space is rich enough to capture the\\nrelevant image features in our dataset, but it’s still not clear\\nhow we can use it for generative purposes. Ideally, to generate\\nnew images similar to the ones in MNIST, we’d want to discard\\nthe encoder and supply random samples from the latent space\\nto the decoder. However, there are some issues we can see in\\nthe plot:\\nThe space taken by the representation is spread out in all\\ndirections.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='There are many overlaps at the center and big regions of\\nempty space.\\nThe plot is non-symmetric: negative values in the Y-axis are\\nused more than positive ones.\\nThis makes it challenging to select appropriate regions in the\\nlatent space that could lead to great generations. Let’s see an\\nexample of image generation with the decoder. We begin by\\ngenerating random latent samples (usually denoted z).\\nN = 16  # We\\'ll generate 16 points\\nz = torch.rand((N, 2)) * 8 - 4\\nLet’s visualize the generated latent samples overlayed on top of\\nthe latent space representation we showed before (Figure 3-13).\\nplt.figure(figsize=(10, 8))\\nfor label in range(10):\\n    points = df[df[\"label\"] == label]\\n    plt.scatter(points[\"x\"], points[\"y\"], \\nlabel=label, marker=\".\")\\nplt.scatter(z[:, 0], z[:, 1], label=\"z\",'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='marker=\"s\", color=\"black\")\\nplt.legend();\\nFigure 3-13. A few samples generated randomly on the latent\\nspace.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Finally, let’s ask the decoder to generate images from the latent\\nsamples we just created.\\nae_decoded = ae_model.decode(z.to(device))\\nshow_images(ae_decoded.cpu(), imsize=1, \\nnrows=1, suptitle=\"AutoEncoder\")\\nFigure 3-14. Images generated from the random samples.\\nThe generated images in Figure 3-14 are reasonable in the\\nplaces where the samples are close to one of the regions carved\\nby the model in the latent space, but they are much less\\nconvincing when they lie outside those areas. In addition, note\\nthat some numbers will be overrepresented because their\\nassigned regions in latent space are larger.\\nThe next section discusses how we can use a diﬀerent type of\\nAutoEncoder to impose some order in latent space and how this\\ncan make generation easier.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Before moving on, here are some (progressively more\\nchallenging) exercises that you can tackle now (or later on at\\nthe end of the chapter) to reinforce your understanding of the\\nconcepts:\\n1. How well does generation work if the model is trained with\\n16 latent dimensions?\\n2. Train the model again with the same parameters we used\\n(just run the code shown in the chapter) but with diﬀerent\\nrandom number initialization , and visualize the latent\\nspace. Chances are that the shapes and structure are\\ndiﬀerent. Is this something you would expect? Why?\\n3. How good are the image features extracted by the encoder?\\nDiscard the decoder part of the AutoEncoder and build a\\nnumber classiﬁer on top of the encoder. You can, for\\nexample, train a couple of linear layers with a non-linearity\\nbetween them. The ﬁnal linear layer should output a vector\\nwith ten dimensions representing the ten labels in the\\ndataset. Train only these layers without updating the\\nweights of the encoder. What accuracy can you get? How\\ndoes the model with 16 latent dimensions compare to the\\none with just two?\\n5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Variational AutoEncoders (VAEs)\\nIn the previous section, we explored how a simple AutoEncoder\\ncan learn eﬃcient representations of the input data in a lower-\\ndimensional latent space. The AutoEncoder can faithfully\\nencode any sample and recover (or decode) it later. This works\\ngreat for feature extraction or data representation but is not\\nwell suited for generating new samples. As discussed before,\\nthe reason is that the AutoEncoder is not incentivized to\\nseparate the representations into consistent portions of the\\nlatent space. As we saw, representations of similar inputs are\\nusually clustered close together, but there’s a signiﬁcant amount\\nof overlap and empty space, as well as substantial variability in\\nthe amount of latent space dedicated to each class. If we choose\\na random point in the latent space, we can’t faithfully predict\\nwhat result will come up after we run it through the decoder.\\nVariational AutoEncoders (VAEs) address this by learning a\\nprobability distribution for each feature in the latent space.\\nInstead of mapping inputs to speciﬁc points, VAEs represent\\neach feature with a Gaussian distribution , capturing the\\nvariability of that feature within the data.\\n6'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Consider, for example, a dataset consisting of images of multiple\\nbreeds of dogs and cats. We don’t know what the features\\nextracted by the encoder will be, but we could imagine that\\nsome could be used to represent characteristics such as furry\\npatches, eyes, ears, legs, or tails. These may have a great degree\\nof overlap between all the images in the dataset (all these\\nanimals have two ears, four legs, and a tail), but there’s also\\nvariability in how ears look in dogs versus how they look in\\ncats. Our \"ear\" feature could be represented by a Gaussian\\ndistribution that covers all of this variability, with the mean of\\nthe distribution representing the average shape of an animal\\near. If we move away from the mean towards diﬀerent\\ndirections, we’ll get a continuous and homogeneous transition\\ntowards diﬀerent ear shapes that may appear in various\\nbreeds.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 3-3. Figure 3-15. Conceptual diagram of a VAE. VAEs learn Gaussian\\nrepresentations of the features and describe them through their means and\\nvariances. Points 𝒵  in the latent space are sampled from the predicted Gaussian\\ndistributions\\nThis approach, conceptually represented in Figure 3-14, creates\\na more structured latent space, where sampling from these\\ndistributions allows us to generate new, plausible instances. Just\\nas with AutoEncoders, class information is typically not used in\\nVAEs.  Let’s see how we can code and train them.\\nVAE Encoders and Decoders\\nVAE encoders are very similar to the basic encoders we saw in\\nthe previous section. In our example, we used a few\\nconvolutional layers and a linear layer to project to the desired\\nsize of the latent representation.\\n7'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='To create our ﬁrst VAE Encoder, we’ll use the same architecture.\\nThe only diﬀerence is that, instead of a linear layer to predict\\nthe latent space of an image, we want to use linear layers to\\nlearn the distribution. A distribution is characterized by two\\nparameters, the mean and the variance, so we’ll need two\\nlinear layers:\\nOne of the linear layers will represent the mean of the\\ndistribution we are trying to learn.\\nThe other linear layer will learn the variance of the\\ndistribution.\\nIn terms of code, this is what it looks like:\\nclass VAEEncoder(nn.Module):\\n    def __init__(self, in_channels, \\nlatent_dims):\\n        super().__init__()\\n        self.conv_layers = nn.Sequential(\\n            conv_block(in_channels, 128),\\n            conv_block(128, 256),\\n            conv_block(256, 512),\\n            conv_block(512, 1024),\\n        )\\n        # Define fully connected layers for \\n8'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='mean and log-variance\\n        self.mu = nn.Linear(1024, \\nlatent_dims)\\n        self.logvar = nn.Linear(1024, \\nlatent_dims)\\n    def forward(self, x):\\n        bs = x.shape[0]\\n        x = self.conv_layers(x)\\n        x = x.reshape(bs, -1)\\n        mu = self.mu(x)\\n        logvar = self.logvar(x)\\n        return (mu, logvar)\\nYou’ll ﬁnd minimal diﬀerences if you compare the code snippet\\nwith the Encoder example from the previous section. We use\\ntwo linear layers instead of one to compute two diﬀerent values\\nfrom the same representation extracted by the convolutional\\nlayers, and we return those two values in the forward()\\nmethod.\\nThe purpose of these two computed values is to represent the\\nmean and the variance of a probability distribution. However,\\nthey are initially just two identical linear layers. The challenge\\nis to ensure that they learn to represent what we intend—mean\\nand variance—during the training process, as we’ll show.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Before that, note that the computed value mu represents the\\nmean, and logvar represents the logarithm of the variance.\\nWe use mu for the mean after the Greek letter μ, frequently\\nused in math notation to represent the mean of a normal\\ndistribution. The reason for using logvar rather than directly\\noutputting the variance is primarily numerical stability, as we’ll\\nexplain later.\\nHow about the decoder? It turns out we don’t need to make any\\nchanges to it. The diﬀerence between a VAE and a simple\\nAutoEncoder lies in the way we ﬁnd a point in latent space to\\nrepresent an input item, but the mission of the decoder is the\\nsame: given a point in latent space (z), show the pixels whose\\nencoded representation is most similar to z. In the case of the\\nAutoEncoder, z is a linear projection of the features extracted\\nby the convolutional layers. When we use a VAE encoder, we\\nobtain a normal distribution, and then we sample from that\\ndistribution to obtain z. Therefore, we can use the same\\nDecoder class we used in the previous section, but we do need\\nto modify the model to sample from the distribution.\\nSampling from the Encoder Distribution\\nOur updated VAE Encoder returns the mean and variance of a\\nnormal distribution that tries to match the input data'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='representations. To obtain a decoded output, we must sample\\nfrom that distribution, as shown in the following snippet.\\nclass VAE(nn.Module):\\n    def __init__(self, in_channels, \\nlatent_dims):\\n        super().__init__()\\n        self.encoder = \\nVAEEncoder(in_channels, latent_dims)  \\n        self.decoder = Decoder(in_channels, \\nlatent_dims)\\n    def encode(self, x):\\n        # Returns mu, log_var\\n        return self.encoder(x)\\n    def decode(self, z):\\n        return self.decoder(z)\\n    def forward(self, x):\\n        # Obtain parameters of the normal \\n(Gaussian) distribution\\n        mu, logvar = self.encode(x)  \\n        # Sample from the distribution\\n        std = torch.exp(0.5 * logvar)  \\n        z = self.sample(mu, std)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='# Decode the latent point to pixel \\nspace\\n        reconstructed = self.decode(z)  \\n        # Return the reconstructed image, and \\nalso the mu and logvar\\n        # so we can compute a distribution \\nloss\\n        return reconstructed, mu, logvar  \\n    def sample(self, mu, std):\\n        # Reparametrization trick\\n        # Sample from N(0, I), translate and \\nscale\\n        eps = torch.randn_like(std)  \\n        return mu + eps * std\\nWe use latent_dims dimensions to represent the mean\\nand log variance of the distributions.\\nThe encoder computes two variables now: mean and log\\nvariance.\\nCompute the standard deviation from the log variance.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Sample from the distribution using the computed mean and\\nstandard deviation.\\nThe decoder converts the sample to an image.\\nWe return not only the reconstructed image but also the\\nmean and the log variance.\\nReparametrization trick: sample from a standard normal\\ndistribution and then translate and scale.\\nSo far, we’ve appealed to intuition to explain how VAEs work. If\\nyou allow us a brief detour into a couple of statistical concepts,\\nwe can quickly review the code above while trying to be more\\nprecise in terminology. Feel free to skip this section or come\\nback to it later. It’s not necessary to understand it to use or train\\nVAEs, but it may help if you want to dive deeper and read\\npapers about the topic.\\nFirst, note that we use multidimensional Gaussian distributions,\\nnot just real-valued 1D normal curves. In this VAE encoder\\nexample, we use latent_dims for both the mean and the\\nvariance. We could use an arbitrary number of dimensions, like\\nthe 16 we used for our ﬁrst MNIST AutoEncoder, or 2 for easier\\nvisualization. A real-valued (1D) normal distribution is denoted\\nas N(μ,σ2) and is deﬁned by two magnitudes: µ, the mean of'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='the distribution, and σ, the standard deviation, which is the\\nsquare root of the variance σ2.\\nOne useful characteristic of normal distributions is that all of\\nthem can be expressed in terms of the standard normal\\ndistribution, whose mean is 0 and variance is 1, by translating\\nand scaling it:\\nN(μ,σ2)=μ+σN(0,1)\\nThis means that to obtain a sample from an arbitrary normal\\ndistribution N(μ,σ2), we can instead sample from N(0,1),\\nthen multiply by σ and add µ. This is called reparametrization\\nand will be quite helpful when we look into diﬀusion models.\\nMultidimensional Gaussian distributions are called\\nmultivariate. They can still be deﬁned by two parameters, with\\nthe diﬀerence that µ is a vector and σ (the covariance matrix,\\nnow denoted with Σ) is a matrix. Hence, the distribution is\\ndeﬁned as N(μ,Σ). If the distribution is independent in all\\ndimensions, meaning each variable is uncorrelated with the\\nothers and has the same variance, it is called isotropic. In an\\nisotropic multivariate Gaussian distribution, the covariance\\nmatrix Σ is a diagonal matrix where all the items in the\\ndiagonal are equal and can be expressed as σ2I, where I is the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='identity matrix. The standard multivariate Gaussian is then\\nexpressed as N(0,I).\\nOur VAE example above models a multivariate, isotropic\\nGaussian distribution, as there’s no reason to think that sample\\ncoordinates depend on each other (and it’s simpler!). This\\nmeans we can use the so-called reparametrization trick to\\nsample from the standard Gaussian, then translate and scale to\\nobtain the latent space vector we’ll decode.\\nThe reparametrization trick is not just used for convenience –\\nit’s a crucial ingredient for training. When we use the\\nexpression mu + eps * std, where eps is a sample from the\\nstandard Gaussian, the gradients with respect to the model\\ninputs are independent of the stochastic process (sampling\\nfrom a distribution) and can, therefore, be computed. This\\nmakes it possible to train the model with the familiar gradient-\\ndescent methods we follow to train any neural network.\\nSpeaking of stability, our model predicts the log of the variance\\ninstead of the variance to increase numerical stability and\\nfacilitate training. Mathematically, it makes no diﬀerence to\\ncompute one or the other. In practice, we know the variance is\\nalways a positive number, usually close to 0. However, there’s\\nno reason for the model to produce positive and small values'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='when we start training. Furthermore, numbers are represented\\nin ﬂoating point format, which makes it diﬃcult to discriminate\\nvalues very close together. By taking the log, we get two\\nbeneﬁts: - We expand the range of acceptable values to -∞, so\\nthe model has a lot more latitude to express the results with\\nﬂoating point values. - We ensure the variance is always\\npositive, because it’s the exponential of the logvar.\\nNOTE\\nGaussian distributions are frequently used in many other areas of Machine Learning,\\nsometimes for convenience because their mathematical characteristics are well\\nknown. In Chapter 4 we’ll explore their use to model the noise corruption that is an\\nessential part of diﬀusion models.\\nTraining the VAE\\nThe key to training the VAE is the loss function. In the\\nAutoEncoder section, the loss function we used measured the\\ndiﬀerence between the reconstructed and original images. We\\nstill want the reconstructed images to resemble the originals as\\nmuch as possible, but we now introduce a second factor to the\\nloss to impose the VAE constraint we’ve been talking about: we\\nwant the features to (more or less) follow a Gaussian\\ndistribution. The way we achieve that goal is by using'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='something called the Kullback–Leibler divergence, also known as\\nrelative entropy, between the distributions. Kullback-Leibler\\ndivergence, or KL divergence (KLD), is a way to measure how\\nmuch a probability distribution diﬀers from another one. In the\\ncase of multivariate isotropic Gaussian distributions, it can be\\nshown that KLD can be computed as:\\nDKL[N(μ,σ2)||N(0,1)] =−1\\n2 ∑(1+log(σ2)−μ2 −σ2)\\nTo combine the two loss factors, we create a loss function called\\nvae_loss that receives the original images and the outputs\\nfrom the encoder  and does the following:\\nCompute the reconstruction loss as the mean squared error\\nbetween the pixels generated by the decoder and the\\noriginal images. This loss factor is identical to the one we\\nused to train AutoEncoders.\\nCompute the KLD term following the equation we just\\npresented.\\nAdd them both together. We could assign more importance\\nto one or the other to balance reconstruction ﬁdelity and\\nconformance to a Gaussian distribution. We’ll sum them\\ntogether for now, but playing with this balance is a great\\nexperiment to try.\\n9'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The loss function returns three values: the total loss, the\\nreconstruction loss, and the KLD term. We only need the total\\nloss for training, but we keep track of the others for\\nvisualization and analysis.\\ndef vae_loss(batch, reconstructed, mu, \\nlogvar):\\n    bs = batch.shape[0]\\n    # Reconstruction loss from the pixels - 1 \\nper image\\n    reconstruction_loss = F.mse_loss(\\n        reconstructed.reshape(bs, -1),\\n        batch.reshape(bs, -1),\\n        reduction=\"none\",\\n    ).sum(dim=-1)\\n    # KL-divergence loss, per input image\\n    kl_loss = -0.5 * torch.sum(1 + logvar - \\nmu.pow(2) - logvar.exp(), dim=-1)\\n    # Combine both losses and get the mean \\nacross images\\n    loss = (reconstruction_loss + \\nkl_loss).mean(dim=0)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='return (loss, reconstruction_loss, \\nkl_loss)\\nNow that we’ve deﬁned the loss, we can proceed to train the\\nmodel. We’ll use the total loss to update the model weights, but\\nwe’ll also keep track of the reconstruction loss and the KLD\\nterm to understand how the model is learning.\\ndef train_vae(model, num_epochs=10, lr=1e-4):\\n    model = model.to(device)\\n    losses = {\\n        \"loss\": [],\\n        \"reconstruction_loss\": [],\\n        \"kl_loss\": [],\\n    }\\n    model.train()\\n    optimizer = \\ntorch.optim.AdamW(model.parameters(), lr=lr, \\neps=1e-5)\\n    for _ in (progress := trange(num_epochs, \\ndesc=\"Training\")):\\n        for _, batch in (\\n            inner := tqdm(\\n                enumerate(train_dataloader), \\ntotal=len(train_dataloader)\\n            )'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='):\\n            batch = batch.to(device)\\n            # Pass through the model\\n            reconstructed, mu, logvar = \\nmodel(batch)\\n            # Compute the losses\\n            loss, reconstruction_loss, \\nkl_loss = vae_loss(\\n                batch, reconstructed, mu, \\nlogvar\\n            )\\n            # Display loss and store for \\nplotting\\n            inner.set_postfix(loss=f\"\\n{loss.cpu().item():.3f}\")\\n            \\nlosses[\"loss\"].append(loss.item())\\n            \\nlosses[\"reconstruction_loss\"].append(\\n                \\nreconstruction_loss.mean().item()\\n            )\\n            \\nlosses[\"kl_loss\"].append(kl_loss.mean().item())'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='# Update model parameters based \\non the total loss\\n            optimizer.zero_grad()\\n            loss.backward()\\n            optimizer.step()\\n        progress.set_postfix(loss=f\"\\n{loss.cpu().item():.3f}\", lr=f\"{lr:.0e}\")\\n    return losses\\nvae_model = VAE(in_channels=1, latent_dims=2)\\nlosses = train_vae(vae_model, num_epochs=10, \\nlr=1e-4)\\nLet’s analyze the three loss terms we stored during training:\\nReconstruction loss: measures how much the output\\nimages resemble the originals.\\nKLD: measures how well the features follow a Gaussian\\ndistribution.\\nTotal loss: The addition of the two previous losses.\\nfor k, v in losses.items():\\n    plt.plot(v, label=k)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='plt.legend();\\nFigure 3-16. Loss components while training a VAE. The total\\nloss is the sum of the reconstruction loss plus the KL term.\\nEven though the total loss was taken as the sum of the two\\nlosses, it is dominated, in this case, by the reconstruction loss\\nbecause its magnitude is much larger than KLD (Figure 3-16).\\nLet’s plot the total loss and its terms separately in Figures 3-17,\\n3-18, and 3-19 to see how they evolve during training.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 3-17. VAE loss component during VAE training.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 3-18. KL loss component during VAE training.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 3-19. Reconstruction loss component during VAE\\ntraining.\\nNote it has a peculiar form: it spikes at the beginning of\\ntraining, then decreases, then slowly increases again. Why does\\nthat happen? We think these are, conceptually, the phases KLD\\ngoes through in training:\\nWhen training starts, the VAE encoder and decoder are\\ninitialized with random weights, and the model knows'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='nothing about the input data. Therefore, the outputs\\nresemble a random distribution, and the KLD is very low.\\nWhen the network has seen just a few batches of data, the\\nreconstructions will be low-quality but not random\\nanymore, and KLD spikes.\\nStill in the early training stages, the model knows just\\nenough to represent the average characteristics of the input\\ndata. The KLD term drives the model to produce outputs\\nthat get closer and closer to a Gaussian distribution,\\ndecreasing the KLD loss.\\nAs the encoder and decoder learn to produce more faithful\\nrepresentations, it becomes harder to improve quality\\nwhile still matching a Gaussian distribution. The\\nreconstruction loss dominates, and KLD increases, but\\nthere’s a balance between the two. If we increased the\\nimportance of KLD in the loss term, we could achieve better\\nGaussian conformance, but the cost would be worse pixel\\nrepresentations.\\nLet’s reconstruct some images and visualize the results in\\nFigure 3-20.\\nvae_model.eval()\\nwith torch.inference_mode():\\n    eval_batch = next(iter(eval_dataloader))'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='predicted, mu, logvar = (v.cpu() for v in \\nvae_model(eval_batch.to(device)))\\nbatch_vs_preds = torch.cat((eval_batch, \\npredicted))\\nshow_images(batch_vs_preds, imsize=1, \\nnrows=2)\\nFigure 3-20. VAE reconstructions from MNIST samples.\\nVisual results are worse than in the AutoEncoder case because\\nthe model not only has to learn how to encode the input images\\nbut is also constrained to trying to avoid diverging too much\\nfrom a normal distribution. Let’s explore what happened after\\nwe added this new goal.\\nIf we plot the means of the standard distribution encoded by\\nthe model for the test set, we obtain a slightly better-behaved\\nresult than in the AutoEncoder case (Figure 3-21). Results are'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='now better centered around zero and don’t get as far away as in\\nthe AutoEncoder case. Areas taken by the diﬀerent classes are\\nof similar size, although there’s still overlap between similar-\\nlooking numbers.\\ndf = pd.DataFrame(\\n    {\\n        \"x\": [],\\n        \"y\": [],\\n        \"label\": [],\\n    }\\n)\\nfor batch in tqdm(\\n    iter(images_labels_dataloader), \\ntotal=len(images_labels_dataloader)\\n):\\n    mu, _ = \\nvae_model.encode(batch[\"image\"].to(device))\\n    mu = mu.to(\"cpu\")\\n    new_items = {\\n        \"x\": [t.item() for t in mu[:, 0]],\\n        \"y\": [t.item() for t in mu[:, 1]],\\n        \"label\": batch[\"label\"],\\n    }'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='df = pd.concat([df, \\npd.DataFrame(new_items)], ignore_index=True)\\n  0%|          | 0/20 [00:00<?, ?it/s]\\nplt.figure(figsize=(10, 8))\\nfor label in range(10):\\n    points = df[df[\"label\"] == label]\\n    plt.scatter(points[\"x\"], points[\"y\"], \\nlabel=label, marker=\".\")\\nplt.legend();'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 3-21. Visualization of the VAE latent space.\\nThe main advantage of trying to ﬁt the encoder to a normal\\ndistribution is that we should now be able to sample random\\ndata from the distribution and, hopefully, obtain images that'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='resemble our input dataset. Let’s see how it works for both the\\nAutoEncoder and the VAE.\\nz = torch.normal(0, 1, size=(10, 2))\\nae_decoded = ae_model.decode(z.to(device))\\nvae_decoded = vae_model.decode(z.to(device))\\nshow_images(ae_decoded.cpu(), imsize=1, \\nnrows=1, suptitle=\"AutoEncoder\")\\nshow_images(vae_decoded.cpu(), imsize=1, \\nnrows=1, suptitle=\"VAE\")\\nFigure 3-22. AutoEncoder vs VAE generation from normally-\\ndistributed features.\\nWe sample pure random data from a normal distribution and\\nthen use the AutoEncoder (top row of Figure 3-22) and VAE'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='(bottom row) decoders to display how those points would be\\nreconstructed. Of course, we will see diﬀerent reconstructions,\\nas the AutoEncoder and VAE were trained separately, and they\\nallocated diﬀerent portions of the latent space to each class.\\nThe results from the VAE are more number-like than those of\\nthe AutoEncoder. This is because the VAE training process\\nencouraged the encoder to not veer away too much from a\\nnormal distribution, while the AutoEncoder had no such\\nrestriction. Can you run the sampling code a few times and\\nobserve it yourself?\\nA fun exercise is to show how representations morph as we\\ntravel in 2D through latent space. We can ﬁx a vertical line at\\n-0.8 (Figure 3-23) and explore diﬀerent points in this line. If\\nwe select points from y = -2 to y = 2, we see (Figure 3-24)\\nthat the reconstructions match the latent space areas that\\nrepresent various numbers.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 3-23. VAE latent space: focusing on samples where x =\\n0.8.\\nimport numpy as np'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='with torch.inference_mode():\\n    inputs = []\\n    for y in np.linspace(-2, 2, 10):\\n        inputs.append([-0.8, y])\\n    z = torch.tensor(inputs, \\ndtype=torch.float32).to(device)\\n    decoded = vae_model.decode(z)\\nshow_images(decoded.cpu(), imsize=1, nrows=1)\\nFigure 3-24. Reconstructions from various samples whose x=0.8.\\nLet’s expand the idea of exploring the latent space to a 2D grid\\n(Figure 3-25). This is a visual representation of what the model\\nlearned, and it’s interesting to see that transitions are not too\\ncrazy!\\ninputs = []\\nfor x in np.linspace(-2, 2, 20):\\n    for y in np.linspace(-2, 2, 20):\\n        inputs.append([x, y])\\nz = torch.tensor(inputs, \\ndtype=torch.float32).to(device)\\ndecoded = vae_model.to(device).decode(z)\\n1 0'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='show_images(decoded.cpu(), imsize=0.4, \\nnrows=20)\\nFigure 3-25. 2D exploration of the VAE latent space.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Once again, here are some exercises to dive into these topics:\\n1. When we trained the VAE, we added the reconstruction and\\nKL-divergence losses. However, both have diﬀerent scales.\\nWhat will happen if we give more importance to one vs.\\xa0the\\nother? Can you run a few experiments and explain the\\nresults?\\n2. The VAE we explored in this section only uses two\\ndimensions to represent the distribution’s mean and the\\nlogvar. Can you repeat a similar exploration using 16\\ndimensions?\\n3. Humans are trained to look at faces and easily identify\\nunrealistic features. Can you train an AutoEncoder and a\\nVAE for a dataset containing faces and analyze the results?\\nYou can start with the Frey Face dataset that was used in\\nthe VAE paper – it’s a homogenous set of monochrome faces\\nfrom the same person sporting diﬀerent facial expressions.\\nIf you want to be more ambitious, you can try your hand at\\nthe CelebFaces dataset, easily usable from the Hugging Face\\nHub at https://huggingface.co/datasets/nielsr/CelebA-faces.\\nAnother interesting example is the Oxford pets dataset, also\\navailable on the Hugging Face Hub.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='VAEs for Generative Modeling\\nTraining an encoder constrained to be close to a distribution is\\na key insight into VAEs and one of the cornerstones of\\ngenerative modeling. With AutoEncoder, we could learn\\neﬃcient representations of a dataset. Still, there was no\\nguarantee that the latent space learned by the model would\\nhelp us generate new data that resembled the original. By\\naiming to learn a distribution, VAEs allow us to generate\\nplausible new images, simply starting from random points in\\nthe latent space. This idea of sampling from random noise takes\\na step further in diﬀusion models, which, in addition to\\nstatistical modeling, introduces the concept of iterative\\nreﬁnement. We’ll discuss them at length in future chapters.\\nCLIP\\nSo far, we’ve focused on image data. With CLIP (Contrastive\\nLanguage-Image Pre-training), we’ll steer away from\\nAutoEncoder/VAE methods and explore a diﬀerent technique to\\nmatch images with text. The process is similar in the sense that\\nwe aim to create rich representations from the input data, but\\nthe method is diﬀerent, and, more importantly, it can deal with\\nboth images and text at once.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Our dataset now consists of two modalities: images and text\\ncaptions describing those images. The goal of CLIP is: given that\\ntraining data, would it be possible to create a model that\\nmeasures how accurately a text describes the contents of an\\nimage for an arbitrary text-image pair that we supply? Turns\\nout we can! The key, as usual, is the loss function we use.\\nContrastive Loss\\nCLIP was introduced by OpenAI in 2021 . It was part of the\\ntools they developed to create the initial DALL·E, an impressive\\ntext-to-image model that took the world by storm. Even though\\nDALL·E was not open-sourced (the model weights remain\\nprivate), CLIP was. This was extraordinary news, as the ability\\nto relate images with text enables quite a few tricks. CLIP and\\nCLIP-like models have since become indispensable tools in the\\ngenerative landscape.\\nCLIP uses a loss function called contrastive loss. The way it\\nworks is shown schematically in Figure 3-26, inspired by\\nOpenAI’s CLIP blog post. The training dataset consists of\\nmillions of images with their associated descriptions or\\ncaptions. For each image-caption pair, we encode the image\\nusing any image encoder to obtain an embedding vector in the\\nencoder’s latent space. Each of the ﬁgure’s I , I , …, I  boxes\\n1 1 \\n1 2 N'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='represent embedding vectors for diﬀerent images. The text is\\nalso encoded, usually with a transformer model like the ones\\nwe saw in Chapter 2. Crucially, we use encoders such that the\\ndimensions of the embedding vectors of images and text are the\\nsame. This way, we can calculate the inner product (or dot\\nproduct) between the text and image embeddings to determine\\nhow close they are.\\nTraining progresses by supplying a lot of image-text pairs in the\\nsame batch. We compute the dot products of all the image\\nembeddings in the batch with all the text embeddings in the\\nsame batch and try to maximize the product of the items\\noriginally from the same pair (i.e., the blue diagonal in the\\nimage) while minimizing the rest. This way, texts and images\\nthat are similar will be represented by vectors close in the\\nlatent space, while diﬀerent concepts will be far away in other\\nregions.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 3-4. Figure 3-26. CLIP: Contrastive Pre-Training Process. Image from\\nhttps://openai.com/research/clip\\nWHY USE THE DOT PRODUCT?\\nIf you don’t remember or haven’t studied calculus before, an important relationship\\nthat holds in vector spaces is:\\nA⋅B=|A||B| cos(θ)\\nThis means that the dot product between two vectors is the same as the product of\\nthe lengths of the two vectors multiplied by the angle between them. This can be\\nshown using the law of cosines from Euclidean geometry and the deﬁnition of dot\\nproduct as A⋅B=∑N\\nn=1aibi. In our brief discussion, we didn’t mention that the\\nvector embeddings are normalized to unit length; therefore, their dot product is the\\nangle between the vectors. This is called cosine similarity, and it measures the\\nproximity between two vectors. Even though the vectors have many dimensions, the\\ndot product is just a scalar (i.e., a real number) which can be used as a similarity\\nscore.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Training a CLIP model requires huge amounts of data and lots\\nof compute. The original CLIP models released by OpenAI used\\na proprietary dataset of 400 million image-text pairs and large\\nbatch sizes of 32K pairs. Since then, there have been multiple\\neﬀorts of other models that can encode diﬀerent modalities:\\nOpenCLIP is an open implementation of CLIP. It was used to\\ntrain several models with diﬀerent datasets, image\\nresolutions, and model sizes.\\nCLAP (Contrastive Language-Audio Pretraining) allows\\nobtaining representations of audio rather than images. This\\ncan be used to train models that generate audio, as we’ll\\nexplore in Chapter 9.\\nUsing CLIP, step by step\\nWe’ll now use pre-trained CLIP models to gain intuition about\\nhow they work and see a few examples of what they can be\\nused for. In the following example, we use clip-vit-large-\\npatch14, one of the OpenAI CLIP models. It uses a Vision\\nTransformer (ViT) as the image encoder (other versions exist\\nthat use the ResNet convolutional architecture). There are\\nlarger and smaller versions as well; you can experiment with a\\nfew of them to see how they work on your hardware in terms of\\nspeed, memory, and quality.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Consider the photo in Figure 3-27, a royalty-free resource from\\nPixabay showing an adorable lion cub looking at us behind a\\ntree branch. Let’s see how we can use it with CLIP.\\nFigure 3-5. Figure 3-27. Photo of a cute lion cub behind a branch\\nSo far we’ve only used transformers to load text models, but we\\ncan also use them to work with other modalities. Same as we\\nhad GPT2LMHeadModel in the previous chapter, we can use\\nCLIPModel. Let’s try it out:\\nimport requests\\nfrom PIL import Image'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from transformers import CLIPModel, \\nCLIPProcessor\\nfrom genai.core import SampleURL\\nclip = \\nCLIPModel.from_pretrained(\"openai/clip-vit-\\nlarge-patch14\").to(device)\\nprocessor = \\nCLIPProcessor.from_pretrained(\"openai/clip-\\nvit-large-patch14\")\\nurl = SampleURL.LionExample\\nimage = Image.open(requests.get(url, \\nstream=True).raw)\\nThe CLIP model we loaded, clip, contains two components: a\\nvision model to encode images and a text model to encode text.\\nThe CLIPProcessor, which you can think of as the tokenizer\\nequivalent, prepares input data to match the pre-processing\\nsteps used during model training: image resizing,\\nnormalization, etc. This is essential to ensure that the inputs we\\nprovide for inference have the same characteristics as the data\\nthe model saw when it was trained. Let’s process the image\\nﬁrst.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='image_inputs = processor(images=image, \\nreturn_tensors=\"pt\")\\npixel_values = image_inputs[\"pixel_values\"]\\npixel_values.shape, pixel_values.min(), \\npixel_values.max()\\n(torch.Size([1, 3, 224, 224]), \\ntensor(-1.7923), tensor(2.0179))\\nThe image has been resized to a square size of 224x224 and\\nnormalized. You can examine the image processor for the full\\nset of applied transformations. Note that the strategy used for\\nresizing is to center-crop (i.e., select a square-sized block\\naround the center of the image and then downscale it to\\n224x224). This method cuts the left and right portions from\\nlandscape images or the top and bottom bands from images\\nwith a portrait aspect ratio. Be mindful that this may result in\\ninformation loss if some of the subjects you want to examine\\nare in those areas.\\nprocessor.image_processor\\nCLIPImageProcessor {\\n  \"crop_size\": {'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='\"height\": 224,\\n    \"width\": 224\\n  },\\n  \"do_center_crop\": true,\\n  \"do_convert_rgb\": true,\\n  \"do_normalize\": true,\\n  \"do_rescale\": true,\\n  \"do_resize\": true,\\n  \"image_mean\": [\\n    0.48145466,\\n    0.4578275,\\n    0.40821073\\n  ],\\n  \"image_processor_type\": \\n\"CLIPImageProcessor\",\\n  \"image_std\": [\\n    0.26862954,\\n    0.26130258,\\n    0.27577711\\n  ],\\n  \"resample\": 3,\\n  \"rescale_factor\": 0.00392156862745098,\\n  \"size\": {\\n    \"shortest_edge\": 224\\n  }\\n}'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Let’s verify that our lion cub photo can survive a center crop\\n(Figure 3-28).\\nwidth, height = image.size\\ncrop_length = min(image.size)\\nleft = (width - crop_length) / 2\\ntop = (height - crop_length) / 2\\nright = (width + crop_length) / 2\\nbottom = (height + crop_length) / 2\\ncropped = image.crop((left, top, right, \\nbottom))\\ncropped'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 3-28. Example photo, center-cropped. The subject is\\nintact.\\nThe subject in our photo is fully preserved. Depending on your\\ndata, you may need to crop the source images before passing\\nthem to the processor to ensure the subject is visible.\\nWe’ll now get the embedding vector from the pre-processed\\nimage. We use the vision model stored inside the clip\\ninstance to do that. This sub-component is sometimes called the\\nvision tower.\\nwith torch.inference_mode():\\n    output = \\nclip.vision_model(pixel_values.to(device))\\nimage_embeddings = output.pooler_output\\nimage_embeddings.shape\\ntorch.Size([1, 1024])\\nThe vision model returns a dictionary with the last hidden\\nstates and the pooler output. This vector with shape [1,\\n1024] represents the result of the encoding process. Let’s now\\nturn our attention to the language portion of the model. We’ll\\nfollow a similar process to get the embeddings for two text'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='prompts: \"a photo of a lion\" and \"a photo of a\\nzebra“. Our end goal is to compare the cosine similarity\\nbetween the image embeddings and each prompt embedding.\\nHopefully, the lion description should match the image better!\\nprompts = [\\n    \"a photo of a lion\",\\n    \"a photo of a zebra\",\\n]\\n# Padding makes sure all inputs have the same \\nlength\\ntext_inputs = processor(text=prompts, \\nreturn_tensors=\"pt\", padding=True)\\n{\\'attention_mask\\': tensor([[1, 1, 1, 1, 1, 1, \\n1],\\n        [1, 1, 1, 1, 1, 1, 1]]),\\n \\'input_ids\\': tensor([[49406,   320,  1125,   \\n539,   320,  5567, 49407],\\n        [49406,   320,  1125,   539,   320, \\n22548, 49407]])}\\nText processing tokenizes the input strings just as in the\\nprevous chapter. We can now pass the tokenized text inputs'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='through the language tower part of the model to get the prompt\\nembeddings.\\ntext_inputs = {k: v.to(device) for k, v in \\ntext_inputs.items()}\\nwith torch.inference_mode():\\n    text_output = \\nclip.text_model(**text_inputs)\\ntext_embeddings = text_output.pooler_output\\ntext_embeddings.shape\\ntorch.Size([2, 768])\\nWe got two vectors in the output using a batch of two input\\nprompts. However, each vector has 768 dimensions, while the\\nembeddings from the image had 1024 dimensions. Remember\\nthat to compute the dot product of two vectors, they have to\\nhave the same number of dimensions, and we insisted in the\\nintroduction to this section that both the text encoder and the\\nimage encoder must produce embeddings with the same\\ndimensionality. There’s an additional step we didn’t mention\\nbefore. Instead of selecting encoder models that produce the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='same dimensions, we can take arbitrary text and image\\nencoders and compute a projection to vectors with the same\\ndimensionality. These projections were learned during the CLIP\\ntraining process and are also part of the clip model wrapper\\nwe downloaded before.\\nIn this case, the learned projections are just linear layers that\\nmap their inputs to vectors with 768 dimensions. There’s a\\nprojection for the text encoder and a diﬀerent one for the vision\\npart of the model.\\nprint(clip.text_projection)\\nprint(clip.visual_projection)\\nLinear(in_features=768, out_features=768, \\nbias=False)\\nLinear(in_features=1024, out_features=768, \\nbias=False)\\nwith torch.inference_mode():\\n    text_embeddings = \\nclip.text_projection(text_embeddings)\\n    image_embeddings ='),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='clip.visual_projection(image_embeddings)\\ntext_embeddings.shape, image_embeddings.shape\\n(torch.Size([2, 768]), torch.Size([1, 768]))\\nWe are almost ready to compute the cosine similarities. We just\\nneed to remember to use normalized vectors with unit norms,\\nwhich we achieve by scaling our embeddings and dividing by\\ntheir respective norms. We can then compute the two dot\\nproducts at once using matrix multiplication.\\ntext_embeddings = text_embeddings / \\ntext_embeddings.norm(\\n    p=2, dim=-1, keepdim=True\\n)\\nimage_embeddings = image_embeddings / \\nimage_embeddings.norm(\\n    p=2, dim=-1, keepdim=True\\n)\\nsimilarities = torch.matmul(text_embeddings, \\nimage_embeddings.T)\\nsimilarities'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='tensor([[0.2171],\\n        [0.1888]], device=\\'cuda:0\\')\\nDuring training, these cosine similarities were interpreted as\\nthe logits to be fed to a cross-entropy loss classiﬁer that predicts\\nthe label for each image-text pair. Since CLIP training used 32K-\\nsized batches, labels are each one of the 32768 positions in the\\nbatch. If you refer again to the CLIP image at the beginning of\\nthe section, once the model has been trained, the cross-entropy\\nprocess will select \"class\" T  for I , class T  for I , and so on.\\nThere’s a ﬁnal detail, though. Because the vectors are\\nnormalized, logits can only lie within the range [-1, 1],\\nwhich, due to ﬂoating point format limitations, may not have\\nenough dynamic range to expressively capture the categorical\\nprobability distributions of the 32K items. The authors used a\\nlearnable temperature parameter to scale the logits to have a\\nwider range. However, they also clipped this scale value to a\\nmaximum of 100 for numerical stability. In all the training\\nruns, they found that the scale always reached the maximum\\nvalue of 100. Therefore, CLIP inference uses a scale factor of 100\\nbefore interpreting the logits as probabilities.\\nLet’s apply that correction to our previous code snippet, and\\nthen we can convert the scaled similarity logits to probabilities.\\n1 1 2 2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='These probabilities represent how conﬁdent the model is that\\nthe image corresponds to one of the two text captions we used.\\nsimilarities = 100 * \\ntorch.matmul(text_embeddings, \\nimage_embeddings.T)\\nsimilarities.softmax(dim=0).cpu()\\ntensor([[0.9441],\\n        [0.0559]])\\nThe model matches the prompt \"a photo of a lion\" with\\nthe image, with a conﬁdence of 94.4%.\\nZero-shot Image Classiﬁcation with CLIP\\nThe process we followed in the previous section is a detailed\\nwalkthrough of the steps needed to implement a zero-shot\\nclassiﬁcation task with CLIP. Fortunately, software libraries\\nsuch as Transformers or OpenCLIP provide higher layers of\\nabstraction that make the process much easier.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='WHY IS THIS CALLED ZERO-SHOT CLASSIFICATION?\\nClassiﬁcation is one of the quintessential Machine Learning problems: given a data\\npoint and a set of predeﬁned classes, estimate the probability that the point\\ncorresponds to one of the classes. According to this deﬁnition, the set of classes must\\nbe ﬁxed in advance, so the model we train will only know about these classes and\\nnothing else. The ImageNet dataset, for example, contains images from 20,000\\ndiﬀerent classes. For years, the ImageNet Large Scale Visual Recognition Challenge\\n(ILSVRC) was a test benchmark for Computer Vision systems, with the goal of\\nclassifying objects belonging to just 1,000 of the ImageNet classes. In 2012, a deep\\nCNN known as AlexNet easily won that year’s challenge, and this started a revolution\\nwhere accuracy ﬁgures increased year after year as newer and better deep learning\\nmodels were designed.\\nZero-shot classiﬁcation refers to the capability of a model to correctly classify data\\nwithout having been trained explicitly for the classes we are asking for. In the\\nprevious chapter, we saw an example of zero-shot sentiment classiﬁcation using\\nlanguage models, and the previous section is another prime demonstration of this\\ncapability. CLIP was trained to match image-caption pairs, but we can leverage this\\nbehavior for classiﬁcation if we construct captions that could reasonably describe\\nthe images we want to classify. For example, if we want to classify cats vs.\\xa0dogs, our\\nprompts can be “A photo of a cat” and “A photo of a dog.” CLIP will match the best\\nprompt for the image we supply, giving us the classiﬁcation result we’re after.\\nLet’s replicate the example in the previous section with a\\nhigher-level transformers API. We load the CLIP model,\\nprocessor, and test image the same way we did before.\\nclip = \\nCLIPModel.from_pretrained(\"openai/clip-vit-'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='large-patch14\").to(device)\\nprocessor = \\nCLIPProcessor.from_pretrained(\"openai/clip-\\nvit-large-patch14\")\\nurl = \\n\"https://cdn.pixabay.com/photo/2014/12/12/19/45/l\\n565820_1280.jpg\"\\nimage = Image.open(requests.get(url, \\nstream=True).raw)\\nWe can leverage the processor to compute the inputs for both\\nthe image and the text prompts simultaneously. We can also\\nconveniently invoke the clip model with the full set of inputs to\\nretrieve the logits—or scaled cosine similarities—between the\\nimage and each prompt. Let’s use a few more prompts to make\\nthings more fun.\\nprompts = [\\n    \"a photo of a lion\",\\n    \"a photo of a zebra\",\\n    \"a photo of a cat\",\\n    \"a photo of an adorable lion cub\",\\n    \"a puppy\",\\n    \"a lion behind a branch\",\\n]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='inputs = processor(\\n    text=prompts, images=image, \\nreturn_tensors=\"pt\", padding=True\\n)\\ninputs = {k: v.to(device) for k, v in \\ninputs.items()}\\noutputs = clip(**inputs)\\nlogits_per_image = outputs.logits_per_image\\nprobabilities = \\nlogits_per_image.softmax(dim=1)\\nprobabilities = \\nprobabilities[0].cpu().detach().tolist()\\nfor prob, prompt in sorted(zip(probabilities, \\nprompts), reverse=True):\\n    print(f\"{100*prob: =2.0f}%: {prompt}\")\\n89%: a photo of an adorable lion cub\\n 9%: a lion behind a branch\\n 2%: a photo of a lion\\n 0%: a photo of a zebra\\n 0%: a photo of a cat\\n 0%: a puppy'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Similarly, we can supply multiple images and prompts in the\\nsame input batch and get all classiﬁcation probabilities\\nsimultaneously. Feel free to explore and adapt to your use case.\\nZero-shot Image Classiﬁcation Pipeline\\nNow that we know how CLIP works and how to use it for zero-\\nshot image classiﬁcation, we can use an even higher-level API\\nfor simplicity and convenience. We’ll use the pipeline\\nabstraction, which we already presented in Chapter 1. In that\\ncase we demonstrated how to use it for the text classiﬁcation\\ntask, but we can also apply it for many other tasks, including\\nzero-shot image classiﬁcation.\\nTo instantiate a pipeline, we simply give it the task name\\n(zero-shot-image-classification, in this case), and the\\nmodel we want to use.\\nfrom transformers import pipeline\\nclassifier = pipeline(\\n    \"zero-shot-image-classification\",\\n    model=\"openai/clip-vit-large-patch14\",\\n    device=device,\\n)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The pipeline takes care of all the details for us: tokenization,\\nimage pre-processing, logits post-processing. We just need to\\ninvoke the pipeline instance with the image we want to classify,\\nand a set of candidate labels. The pipeline returns a dictionary,\\nconveniently sorted by score, containing all the scores\\nassociated to the labels we provided.\\nscores = classifier(\\n    image,\\n    candidate_labels=prompts,\\n    hypothesis_template=\"{}\",\\n)\\nThe +hypothesis_template+ is a Python format string that is\\napplied to each candidate label to build the text prompt for\\nclassiﬁcation. If we omit it, the pipeline will automatically use\\n+\"This is a photo of a {}\"+, which is appropriate to\\nformat class labels indicated by their name, such as \"cat\" or\\n\"lion“. Since we already built prompts that work well with\\nCLIP, we use +\"{}\"+ to use our labels untouched.\\n[{\\'label\\': \\'a photo of an adorable lion cub\\',\\n  \\'score\\': 0.886413037776947},\\n {\\'label\\': \\'a lion behind a branch\\', \\'score\\': \\n0.09321863204240799},'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"{'label': 'a photo of a lion', 'score': \\n0.018809959292411804},\\n {'label': 'a photo of a zebra', 'score': \\n0.0011134858941659331},\\n {'label': 'a photo of a cat', 'score': \\n0.0004198708338662982},\\n {'label': 'a puppy', 'score': \\n2.4912407752708532e-05}]\\nCLIP Use Cases\\nThe original use case that CLIP was designed to solve is zero-\\nshot image classiﬁcation. The results are impressive: it achieves\\nsimilar performance as models trained for ImageNet\\nclassiﬁcation, without ever using the ImageNet labels during\\ntraining. As a consequence, performance remains equally\\nstrong on many other datasets, with no need for ﬁne-tuning on\\nthem, as you can read in the original blog post.\\nThe previous sections have shown that the basis for solving\\nzero-shot image classiﬁcation is the ability to compute the\\nsimilarity between an arbitrary image and a text prompt, and\\nthis is possible because both the image and the text embeddings\\ncapture the essential semantics of the data. The way CLIP works\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='has enabled the community to use it for many tasks, not just\\nzero-shot classiﬁcation.\\nBeing able to compute the similarity between text and images\\nenables applications such as semantic search, which makes it\\npossible to search for photos based on natural-language\\ndescriptions of their contents or ﬁnd images similar to an\\nexample image we provide. Applications of these techniques\\nexist in multiple domains, including consumer hardware (such\\nas phones), medical systems, fashion, and others. In the\\nprevious chapter, we proposed a similar challenge, but is\\nconstrained to textual data: building an FAQ system by\\ncomputing similarities between the embedding outputs of a\\nlanguage model. At the end of this chapter we propose a\\nchallenge using CLIP for semantic search.\\nCLIP can also be used to obtain rich embeddings for\\ndownstream tasks. For example, some text-to-image models use\\nCLIP to obtain semantically rich representations of the prompts\\nsupplied by the user.\\nCLIP was also adopted as an essential tool for generative use\\ncases. The CLIP Guidance method, developed by Ryan Murdock,\\nKatherine Crowson, and others, uses CLIP as a loss to guide\\nmodel gradients towards the desired representation (expressed'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='with a prompt). It spawned a creative explosion in the\\ngenerative art community. Later, CLIP conditioning became\\nessential in models such as Stable Diﬀusion, which we’ll explore\\nin Chapter 5.\\nCLIP scoring capabilities have also been used to ﬁlter and score\\nmassive datasets of image-caption pairs, such as LAION,\\ncrawled from Internet sources. Relying on CLIP allows dataset\\ncreators to select pairs where the similarity between the image\\nand the caption yields a high score and discard the others. This\\nhas made it possible to build and reﬁne datasets in the order of\\nbillions of items, which can be used to build better models that\\ncan reﬁne datasets even more precisely. So meta!\\nAlternatives to CLIP\\nBecause CLIP is extensively used in industry and research,\\nthere has been much activity around the ideas presented in the\\nmodel and how to make them better, faster, or adapted to other\\ntasks. A signiﬁcant eﬀort has been made to make CLIP fully\\nopen-source and reproducible. The OpenCLIP repository\\nprovides open-source code to implement CLIP and a number of\\narchitecture variants. Using the OpenCLIP codebase and the\\nhuge LAION dataset we mentioned in the previous section, the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='LAION team trained very powerful CLIP models of various sizes\\nand made all the checkpoints available for everyone to use.\\nRecent research has shown that better text understanding can\\nlead to better text-image models. BLIP , CoCa  and CapPa\\ndemonstrate that the captioning task (generate a detailed\\ndescription of what an image represents) can produce models\\nthat are capable of generating excellent image representations\\nand solve a wide range of vision-language tasks. In parallel\\nresearch, the use of a diﬀerent loss function (sigmoid loss used\\nin SigLIP , instead of softmax normalization) can make\\ntraining easier and alleviate CLIP’s problem of requiring huge\\nbatch sizes to train eﬀectively.\\nAnother promising direction is building smaller, faster models\\nthat can run on personal computers and mobile devices. This is\\nthe case with Apple’s MobileCLIP , which achieves the\\nperformance of OpenAI’s CLIP models with much smaller (and\\nfaster) models. Another Apple eﬀort, Data Filtering Networks ,\\naims to improve the quality of text-image datasets and also\\ntrained several CLIP variants with those methods.\\nAs we’ll see throughout the rest of this book, CLIP is an essential\\ncomponent of image generation systems. This healthy research\\n1 2 1 3 1 4 \\n1 5 \\n1 6 \\n1 7'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='on more robust, capable, and faster CLIP-like models makes us\\nvery optimistic about the future\\nProject Time: Semantic Image Search\\nA fun project is building a semantic search engine for your\\nphotos. When you are done, you should be able to look for\\nphotos in your library by simply describing their contents (for\\nexample: \"dog jumping into the water on a hot summer\\nday\" or \"woman with umbrella walking down a busy\\nstreet“), instead of trying to remember where in your\\ncollection those photos are stored. You can also use any other\\nimage dataset you like, but using content that means something\\nto you will be rewarding and allow you to evaluate how well\\nthe system works. It may also make you want to think of ideas\\nfor improvement or share them with your family.\\nThese are some suggested steps to tackle the project:\\nChoose a text-image model, such as CLIP, that can produce\\nembeddings for both images and text descriptions. You can\\nexplore other alternatives, but CLIP should be a good start.\\nChoose a family of models with multiple-size variants that\\nyou can easily replace with one another. This way, you can'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='use a small model to iterate faster while working and see\\nhow much performance increases when you use a larger\\nmodel.\\nFind a good number of photos and copy them all to a folder\\non your computer. Several hundred or thousand photos\\nshould be ﬁne.\\nWrite a loop to create embeddings from your photos using\\nthe model you chose.\\nRead the photos from disk, crop and/or resize them so\\nthey are the same size, and create a batch. You can use\\na PyTorch DataLoader, as we did in the training loops\\nin this chapter. For pre-processing, you may do it\\nmanually with torchvision.transforms, or you can\\nleverage the model’s built-in pre-processor if it exists.\\nChoose a batch size that ﬁts your hardware.\\nRun each batch of images through the image portion of\\nthe CLIP model. Use inference mode (no gradients need\\nto be computed, as you won’t be training anything).\\nGet the embeddings from the output and save them to\\ndisk. You’ll get a multidimensional vector for each\\nimage ﬁle. You can convert them to numpy arrays and\\nwrite them all into the same ﬁle. Don’t forget to store\\nthe names or paths to the original photos, you’ll need\\nthem to retrieve the photos later.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='At this point, you have an array of vectors in numpy format.\\nYou can now use the text portion of the model to run\\nqueries:\\nWrite a function that receives an input prompt and\\ngenerates an embedding vector using the text tower of\\nthe model.\\nCompute the cosine similarity between that vector and\\nall the vectors in the image embedding table. If you have\\nenough RAM, you can do it by simply using PyTorch’s\\nmatmul operation.\\nSort the outputs and select the top ones.\\nFind the images associated with the top scores and\\nvisualize them. Do they match the prompt you used?\\nBonus tasks:\\nCan you try to ﬁnd images that look similar to another\\nimage? (i.e., semantic search based on an input image, not a\\ntext description).\\nIf you have a lot of photos, you may have trouble\\ncomputing the scores. How could you solve this problem?\\nAre there any frameworks or services that help with this?\\nHow many photos does it take to reach the limits of your\\ncomputer?'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Pre-trained models don’t know anything about the subjects\\nthat appear in your photos. What could you do to be able to\\nsearch by personal names or places?\\nIf you are adventurous, you can use MobileCLIP to run the\\nsearch engine on your phone. This is a big challenge in\\nitself; don’t underestimate the eﬀort!\\nSummary\\nThis chapter showed how learning compressed representations\\nfrom input data is a way to capture the dataset’s essential\\ncharacteristics and how those representations can be eﬀectively\\nused for many additional downstream tasks. We started this\\nexploration by looking at a classical system, the AutoEncoder,\\nwhose goal is to encode input samples into a latent space of\\nreduced dimensionality and then recover the original data\\npoints from the latent representations. By splitting the\\nAutoEncoder into two components –the encoder and the\\ndecoder–we can imagine new applications beyond\\nreconstruction. The encoder, for example, can be used as a\\nfeature extractor. Because it learned the essential features of\\nthe input dataset, we can use it to train other systems, such as\\nclassiﬁers, whose input data are latent representations. We also\\nexplored the idea of using the decoder for generative purposes.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='If the latent space is a representation of the original dataset, can\\nwe move to arbitrary points there and see what outputs we get?\\nAutoEncoders, however, have some limitations for this task\\nbecause of how they are trained.\\nVAEs are a special kind of AutoEncoder that tries to achieve\\n\"better behaved\" representations in latent space. By trying\\nto make latent features match a probability distribution, we can\\nsample from the desired distribution to obtain new random\\nlatent features. If we feed those features to the decoder, we can\\ngenerate data points that look like they came from the original\\ndataset. This is a crucial result for generative applications.\\nExploring this further, the fact that the latent space is a compact\\nrepresentation of the data is an essential idea behind\\ngenerative systems such as Stable Diﬀusion. As we’ll see in\\nmore detail in Chapter 5, we can conduct computation in the\\nlatent space that represents images rather than on the raw\\nimage data. This enabled the training of high-quality image\\ngeneration systems that can run fast and eﬃciently on\\nconsumer hardware.\\nThe ﬁnal section of the chapter focuses on CLIP, a highly\\ninﬂuential model developed and published by OpenAI that\\nencodes image and text data into the same latent space. New\\ntricks are possible with CLIP that were pretty hard problems to'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='solve. For example, given an image and a few sentences, we can\\nmeasure which sentence matches the image better. Conversely,\\ngiven a caption and a few image candidates, we can select the\\nimage that best matches the caption. CLIP was published as part\\nof OpenAI’s Dall·e image generation project (which was not\\npublished itself), and it spawned a revolution in generative\\nresearch. CLIP-like models are key components of Stable\\nDiﬀusion and other text-to-image models, but they are used for\\nmany other applications: natural-language image retrieval,\\nsemantic search, extracting images that are similar based on\\ncontent or style, and a lot more.\\nAll of this came about from the initial realization that learning\\nhow to compress data is equivalent to learning about the data.\\nMany variations on these concepts use diﬀerent types of data\\nextraction and representation techniques: CNNs, transformers,\\nor combining the best of both with systems like VQGAN. We just\\noﬀered a glimpse of the motivations and ideas behind these\\nfoundational blocks with the hope that they will be useful for\\nnavigating this rich and fascinating space.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Exercises\\nMost of these exercises are the same ones we proposed during\\nthe chapter, compiled here for your convenience. Depending on\\nyour learning style, you may like to work on them as you go\\nthrough the chapter, or you may try them all after a ﬁrst read.\\n1. How does generation work if the AutoEncoder model is\\ntrained with 16 latent dimensions? Can you compare\\ngenerations between the model with 16 latent dimensions\\nand the one with just 2?\\n2. Train the model again with the same parameters we used\\n(just run the code shown in the chapter) but with diﬀerent\\nrandom number initialization, and visualize the latent\\nspace. Chances are that the shapes and structure are\\ndiﬀerent. Is this something you would expect? Why?\\n3. How good are the image features extracted by the encoder?\\nExplore it by training a number classiﬁer on top of the\\nencoder.\\n4. When we trained the VAE, we added the reconstruction and\\nKL-divergence losses. However, both have diﬀerent scales.\\nWhat will happen if we give more importance to one vs.\\xa0the\\nother? Can you run a few experiments and explain the\\nresults?'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='5. The VAE we trained only uses two dimensions to represent\\nthe mean and the logvar of the distribution. Can you repeat\\na similar exploration using 16 dimensions?\\n6. Humans are trained to look at faces and easily identify\\nunrealistic features. Can you try to train an AutoEncoder\\nand a VAE for a dataset containing faces, and see what the\\nresults look like? You can start with the Frey Face dataset\\nthat was used in the VAE paper – it’s an homogenous set of\\nmonochrome faces from the same person sporting diﬀerent\\nfacial expressions. If you want to be more ambitious, you\\ncan try your hand at the CelebFaces dataset, also hosted on\\nthe Hugging Face Hub. Another interesting example could\\nbe to try the Oxford pets dataset, also available on the Hub.\\nChallenge\\n7. BLIP-2 for search. The hands-on project on semantic image\\nsearch is quite challenging, but here’s another idea. Can\\nyou use the BLIP-2 model for similarity tasks, just like we\\ndid with CLIP in this chapter? How would you go about it,\\nand how does it compare with CLIP? What other tasks can\\nyou solve with BLIP-2?'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='You can ﬁnd the solutions to these exercises in the\\nsupplementary material.\\nReferences\\n1. Chiaki Yanagisawa. Conv2d and ConvTransposed2d.\\nhttps://indico.cern.ch/event/996880/contributions/4188468/att\\nachments/2193001/3706891/ChiakiYanagisawa_20210219_Co\\nnv2d_and_ConvTransposed2d.pdf\\n2. Clément Chadebec. pythae, a uniﬁed implementation of\\nmultiple Autoencoder models, that allows easy\\nexperimentation, comparison and benchmarking.\\n3. Stanford University. CS231n Convolutional Neural Networks\\nfor Visual Recognition. Explains convolutions at\\nhttps://cs231n.github.io/convolutional-networks/#conv.\\nDavid Foster. Generative Deep Learning (2nd Edition).\\nPublished by O’Reilly Media, Inc.\\xa0Chapter 3. Variational\\nAutoencoders.\\n4. Esser, Patrick, Robin Rombach, and Bjorn Ommer. Taming\\ntransformers for high-resolution image synthesis. In\\nProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pp.\\xa012873-12883. 2021.\\narXiv preprint'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='5. Fang, Alex, et al.\\xa0Data Filtering Networks. arXiv, 29 Sept.,\\n2023. arXiv.org, https://arxiv.org/abs/2309.17425\\n6. François Chollet. Variational AutoEncoder. Keras\\nimplementation of a VAE.\\nhttps://keras.io/examples/generative/vae/\\n7. François Floret, _Deep Learning Course _(2024 version).\\nAvailable at https://ﬂeuret.org/dlc/, revised VAE handouts\\n(PDF) at https://ﬂeuret.org/dlc/materials/dlc-handout-7-4-\\nVAE.pdf.\\n8. Howard, Jeremy, and Gugger, Sylvain. Deep Learning for\\nCoders with fastai & PyTorch. O’Reilly 2020.\\n9. Johannes Maucher. Animations of Convolution and\\nDeconvolution.\\nhttps://hannibunny.github.io/mlbook/neuralnetworks/convolu\\ntionDemos.html.\\n10. Jonathan Whitaker. A Deep Dive Into OpenCLIP from\\nOpenAI. https://wandb.ai/johnowhitaker/openclip-\\nbenchmarking/reports/Exploring-OpenCLIP–\\nVmlldzoyOTIzNzIz\\n11. Kingma, Diederik P., and Max Welling. Auto-Encoding\\nVariational Bayes. arXiv, 20 Dec., 2023. arXiv.org,\\nhttps://arxiv.org/abs/1312.6114\\n12. LAION (various authors). Large Scale OpenCLIP trained on\\nLAION-2B.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='13. Li, Junnan, et al.\\xa0BLIP: Bootstrapping Language-Image Pre-\\ntraining for Uniﬁed Vision-Language Understanding and\\nGeneration. In International conference on machine\\nlearning, pp.\\xa012888-12900. PMLR, 2022. arXiv preprint.\\n14. Li, Junnan, et al.\\xa0BLIP-2: Bootstrapping Language-Image Pre-\\ntraining with Frozen Image Encoders and Large Language\\nModels. In International conference on machine learning,\\npp.\\xa019730-19742. PMLR, 2023. arXiv preprint.\\n15. ML Foundations (various authors). OpenCLIP.\\n16. pytorch-mnist-vae, a VAE PyTorch implementation that\\nfollows the paper.\\n17. PyTorch team. PyTorch VAE example.\\nhttps://github.com/pytorch/examples/tree/main/vae\\n18. Radford, Alec, et al.\\xa0Learning Transferable Visual Models\\nFrom Natural Language Supervision. In International\\nconference on machine learning, pp.\\xa08748-8763. PMLR,\\n2021. Predicting which caption goes with which image is\\nenough to learn powerful image representations. arXiv\\npreprint\\n19. Stefano Ermon, et al.\\xa0CS 228 - Probabilistic Graphical\\nModels. The variational auto-encoder.\\n20. Tschannen, Michael, et al.\\xa0Image captioners are scalable\\nvision learners too. Advances in Neural Information\\nProcessing Systems 36 (2024). arXiv preprint.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='21. Vasu, Pavan Kumar Anasosalu, et al.\\xa0MobileCLIP: Fast\\nImage-Text Models through Multi-Modal Reinforced\\nTraining, CVPR 2024, arXiv preprint arXiv:2311.17049. Code\\nrepository: https://github.com/apple/ml-mobileclip\\n22. Yu, Jiahui,et al.\\xa0Coca: Contrastive captioners are image-text\\nfoundation models. arXiv preprint arXiv:2205.01917 (2022).\\n23. Zhai, Xiaohua, et al.\\xa0Sigmoid Loss for Language Image Pre-\\nTraining. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision, pp.\\xa011975-11986. 2023.\\narXiv preprint.\\n torchvision transforms, as we’ll see later in the book, is a collection of common\\nimage transformation, conversion, augmentation, and manipulation routines.\\n For an in-depth discussion of these and many other design choices, as well as an\\nexcellent and practical overview of deep learning, we recommend Deep Learning for\\nCoders with fastai and PyTorch, by Jeremy Howard and Sylvain Gugger, available at\\nhttps://www.oreilly.com/library/view/deep-learning-for/9781492045519\\n In eval mode, BatchNorm2d applies the mean and standard deviation learned from\\nall the mini-batches during training. Since we haven’t trained the model yet, this will\\nbe random data, but we are only interested in seeing if the model deﬁnition works.\\nDuring actual training, we use a batch size larger than 1, and BatchNorm2d will\\nwork ﬁne.\\n A transposed convolution works just like a convolution, but instead of applying it to\\nthe 2D input data, it is applied to an enlarged version of it (the 2D input is ﬁlled with\\nzeros between the rows and the columns). This results in an output 2D matrix that is\\n1 \\n2 \\n3 \\n4'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='larger than the input after the ﬁlter is applied. This blog post shows excellent\\nvisualizations of how convolutions and transposed convolutions work.\\n You can use torch.manual_seed(num) to specify a seed\\n A Gaussian distribution, also called normal, has a bell-shaped curve with most\\nvalues clustered around the mean and fewer values at the extremes\\n There’s a family of VAEs, called conditional VAEs (or C-VAEs), that uses class\\ninformation to further separate distributions in\\nlatent\\xa0space\\xa0while\\xa0still\\xa0keeping\\xa0Gaussian\\xa0representation of the features.\\xa0This\\xa0makes it\\neasier to generate samples resembling the speciﬁc class we’re interested in.\\n Actually, not really the variance, as we’ll see shortly.\\n Remember that the VAE encoder returns not only the reconstructed images but also\\nthe mean and the log variance of the distributions. This is the reason why.\\n This is a rather crude way to show the learned manifold of the model. For a better\\nway to display this result and additional experiments, we recommend this GitHub\\nrepo by Jackie Loong: https://github.com/dragen1860/pytorch-mnist-vae\\n Radford, Alec, et al.\\xa0Learning Transferable Visual Models From Natural Language\\nSupervision. In International Conference on Machine Learning, pp.\\xa08748-8763. PMLR,\\n2021.\\n Li, Junnan, et al.\\xa0BLIP: Bootstrapping Language-Image Pre-training for Uniﬁed Vision-\\nLanguage Understanding and Generation. In International Conference on Machine\\nLearning,, pp.\\xa012888-12900. PMLR, 2022. arXiv preprint.\\n Yu, Jiahui,et al.\\xa0Coca: Contrastive captioners are image-text foundation models. arXiv\\npreprint arXiv:2205.01917 (2022)\\n5 \\n6 \\n7 \\n8 \\n9 \\n 0 \\n 1 \\n 2 \\n 3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Tschannen, Michael, et al.\\xa0Image captioners are scalable vision learners too.\\nAdvances in Neural Information Processing Systems 36 (2024). arXiv preprint\\n Zhai, Xiaohua, et al.\\xa0Sigmoid Loss for Language Image Pre-Training. In Proceedings\\nof the IEEE/CVF International Conference on Computer Vision, pp.\\xa011975-11986. 2023.\\narXiv preprint\\n Vasu, Pavan Kumar Anasosalu, et al.\\xa0MobileCLIP: Fast Image-Text Models through\\nMulti-Modal Reinforced Training, CVPR 2024, arXiv preprint arXiv:2311.17049. Code\\nrepository: https://github.com/apple/ml-mobileclip\\n Fang, Alex, et al.\\xa0Data Filtering Networks. arXiv, 29 Sept., 2023. arXiv.org,\\nhttps://arxiv.org/abs/2309.17425\\n 4 \\n 5 \\n 6 \\n 7'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Chapter 4. Diﬀusion Models\\nA NOTE FOR EARLY RELEASE READERS\\nWith Early Release ebooks, you get books in their earliest form\\n—the authors’ raw and unedited content as they write—so you\\ncan take advantage of these technologies long before the oﬃcial\\nrelease of these titles.\\nThis will be the fourth chapter of the ﬁnal book. Please note\\nthat the GitHub repo will be made active later on.\\nIf you have comments about how we might improve the content\\nand/or examples in this book, or if you notice missing material\\nwithin this chapter, please reach out to the editor at\\njleonard@oreilly.com.\\nThe ﬁeld of image generation became widely popular with Ian\\nGoodfellow’s introduction of Generative Adversarial Nets\\n(GANs) in 2014. The key ideas of GANs led to a big family of\\nmodels that could quickly generate high-quality images.\\nHowever, despite their success, GANs posed challenges,\\nrequiring many parameters and help to generalize eﬀectively.\\nThese limitations sparked parallel research endeavors, leading'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='to the exploration of diﬀusion models—a class of models that\\nwould redeﬁne the landscape of high-quality, ﬂexible image\\ngeneration.\\nIn late 2020, a little-known class of models called diﬀusion\\nmodels began causing a stir in the Machine Learning world.\\nResearchers ﬁgured out how to use these diﬀusion models to\\ngenerate higher-quality images than those produced by GANs. A\\nﬂurry of papers followed, proposing improvements and\\nmodiﬁcations that pushed the quality up even further. By late\\n2021, models like GLIDE showcased incredible results on text-\\nto-image tasks. Just a few months later, these models had\\nentered the mainstream with tools like DALL-E 2 and Stable\\nDiﬀusion. These models made it easy for anyone to generate\\nimages just by typing in a text description of what they wanted\\nto see.\\nIn this chapter, we will dig into how these models work. We’ll\\noutline the key insights that make them so powerful, generate\\nimages with existing models to get a feel for how they work,\\nand then train our own to deepen this understanding further.\\nThe ﬁeld is still rapidly evolving, but the topics covered here\\nshould give you a solid foundation to build on, which will be\\nextended further in Chapters 5, 7, and 8.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The high-level idea of diﬀusion models is that they receive\\nimages blurred with noise and learn to denoise them,\\noutputting a clear image. When diﬀusion models are trained,\\nthe dataset contains images with diﬀerent amounts of noise\\n(even when the input is pure noise). In inference, we can begin\\nwith pure noise, and the model will generate an image that\\nmatches the training distribution. The model does multiple\\niterations to accomplish this, correcting itself and leading to\\nimpressively high-quality generations.\\nThe Key Insight: Iterative Reﬁnement\\nFigure 4-1. Figure 4-1. Progressive denoising process.\\nSo, what is it that makes diﬀusion models so powerful?\\nPrevious techniques, such as VAEs or GANs, generate their ﬁnal\\noutput via a single forward pass of the model. This means the\\nmodel must get everything right on the ﬁrst try. If it makes a\\nmistake, it can’t go back and ﬁx it. Diﬀusion models, on the\\nother hand, generate their output by iterating over many steps .\\nThis iterative reﬁnement allows the model to correct mistakes\\n1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='made in previous steps and gradually improve the output, as\\nshown in Figure 4-1. To illustrate this, let’s look at an example of\\na diﬀusion model in action.\\nWe can load a pre-trained diﬀusion model using the Hugging\\nFace diﬀusers library. The library provides a high-level pipeline\\nthat can be used to create images directly. We’ll load the ddpm-\\ncelebahq-256 model, one of the ﬁrst shared diﬀusion models\\nfor image generation. This model was trained with the CelebA-\\nHQ dataset, a then popular dataset of high-quality images of\\ncelebrities, so it will generate images that look like they came\\nfrom that dataset. We’ll use this model to generate an image\\nfrom noise.\\nimport torch\\nfrom diffusers import DDPMPipeline\\nfrom genaibook.core import get_device\\n# We can set the device to use our GPU or CPU\\ndevice = get_device()\\n# Load the pipeline\\nimage_pipe = \\nDDPMPipeline.from_pretrained(\"google/ddpm-\\ncelebahq-256\")'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='image_pipe.to(device)\\n# Sample an image\\nimage_pipe().images[0]\\nLoading pipeline components...:   0%|          \\n| 0/2 [00:00<?, ?it/s]\\n  0%|          | 0/1000 [00:00<?, ?it/s]\\nFigure 4-2. Example image generated with the DDPM pipeline\\nafter 1000 iterations.\\nThe pipeline does not show us what is under the hood, so let’s\\ndive into its internals. If you run the code, you will notice that\\ngeneration took 1,000 steps. This diﬀusion pipeline has to go'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='through 1,000 reﬁnement steps (and forward passes) to get to\\nthe ﬁnal image shown in Figure 4-2. This is one of the major\\ndrawbacks of the vanilla diﬀusion models compared to the\\nGANs - they require many steps to generate high-quality images,\\nmaking the models slow at inference time.\\nWe can re-create this sampling process step by step to\\nunderstand better what is happening under the hood. At the\\nbeginning of the diﬀusion process, we initialize our sample x\\nwith a batch of four random images (in other words, we sample\\nsome random noise). We’ll run 30 steps to progressively denoise\\nthe input images and end up with a sample from the real\\ndistribution.\\nLet’s generate some images! On the left side of Figure 4-3, you\\ncan see the input at a given step (beginning with the random\\nnoise). You can see the model’s prediction for the ﬁnal images\\non the right. The results of the ﬁrst row are not particularly\\ngood. Instead of jumping right to that ﬁnal predicted image in a\\ngiven diﬀusion step, we only modify the input x (shown on the\\nleft) by a small amount in the direction of the prediction. We\\nthen feed this new, slightly better x through the model again\\nfor the next step, hopefully resulting in a slightly improved\\nprediction, which can be used to update x a little more, and so'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"on. With enough steps, the model can produce some\\nimpressively realistic images.\\nfrom genaibook.core import \\nplot_noise_and_denoise\\n# The random starting point is a batch of 4 \\nimages\\n# Each image is 3-channel (RGB) 256x256 pixel \\nimage\\nimage = torch.randn(4, 3, 256, \\n256).to(device)\\n# Set the specific number of diffusion steps\\nimage_pipe.scheduler.set_timesteps(num_inference_\\n# Loop through the sampling timesteps\\nfor i, t in \\nenumerate(image_pipe.scheduler.timesteps):\\n    # Get the prediction given the current \\nsample x and the timestep t\\n    # As we're running inference, we don't \\nneed to calculate gradients,\\n    # so we can use torch.inference_mode().\\n    with torch.inference_mode():\\n        # We need to pass in the timestep t \\nso that the model knows what\\n        # timestep it's currently at. We'll\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='learn more about this in the\\n        # coming sections.\\n        noise_pred = image_pipe.unet(image, \\nt)[\"sample\"]\\n    # Calculate what the updated x should \\nlook like with the scheduler\\n    scheduler_output = \\nimage_pipe.scheduler.step(noise_pred, t, \\nimage)\\n    # Update x\\n    image = scheduler_output.prev_sample\\n    # Occasionally display both x and the \\npredicted denoised images\\n    if i % 10 == 0 or i == \\nlen(image_pipe.scheduler.timesteps) - 1:\\n        \\nplot_noise_and_denoise(scheduler_output, i)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 4-3. Denoising diﬀusion process. Predictions at step t\\nare fed back into the model with some noise added.\\nDon’t worry if that chunk of code looks intimidating - we’ll\\nexplain how this all works throughout this chapter. Focus on the\\nidea for now.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='This core idea of learning how to iteratively reﬁne a noisy input\\ncan be applied to a wide range of tasks. This chapter will focus\\non unconditional image generation, generating images that\\nresemble the training data distribution. For example, we can\\ntrain an unconditional image generation model with a dataset\\nof butterﬂies so it can also generate new, high-quality images.\\nThis model would not be able to create images diﬀerent than\\nthe distribution of its training dataset, so don’t expect it to\\ngenerate dinosaurs. In Chapter 5, we’ll do a deep dive into\\ndiﬀusion models conditioned on text, but we can do many other\\nthings. Diﬀusion models have been applied to audio, video, text,\\n3D objects, protein structures, and other domains. While most\\nimplementations use some variant of the denoising approach\\nwe’ll cover here, emerging approaches that apply diﬀerent\\ntypes of “corruption” (always combined with iterative\\nreﬁnement) may move the ﬁeld beyond the current focus on\\ndenoising diﬀusion.\\nTraining a Diﬀusion Model\\nIn this section, we’re going to train a diﬀusion model from\\nscratch to gain a better understanding of how they work. We’ll\\nstart by using components from the diﬀusers library. As the\\nchapter progresses, we’ll gradually demystify how each'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='component works. Training a diﬀusion model is relatively\\nstraightforward compared to other generative models. To train\\na model, we repeatedly:\\n1. Load some images from the training data\\n2. Add noise in diﬀerent amounts. Remember, we want the\\nmodel to do a good job estimating how to \"fix\" (denoise)\\nboth extremely noisy images and images that are close to\\nperfect, so we want a dataset with diverse amounts of noise\\n3. Feed the noisy versions of the inputs into the model\\n4. Evaluate how well the model does at denoising these inputs\\n5. Use this information to update the model weights\\nTo generate new images with a trained model, we begin with a\\ncompletely random input and repeatedly feed it through the\\nmodel, updating the input on each iteration by a small amount\\nbased on the model prediction. As we’ll see, several sampling\\nmethods streamline this process to generate good images with\\nas few steps as possible.\\nThe Data\\nFor this example, we’ll use a dataset of images from the\\nHugging Face Hub- speciﬁcally, this collection of 1000 butterﬂy\\n2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='pictures . Later on, in the projects section, you will see how to\\nuse your own data.\\nfrom datasets import load_dataset\\ndataset = \\nload_dataset(\"huggan/smithsonian_butterflies_subs\\nsplit=\"train\")\\nWe must prepare the data before using it to train a model.\\nImages are typically represented as a grid of pixels. Unlike the\\nprevious chapter, where we used grayscale images, these\\nimages are in color. Each pixel is represented with color values\\nbetween 0 and 255 for each of the three color channels (Red,\\nGreen, and Blue). To process these and make them ready for\\ntraining, we:\\nResize them to a ﬁxed size. This is necessary because the\\nmodel expects all images to have the same dimensions.\\n(Optional) Add some augmentation by randomly ﬂipping\\nthem horizontally, making the model more robust and\\nallowing us to train with more data. Augmentation (Figure\\n4-4) is a common practice in Computer Vision tasks, as it\\nhelps the model generalize better to unseen data. Flipping\\n2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='is just one technique of augmentation with image data.\\nOther techniques are translating, scaling, and rotating.\\nConvert them to a PyTorch tensor (representing the color\\nvalues as ﬂoats between 0 and 1). Model inputs must\\nalways be formatted as multi-dimensional matrices, or\\ntensors.\\nNormalize them to have a mean of 0, with values between\\n-1 and 1. This is a common practice in training deep\\nlearning models, as it helps the model learn faster and\\nmore eﬀectively.\\nFigure 4-2. Figure 4-4. Augmentation creates more data from the training dataset,\\nimproving generalization.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='We can deﬁne these transformations using\\ntorchvision.transforms:\\nfrom torchvision import transforms\\nimage_size = 64\\n# Define data augmentations\\npreprocess = transforms.Compose(\\n    [\\n        transforms.Resize((image_size, \\nimage_size)),  # Resize\\n        transforms.RandomHorizontalFlip(),  # \\nRandomly flip (data augmentation)\\n        transforms.ToTensor(),  # Convert to \\ntensor (0, 1)\\n        transforms.Normalize([0.5], [0.5]),  \\n# Map to (-1, 1)\\n    ]\\n)\\nThe datasets library provides a convenient method,\\nset_transform(), which allows us to specify transformations\\nthat will be applied on the ﬂy as the data is used. Finally, we can\\nwrap the dataset with a DataLoader, a loading utility that\\n3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='makes it easy to iterate over batches of data, simplifying our\\ntraining code.\\ndef transform(examples):\\n    examples = [preprocess(image) for image \\nin examples[\"image\"]]\\n    return {\"images\": examples}\\ndataset.set_transform(transform)\\nbatch_size = 16\\ntrain_dataloader = \\ntorch.utils.data.DataLoader(\\n    dataset, batch_size=batch_size, \\nshuffle=True\\n)\\nWe can check that this worked by loading a batch and\\ninspecting the images. Figure 4-5 shows an example batch from\\nthe training set.\\nfrom genaibook.core import show_images\\nbatch = next(iter(train_dataloader))\\n# When we normalized, we mapped (0, 1) to \\n4'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='(-1, 1)\\n# Now we map back to (0, 1) for display\\nshow_images(batch[\"images\"][:8] * 0.5 + 0.5)\\nFigure 4-5. A few butterﬂies from the training set.\\nAdding Noise\\nHow do we gradually corrupt our data? The most common\\napproach is to add noise to the images. We will add diﬀerent\\namounts of noise to the training data, as the goal is to train a\\nrobust model to denoise no matter how much noise is in the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"input. The amount of noise we add is controlled by a noise\\nschedule, which is a critical aspect of diﬀusion models.\\nDiﬀerent papers and approaches tackle this in diﬀerent ways.\\nFor now, let’s explore one common approach in action based on\\nthe DDPM paper.  In diﬀusers, adding noise is handled by a class\\ncalled a Scheduler, which takes in a batch of images and a list\\nof timesteps and determines how to create the noisy versions of\\nthose images. We’ll explore the math behind this later in the\\nchapter, but for now, let’s see how it works in practice. The\\nfollowing code snippet applies increasingly larger amounts of\\nnoise to each one of the input images, with the result shown in\\nFigure 4-6.\\nfrom diffusers import DDPMScheduler\\n# We'll learn about beta_start and beta_end \\nin the next sections\\nscheduler = DDPMScheduler(\\n    num_train_timesteps=1000, \\nbeta_start=0.001, beta_end=0.02\\n)\\n# Create a tensor with 8 evenly spaced values\\n# from 0 to 999\\ntimesteps = torch.linspace(0, 999, 8).long()\\n5\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='# We load 8 images from the dataset and\\n# add increasing amounts of noise to them\\nx = batch[\"images\"][:8]\\nnoise = torch.rand_like(x)\\nnoised_x = scheduler.add_noise(x, noise, \\ntimesteps)\\nshow_images((noised_x * 0.5 + 0.5).clip(0, \\n1))\\nFigure 4-6. Using a scheduler to add varying amounts of noise to\\ninput images.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='During training, we’ll pick the timesteps at random. The\\nscheduler takes some parameters (beta_start and\\nbeta_end), which it uses to determine how much noise should\\nbe present for a given timestep. We will cover schedulers in\\nmore detail in the In Depth: Noise Schedules section later on in\\nthis chapter.\\nThe UNet\\nThe UNet is a convolutional neural network invented for tasks\\nsuch as image segmentation, where the desired output has the\\nsame shape as the input. For example, UNets are used in\\nmedical imaging to segment diﬀerent anatomical structures.\\nThe UNet consists of a series of downsampling layers that\\nreduce the spatial size of the input, followed by a series of\\nupsampling layers that increase the spatial extent of the input\\nagain. The downsampling layers are typically followed by skip\\nconnections that connect the downsampling layers outputs to\\nthe upsampling layers inputs. This allows the upsampling layers\\nto incorporate ﬁner details from earlier layers, preserving\\nimportant high-resolution information during the denoising\\nprocess.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 4-3. Figure 4-7. Architecture of a simpliﬁed UNet.\\nThe UNet architecture used in the diﬀusers library is more\\nadvanced than the original UNet proposed in 2015,  with\\nadditions like attention and residual blocks. We’ll take a closer\\nlook later, but the key idea here is that it can take in an input\\nand produce a prediction that is the same shape. In diﬀusion\\nmodels, the input can be a noisy image, and the output can be\\nthe predicted noise. With this information, we can now denoise\\nthe input image.\\nHere’s how we might create a UNet and feed our batch of noisy\\nimages through it:\\n6'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from diffusers import UNet2DModel\\n# Create a UNet2DModel\\nmodel = UNet2DModel(\\n    in_channels=3,  # 3 channels for RGB \\nimages\\n    sample_size=64,  # Specify our input size\\n    # The number of channels per block \\naffects the model size\\n    block_out_channels=(64, 128, 256, 512),\\n    down_block_types=(\\n        \"DownBlock2D\",\\n        \"DownBlock2D\",\\n        \"AttnDownBlock2D\",\\n        \"AttnDownBlock2D\",\\n    ),\\n    up_block_types=(\"AttnUpBlock2D\", \\n\"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\\n).to(device)\\n# Pass a batch of data through to make sure \\nit works\\nwith torch.inference_mode():\\n    out = model(noised_x.to(device), \\ntimestep=timesteps.to(device)).sample'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='print(noised_x.shape)\\nprint(out.shape)\\ntorch.Size([8, 3, 64, 64])\\ntorch.Size([8, 3, 64, 64])\\nNote that the output is the same shape as the input, which is\\nexactly what we want.\\nTraining\\nNow that we have our data and model ready, let’s train it. For\\neach training step, we:\\n1. Load a batch of images.\\n2. Add noise to the images. The amount of noise added\\ndepends on a speciﬁed number of timesteps: the more\\ntimesteps, the more noise. As mentioned, we want our\\nmodel to denoise images with little noise and images with\\nlots of noise. To achieve this, we’ll add random amounts of\\nnoise, so we’ll pick a random number of timesteps.\\n3. Feed the noisy images into the model.\\n4. Calculate the loss using mean squared error (MSE). MSE is a\\ncommon loss function for regression tasks, including the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='UNet model’s noise prediction. It measures the average\\nsquared diﬀerence between predicted and true values,\\npenalizing larger errors more. In the UNet model, MSE is\\ncalculated between predicted and actual noise, helping the\\nmodel generate more realistic images by minimizing the\\nloss. This is called the noise or epsilon objective.\\n5. Backpropagate the loss and update the model weights with\\nthe optimizer.\\nHere’s what all of that looks like in code. Training will take a\\nwhile, so this is a great moment to pause, review the chapter’s\\ncontent, or get some food.\\nfrom torch.nn import functional as F\\nnum_epochs = 50  # How many runs through the \\ndata should we do?\\nlr = 1e-4  # What learning rate should we use\\noptimizer = \\ntorch.optim.AdamW(model.parameters(), lr=lr)\\nlosses = []  # Somewhere to store the loss \\nvalues for later plotting\\n# Train the model (this takes a while)\\nfor epoch in range(num_epochs):\\n    for batch in train_dataloader:'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='# Load the input images\\n        clean_images = \\nbatch[\"images\"].to(device)\\n        # Sample noise to add to the images\\n        noise = \\ntorch.randn(clean_images.shape).to(device)\\n        # Sample a random timestep for each \\nimage\\n        timesteps = torch.randint(\\n            0,\\n            \\nscheduler.config.num_train_timesteps,\\n            (clean_images.shape[0],),\\n            device=device,\\n        ).long()\\n        # Add noise to the clean images \\naccording\\n        # to the noise magnitude at each \\ntimestep\\n        noisy_images = \\nscheduler.add_noise(clean_images, noise, \\ntimesteps)\\n        # Get the model prediction for the \\nnoise'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='# The model also uses the timestep as \\nan input\\n        # for additional conditioning\\n        noise_pred = model(noisy_images, \\ntimesteps, return_dict=False)[0]\\n        # Compare the prediction with the \\nactual noise\\n        loss = F.mse_loss(noise_pred, noise)\\n        # Store the loss for later plotting\\n        losses.append(loss.item())\\n        # Update the model parameters with \\nthe optimizer based on this loss\\n        loss.backward()\\n        optimizer.step()\\n        optimizer.zero_grad()\\n    # Print out the average of the loss \\nvalues for this epoch:\\n    avg_loss = sum(losses[-\\nlen(train_dataloader) :]) / \\nlen(train_dataloader)\\n    print(\\n        f\"Finished epoch {epoch}. Average'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='loss for this epoch: {avg_loss:05f}\"\\n    )\\nNow that the model is trained, let’s plot the training loss.\\nfrom matplotlib import pyplot as plt\\nplt.subplots(1, 2, figsize=(12, 4))\\nplt.subplot(1, 2, 1)\\nplt.plot(losses)\\nplt.title(\"Training loss\")\\nplt.xlabel(\"Training step\")\\nplt.subplot(1, 2, 2)\\nplt.plot(range(400, len(losses)), \\nlosses[400:])\\nplt.title(\"Training loss from step 400\")\\nplt.xlabel(\"Training step\");'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 4-8. Training loss of a simple UNet for diﬀusion. The left\\nshows all the steps while the right skips the ﬁst 400 steps.\\nThe loss curve, shown in Figure 4-8, trends downwards as the\\nmodel learns to denoise the images. The curve is somewhat\\nnoisy - the loss is not very stable. This is because each iteration\\nuses diﬀerent numbers of noising time steps. It is hard to tell\\nwhether this model will be good at generating samples by\\nlooking at the mean squared error of the noise predictions, so\\nlet’s move on to the next section and see how well it does.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Sampling\\nNow that we have a model, let’s do inference and generate\\nsome images. The diﬀusers library uses the idea of pipelines to\\nbundle together all of the components needed to generate\\nsamples with a diﬀusion model. We can use a pipeline to test\\nthe UNet we just trained, a few generations are shown in Figure\\n4-9. :\\npipeline = DDPMPipeline(unet=model, \\nscheduler=scheduler)\\nims = pipeline(batch_size=4).images\\nshow_images(ims, nrows=1)\\n  0%|          | 0/1000 [00:00<?, ?it/s]\\n7'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 4-9. Sample low-resolution generations after training the\\nUNet.\\nOﬄoading the job of creating samples to the pipeline doesn’t\\nshow us what is going on under the hood. So, let’s do a simple\\nsampling loop showing how the model gradually reﬁnes the\\ninput image based on the code in the pipeline’s call()\\nmethod (Figure 4-10).'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='# Random starting point (4 random images):\\nsample = torch.randn(4, 3, 64, 64).to(device)\\nfor t in scheduler.timesteps:\\n    # Get the model prediction\\n    with torch.inference_mode():\\n        noise_pred = model(sample, t)\\n[\"sample\"]\\n    # Update sample with step\\n    sample = scheduler.step(noise_pred, t, \\nsample).prev_sample\\nshow_images(sample.clip(-1, 1) * 0.5 + 0.5, \\nnrows=1)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 4-10: A few additional images from the same model,\\ngenerated with a sampling loop.\\nThis is the same code we used at the beginning of the chapter to\\nillustrate the idea of iterative reﬁnement, but now you better\\nunderstand what is happening here. If you look at the\\nimplementation of the DDPMPipeline in the diﬀusers library,\\nyou’ll see that the logic closely resembles our implementation\\nin the previous snippet.\\nWe start with a completely random input, which the model then\\nreﬁnes in a series of steps. Each step is a small update to the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='input based on the model’s prediction for the noise at that\\ntimestep. We’re still abstracting away some complexity behind\\nthe call to pipeline.scheduler.step(); later, we will dive\\ndeeper into diﬀerent sampling methods and how they work.\\nEvaluation\\nEvaluating generative models is complex - it’s a subjective task\\nin nature. For example, given an input prompt \"image of a\\ncat with sunglasses“, there are many potential correct\\ngenerations. A common approach is to combine qualitative\\nevaluation (e.g., by having humans compare generations) and\\nquantitative metrics, which provide a framework for\\nevaluation but don’t necessarily correspond to high image\\nquality.\\nFID (Fréchet Inception Distance) scores can evaluate generative\\nmodel performance. FID scores compare how similar two image\\ndatasets are. Using a pretrained neural network (example\\nshown in Figure 4-11), they measure how closely generated\\nsamples match real samples by comparing statistics between\\nfeature maps extracted from both datasets. The lower the score,\\nthe better the quality and realism of generated images\\nproduced by a given model. FID scores are popular due to their\\nability to provide an \"objective\" comparison metric for'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='diﬀerent types of generative networks without relying on\\nhuman judgment.\\nFigure 4-4. Figure 4-11: CNN network used to extract feature maps from images.\\nAs convenient as FID scores are, there are important caveats to\\nbe aware of (which might be true for other evaluation metrics\\nas well):\\nFID scores are designed to compare two distributions.\\nBecause of this, it assumes that we have access to a source\\ndataset for comparison. A second issue is that you cannot\\ncalculate the FID score of a single generation. If we have\\none image, there’s no way to calculate its FID score.\\nThe FID score for a given model depends on the number of\\nsamples used to calculate it, so when comparing models, we\\nneed to make sure both reported scores are calculated\\nusing the same number of samples. The common practice is\\nto use 50,000 samples for this purpose, although to save\\ntime, you may evaluate a smaller number of samples'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='during development and only do the complete evaluation\\nonce you’re ready to publish the results.\\nThe FID can be sensitive to many factors. For example, a\\ndiﬀerent number of inference steps will lead to a very\\ndiﬀerent FID. The scheduler (DDPM in this case) will also\\naﬀect the FID.\\nWhen calculating the FID, images are resized to 299x299\\nimages. This makes it less useful as a metric for extremely\\nlow or high-resolution images. There are also minor\\ndiﬀerences between how resizing is handled by diﬀerent\\ndeep learning frameworks, which can result in slight\\ndiﬀerences in the FID score.\\nThe network used as a feature extractor for FID is typically\\na model trained on the ImageNet classiﬁcation task . When\\ngenerating images in a diﬀerent domain, the features\\nlearned by this model may be less useful. A more accurate\\napproach is to ﬁrst train a classiﬁcation network on\\ndomain-speciﬁc data, making comparing scores between\\ndiﬀerent papers and techniques harder. For now, the\\nImageNet model is the standard choice.\\nIf you save generated samples for later evaluation, the\\nformat and compression can aﬀect the FID score. Avoid\\nlow-quality JPEG images where possible.\\n8'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Even if you account for all these caveats, FID scores are just a\\nrough measure of quality and do not perfectly capture the\\nnuances of what makes images look more \"real“. The\\nevaluation of generative models is an active research area.\\nStandard metrics like Kernel Inception Distance (KID) and\\nInception Score share similar issues with FID. So, use these\\nmetrics to get an idea of how one model performs relative to\\nanother, but also look at the actual images generated by each\\nmodel to get a better sense of how they compare.\\nImage quality, as measured by FID or KID, is only one of the\\nmetrics we can use to evaluate the performance of text-to-\\nimage models. Eﬀorts such as HEIM (Holistic Evaluation of Text-\\nTo-Image Models) attempt to take into account additional\\ndesirable characteristics of text-to-image models, such as\\nprompt adherence, originality, reasoning capabilities,\\nmultilinguality, absence of bias and toxicity, and others.\\nHuman preference is still the gold standard for quality in what\\nis ultimately a fairly subjective ﬁeld. For example, the Parti\\nPrompts dataset contains 1600 prompts of varying diﬃculties\\nand categories and allows comparing text-to-image models such\\nas the ones we’ll explore in Chapter 5.9'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='In Depth: Noise Schedules\\nIn the training example above, one of the steps was to \"add\\nnoise in different amounts“. We achieved this by picking\\na random timestep between 0 and 1000 and then relying on the\\nscheduler to add the appropriate amount of noise. Likewise,\\nduring inference, we again relied on the scheduler to tell us\\nwhich timesteps to use and how to move from one to the next,\\ngiven the model predictions. Choosing how much noise to add is\\na crucial design decision that can drastically aﬀect the\\nperformance of a given model. In this section, we’ll see why this\\nis the case and explore diﬀerent approaches used in practice.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Why Add Noise?\\nFigure 4-5. Figure 4-12: The general principles of diﬀusion work for other types of\\ncorruption, not just Gaussian noise (image from the Cold Diﬀusion paper,\\nhttps://arxiv.org/pdf/2208.09392)\\nAt the start of this chapter, we said that the key idea behind\\ndiﬀusion models is that of iterative reﬁnement. During training,\\nwe corrupt an input by diﬀerent amounts. During inference, we\\nbegin with a maximally corrupted input (that is, a pure noise\\nimage) and iteratively de-corrupt it, expecting to end up with a\\nnice ﬁnal result eventually.\\nSo far, we’ve focused on one speciﬁc kind of corruption: adding\\nGaussian noise. Gaussian noise is a type of noise that follows a\\nnormal distribution, which as we saw in Chapter 3 has most'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='values around the mean and fewer values as we get further\\naway.  One reason for this focus is the theoretical\\nunderpinnings of diﬀusion models, which assume the use of\\nGaussian noise – if we use a diﬀerent corruption method, we\\nare no longer technically doing diﬀusion. However, a paper\\ntitled Cold Diﬀusion  demonstrated that we do not necessarily\\nneed to constrain ourselves to this method just for theoretical\\nconvenience. They showed (Figure 4-12) that a diﬀusion-model-\\nlike approach works for many diﬀerent corruption methods.\\nThat means that rather than using noise, we can use other\\nimage transformations. For example, models such as Muse,\\nMaskGIT, and Paella have used random token masking or\\nreplacement as equivalent corruption methods.\\nNonetheless, adding noise remains the most popular approach\\nfor several reasons:\\nWe can easily control the amount of noise added, giving a\\nsmooth transition from \"perfect\" to \"completely\\ncorrupted“. This is not the case for something like\\nreducing the resolution of an image, which may result in\\n\"discrete\" transitions.\\nWe can have many valid random starting points for\\ninference, unlike some methods, which may only have a\\n1 0 \\n1 1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='limited number of possible initial (fully corrupted) states,\\nsuch as a completely black image or a single-pixel image.\\nSo, for now, we’ll add noise as our corruption method. Next,\\nlet’s explore how we add noise to our images.\\nStarting Simple\\nWe have some images x, and we’d like to add some random\\nnoise to them. We generate pure Gaussian noise of the same\\ndimensions as the input images with torch.rand_like().\\nx = next(iter(train_dataloader))[\"images\"]\\n[:8]\\nnoise = torch.rand_like(x)\\nOne way we could add varying amounts of noise is to linearly\\ninterpolate (”lerp\" for short) between the images and the\\nnoise by some amount. This gives us a function that smoothly\\ntransitions from the original image x to pure noise as the\\namount varies from 0 to 1:\\ndef corrupt(x, noise, amount):\\n    # Reshape amount so it works correctly \\nwith the original data'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"amount = amount.view(-1, 1, 1, 1)  # make \\nsure it's broadcastable\\n    # Blend the original data and noise based \\non the amount\\n    return (\\n        x * (1 - amount) + noise * amount\\n    )  # equivalent to x.lerp(noise, amount)\\nLet’s see this in action on a batch of data, with the amount of\\nnoise varying from 0 to 1:\\namount = torch.linspace(0, 1, 8)\\nnoised_x = corrupt(x, noise, amount)\\nshow_images(noised_x * 0.5 + 0.5)\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 4-13: Applying noise with linear interpolation between\\nthe image and pure noise.\\nThis is doing what we want: smoothly transitioning from the\\noriginal image to pure noise (Figure 4-13). We’ve created a noise\\nschedule with the continuous time approach, where we\\nrepresent the full path on a time scale from 0 to 1. Other\\napproaches use a discrete time approach, with some large\\ninteger number of timesteps used to deﬁne the noise scheduler.\\nWe can wrap our function into a class that converts from'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='continuous time to discrete timesteps and adds noise\\nappropriately (Figure 4-14):\\nclass SimpleScheduler:\\n    def __init__(self):\\n        self.num_train_timesteps = 1000\\n    def add_noise(self, x, noise, timesteps):\\n        amount = timesteps / \\nself.num_train_timesteps\\n        return corrupt(x, noise, amount)\\nscheduler = SimpleScheduler()\\ntimesteps = torch.linspace(0, 999, 8).long()\\nnoised_x = scheduler.add_noise(x, noise, \\ntimesteps)\\nshow_images(noised_x * 0.5 + 0.5)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 4-14: Demonstration of the SimpleScheduler class,\\nwhich uses linear interpolation to add noise.\\nNow we have something we can directly compare to the\\nschedulers used in the diﬀusers library, such as the\\nDDPMScheduler we used during training. Let’s see how it\\ncompares:\\nscheduler = DDPMScheduler(beta_end=0.01)\\ntimesteps = torch.linspace(0, 999, 8).long()\\nnoised_x = scheduler.add_noise(x, noise, \\ntimesteps)\\nshow_images((noised_x * 0.5 + 0.5).clip(0, \\n1))'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 4-15: Results from the DDPMScheduler on the same\\nbatch shown in Figure 4-14.\\nIf you compare Figure 4-14 (our scheduler) with Figure 4-15\\n(DDPMScheduler), the results are not exactly the same, but\\nsimilar enough to explore training the model with our noise\\nscheduler.\\nThe Math\\nLet’s dive into the underlying math that explains how noise is\\nadded to the original images. One thing to remember is that\\nthere are many notations and approaches in the literature. For'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='example, in some papers, the noise schedule is parametrized\\ncontinuously, so t runs from 0 (no noise) to 1 (fully corrupted),\\nas we did in our corrupt function. Other papers use a discrete\\ntime approach in which the timesteps are integers and run\\nfrom 0 to some large number T, typically 1000. It is possible to\\nconvert between these two approaches the way we did with our\\nSimpleScheduler class - make sure you’re consistent when\\ncomparing diﬀerent models. We’ll stick with the discrete-time\\napproach here.\\nA good place to start for going deeper into the math is the\\nDDPM paper or the Annotated Diﬀusion Model blog post.  If\\nyou feel this section is too dense, it’s okay to focus on the high-\\nlevel concepts and come back to the math later on.\\nLet’s kick things oﬀ by deﬁning how to do a single noise step to\\ngo from timestep t-1 to timestep t. As mentioned earlier, the\\nidea is to add Gaussian noise (ϵ). The noise has unit variance,\\nwhich controls the spread of the noise values. By adding this\\nnoise to the previous step’s image, we gradually corrupt the\\noriginal image, which is a key part of the diﬀusion model’s\\ntraining process.\\nxt =xt−1 +ϵ\\n1 2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='To control the amount of noise added at each step, let’s\\nintroduce βt. This parameter is deﬁned for all timesteps t and\\nspeciﬁes how much noise should be added at each step. In other\\nwords, xt is a mix of xt−1 and some random noise scaled by βt.\\nThis allows us to gradually increase the amount of noise added\\nto the image as we move through the timesteps, which is a key\\npart of the diﬀusion model’s training process.\\nxt =√1−βtxt−1 +√βtϵ\\nWe can further deﬁne the noise addition process as a\\ndistribution, where the noisy xt has a mean √1−βtxt−1 and\\na variance of βt. This distribution helps us model the noise\\naddition process more accurately. This is what the formula\\nlooks like in distribution form:\\nq(xt|xt−1)=N(xt;√1−βtxt−1,βtI).\\nWe’ve now deﬁned a distribution to sample x conditioned on\\nthe previous value. To get the noisy input at timestep t, we\\ncould begin at t=0 and repeatedly apply this single step, which\\nwould be very ineﬃcient. Instead, we can ﬁnd a formula to\\nmove to any timestep t in one go by doing the\\nreparameterization trick. The idea is to precompute the noise\\nschedule, which is deﬁned by the βt values. We can then deﬁne\\nαt =1−βt and α as the cumulative product of all the α'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='values up to the time t, which can be expressed as\\nαt :=Πts=1αs. Using these tools and notation, we can redeﬁne\\nthe distribution and how to sample at a particular time. The\\nnew distribution, q(xt|xt−1), has a mean of αtxt−1 and a\\nvariance of (1−αt)I.\\nq(xt|xt−1)=N(xt;αtxt−1,(1−αt)I).\\nExploring this reparameterization trick is part of the challenges\\nat the end of the chapter. We can now sample a noisy image at\\ntimestep t by using the formula below.\\nxt =√αtx0 +√1−αtϵ\\nThe equation for xt shows that the noisy input at timestep t is a\\ncombination of the original image x0 (scaled by √αt) and ϵ\\n(scaled by √1−αt). Note that we can now calculate a sample\\ndirectly without looping over all previous timesteps, making it\\nmuch more eﬃcient for training diﬀusion models.\\nIn the diﬀusers library, the α values are stored in\\nscheduler.alphas_cumprod. Knowing this, we can plot the\\nscaling factors for the original image x0 and the noise ϵ across\\nthe diﬀerent timesteps for a given scheduler. The diﬀusers\\nlibrary allows us to control the beta values by deﬁning its initial\\nvalue (beta_start), ﬁnal value (beta_end), and how the\\nvalues will step, for example, linearly'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='(beta_schedule=\"linear\"). Figure 4-16 shows that the\\nnoise is scaled up more as we have more timesteps, as expected.\\nfrom genaibook.core import plot_scheduler\\nplot_scheduler(\\n    DDPMScheduler(beta_start=0.001, \\nbeta_end=0.02, beta_schedule=\"linear\")\\n)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 4-16: DDPMScheduler: Amount of noise (orange) added\\nto input image (blue), per timestep).\\nOur SimpleScheduler just linearly mixes between the\\noriginal image and noise, as we can see in Figure 4-17 if we plot\\nthe scaling factors (equivalent to √αt and √(1−αt) in the\\nDDPM case):'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='plot_scheduler(SimpleScheduler())\\nFigure 4-17. SimpleScheduler: Amount of noise added, note\\nhow it compares with DDPMScheduler in Figure 4-17.\\nA good noise schedule will ensure the model sees a mix of\\nimages at diﬀerent noise levels. The best choice will diﬀer\\nbased on the training data. Visualizing a few more options, note\\nthat:'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Setting beta_end too low means we never completely\\ncorrupt the image, so the model will never see anything like\\nthe random noise used as a starting point for inference.\\nSetting beta_end extremely high means that most of the\\ntimesteps are spent on almost complete noise, resulting in\\npoor training performance.\\nDiﬀerent beta schedules give diﬀerent curves. The cosine\\nschedule is popular, as it smoothly transitions from the\\noriginal image to the noise.\\nLet’s visualize these in Figure 4-18.\\nfig, (ax) = plt.subplots(1, 1, figsize=(8, \\n5))\\nplot_scheduler(\\n    DDPMScheduler(beta_schedule=\"linear\"),\\n    label=\"default schedule\",\\n    ax=ax,\\n    plot_both=False,\\n)\\nplot_scheduler(\\n    \\nDDPMScheduler(beta_schedule=\"squaredcos_cap_v2\"),\\n    label=\"cosine schedule\",\\n    ax=ax,\\n    plot_both=False,\\n)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='plot_scheduler(\\n    DDPMScheduler(beta_start=0.001, \\nbeta_end=0.003, beta_schedule=\"linear\"),\\n    label=\"Low beta_end\",\\n    ax=ax,\\n    plot_both=False,\\n)\\nplot_scheduler(\\n    DDPMScheduler(beta_start=0.001, \\nbeta_end=0.1, beta_schedule=\"linear\"),\\n    label=\"High beta_end\",\\n    ax=ax,\\n    plot_both=False,\\n)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 4-18. Comparison of diﬀerent DDPMScheduler\\nschedulers, varying hyperparameters and β schedules.\\nNOTE\\nAll of the schedules shown here are called Variance Preserving (VP), meaning that the\\nvariance of the model input is kept close to 1 across the entire schedule. You may also\\nencounter Variance Exploding (VE) formulations where noise is added to the original\\nimage in diﬀerent amounts (resulting in high-variance inputs). Our\\nSimpleScheduler is almost a VP schedule, but the variance is not quite preserved\\ndue to the linear interpolation.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The importance of exposing the model to a good mix of noised\\nimages –including pure noise, which is the initial state for\\ninference– was explored in a paper titled Common Diﬀusion\\nNoise Schedules and Sample Steps are Flawed , which showed\\nthat some diﬀusion models were not able to generate images\\ntoo bright or too dark because the training schedule didn’t\\ncover all states. As with many diﬀusion-related topics, there is a\\nconstant stream of new papers exploring the topic of noise\\nschedules, so by the time you read this, there will likely be an\\nextensive collection of options to try out.\\nEﬀect of Input Resolution and Scaling\\nOne aspect of noise schedules that has mostly been overlooked\\nuntil recently is the eﬀect of the input size and scaling. Many\\npapers test potential schedulers on small-scale datasets and at\\nlow resolution and then use the best-performing scheduler to\\ntrain their ﬁnal models on larger images. The problem with this\\ncan be seen (Figure 4-19) if we add the same amount of noise to\\ntwo images of diﬀerent sizes:\\n1 3 \\n1 4'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 4-19. Applying the same amount of input noise to images\\nwith diﬀerent resolutions.\\nImages at high resolution tend to contain a lot of redundant\\ninformation. This means that even if a single pixel is obscured\\nby noise, the surrounding pixels have enough information to\\nreconstruct the original image. This is diﬀerent for low-\\nresolution images, where a single pixel can contain a lot of\\nuseful information. Adding the same amount of noise to a low-\\nresolution image will result in a much more corrupted image\\nthan adding the equivalent amount of noise to a high-resolution\\nimage.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Two independent papers from early 2023 thoroughly\\ninvestigated this eﬀect. Each used the new insights to train\\nmodels capable of generating high-resolution outputs without\\nrequiring any of the tricks that have previously been necessary.\\nSimple diﬀusion  introduced a method for adjusting the noise\\nschedule based on the input size, allowing a schedule optimized\\non low-resolution images to be appropriately modiﬁed for a\\nnew target resolution. The other paper  performed similar\\nexperiments and noted another critical variable: input scaling.\\nThat is, how do we represent our images? If the images are\\nrepresented as ﬂoats between 0 and 1, they will have a lower\\nvariance than the noise (typically unit variance). Thus, the\\nsignal-to-noise ratio will be lower for a given noise level than if\\nthe images were represented as ﬂoats between -1 and 1 (which\\nwe used in the training example above) or something else.\\nScaling the input images shifts the signal-to-noise ratio, so\\nmodifying this scaling is another way to adjust when training\\non larger images. This paper, in fact, recommends input scaling\\nas an easy way to adapt training for diﬀerent image sizes. It is\\nalso possible to adjust the noise schedule depending on the\\nresolution, but then it’s more diﬃcult to ﬁnd the optimal\\nschedule because several hyperparameters are involved.\\n1 5 \\n1 6'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='import numpy as np\\nscheduler = DDPMScheduler(beta_end=0.05, \\nbeta_schedule=\"scaled_linear\")\\nimage = load_image(\\n    \\n\"https://images.pexels.com/photos/40986/dog-\\nbulldog-white-tongue-40986.jpeg\",\\n    size=((512, 512)),\\n    return_tensor=True,\\n)\\nt = torch.tensor(300)  # The timestep we\\'re \\nnoising to\\nscales = np.linspace(0.1, 1.0, 4)\\nimages = [image]\\nnoise = torch.randn_like(image)\\nfor b in reversed(scales):\\n    noised = (\\n        scheduler.add_noise(b * (image * 2 - \\n1), noise, t).clip(-1, 1) * 0.5\\n        + 0.5\\n    )\\n    images.append(noised)\\nshow_images(\\n    images[1:],'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='nrows=1,\\n    titles=[f\"Scale: {b}\" for b in \\nreversed(scales)],\\n    figsize=(15, 5),\\n)\\nFigure 4-20. Eﬀect of input scaling. All the images have the same\\ninput noise applied, corresponding to step +t=300+, but we\\nmultiply the input image by diﬀerent scale factors. The noise is\\nmore noticeable as the scale aﬀects the image more. The scale\\nalso decreases the dynamic range (or variance), resulting in\\ndarker-looking inputs. .1 7'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='In Depth: UNets and Alternatives\\nLet’s address the actual model that makes the all-important\\npredictions. To recap, this model must be capable of taking in a\\nnoisy image and outputting its noise, hence enabling denoising\\nthe input image. This requires a model that can take in an\\nimage of arbitrary size and output an image of the same size.\\nFurthermore, the model should be able to make precise\\npredictions at the pixel level while capturing higher-level\\ninformation about the image. A popular approach is to use an\\narchitecture called a UNet. UNets were invented in 2015 for\\nmedical image segmentation and have since become a popular\\nchoice for various image-related tasks.\\nLike the AutoEncoders and VAEs we looked at in the previous\\nchapter, UNets are made up of a series of downsampling and\\nupsampling blocks. The downsampling blocks are responsible\\nfor reducing the image size, while the upsampling blocks are\\nresponsible for increasing the image size. The downsampling\\nblocks typically comprise a series of convolutional layers,\\nfollowed by a pooling or downsampling  layer. The upsampling\\nblocks generally include a series of convolutional layers,\\nfollowed by an upsampling or transposed convolution layer.\\nThe transposed convolution layer is a particular type of\\n1 8'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='convolutional layer that increases the size of the image rather\\nthan reducing it.\\nRegular AutoEncoders and VAE are not good choices for this\\ntask because they are less capable of making precise\\npredictions at the pixel level since they must reconstruct the\\nimages from the low-dimensional latent space. In a UNet, the\\ndownsampling and upsampling blocks are connected by skip\\nconnections, which allow information to ﬂow directly from the\\ndownsampling blocks to the upsampling blocks. This allows the\\nmodel to make precise predictions at the pixel level while also\\ncapturing higher-level information about the image as a whole.\\nA Simple UNet\\nTo better understand the structure of a UNet, let’s build a simple\\nUNet from scratch. Figure 4-21 shows the architecture diagram\\nof a basic UNET.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 4-6. Figure 4-21. Architecture of a basic UNet.\\nWe’ll design a UNet that works with single-channel images (e.g.,\\ngrayscale images), which we could use to build a diﬀusion\\nmodel for datasets such as MNIST. We’ll use three layers in the\\ndownsampling path and another three in the upsampling path.\\nEach layer consists of a convolution followed by an activation\\nfunction and an upsampling or downsampling step, depending\\non whether they are in the encoding or decoding path. The skip\\nconnections, as mentioned, directly connect the downsampling\\nblocks to the upsampling ones. There are multiple ways to\\nimplement the skip connections.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='One approach, which we’ll use here, is to add the output of the\\ndownsampling block to the input of the corresponding\\nupsampling block. Another method is concatenating the\\ndownsampling block’s output to the upsampling block’s input.\\nWe could even add some additional layers in the skip\\nconnections. Let’s keep things simple for now with the initial\\napproach. Here’s what this network looks like in code:\\nfrom torch import nn\\nclass BasicUNet(nn.Module):\\n    \"\"\"A minimal UNet implementation.\"\"\"\\n    def __init__(self, in_channels=1, \\nout_channels=1):\\n        super().__init__()\\n        self.down_layers = nn.ModuleList(\\n            [\\n                nn.Conv2d(in_channels, 32, \\nkernel_size=5, padding=2),\\n                nn.Conv2d(32, 64, \\nkernel_size=5, padding=2),\\n                nn.Conv2d(64, 64, \\nkernel_size=5, padding=2),\\n            ]\\n        )'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='self.up_layers = nn.ModuleList(\\n            [\\n                nn.Conv2d(64, 64, \\nkernel_size=5, padding=2),\\n                nn.Conv2d(64, 32, \\nkernel_size=5, padding=2),\\n                nn.Conv2d(32, out_channels, \\nkernel_size=5, padding=2),\\n            ]\\n        )\\n        # Use the SiLU activation function, \\nwhich has been shown to work well\\n        # due to different properties \\n(smoothness, non-monotonicity, etc.).\\n        self.act = nn.SiLU()\\n        self.downscale = nn.MaxPool2d(2)\\n        self.upscale = \\nnn.Upsample(scale_factor=2)\\n    def forward(self, x):\\n        h = []\\n        for i, l in \\nenumerate(self.down_layers):\\n            x = self.act(l(x))\\n            if i < 2:  # For all but the \\nthird (final) down layer:\\n                h.append(x)  # Storing output'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='for skip connection\\n                x = self.downscale(x)  # \\nDownscale ready for the next layer\\n        for i, l in \\nenumerate(self.up_layers):\\n            if i > 0:  # For all except the \\nfirst up layer\\n                x = self.upscale(x)  # \\nUpscale\\n                x += h.pop()  # Fetching \\nstored output (skip connection)\\n            x = self.act(l(x))\\n        return x\\nIf you take a grayscale input image of shape (1, 28, 28), the path\\nthrough the model would be as follows:\\n1. The image goes through the downscaling block. The ﬁrst\\nlayer, a 2D convolution with 32 ﬁlters, will make it of shape\\n[32, 28, 28].\\n2. The image is then downscaled with max pooling, making it\\nof shape [32, 14, 14]. The MNIST dataset contains white\\nnumbers drawn on black background (where black is\\nrepresented with number zero). We choose max pooling to'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='select the largest values in a region and thus focus on the\\nbrightest pixels .\\n3. The image goes through the second downscaling block. The\\nsecond layer, a 2D convolution with 64 ﬁlters, will make it\\nof shape [64, 14, 14].\\n4. After another downscaling, the shape is [64, 7, 7].\\n5. There is a third layer in the downscaling block, but no\\ndownscaling this time because we are already using very\\nsmall 7x7 blocks. This will keep the shape of [64, 7, 7].\\n6. We do the same process but in inverse, upscaling to [64, 14,\\n14], [32, 14, 14] and ﬁnally [1, 28, 28].\\nA diﬀusion model trained with this architecture on MNIST\\nproduces the samples shown in Figure 4-22 (code included in\\nthe supplementary material but omitted here for brevity):\\nFigure 4-7. Figure 4-22. Loss and generations of a basic UNet.\\n1 9'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Improving the UNet\\nThis simple UNet works for this relatively easy task. How can\\nwe handle more complex data?\\nAdd more parameters. This can be accomplished by using\\nmultiple convolutional layers in each block, using a larger\\nnumber of ﬁlters in each convolutional layer, or making the\\nnetwork deeper.\\nAdd normalization, such as batch normalization. Batch\\nnormalization can help the model learn more quickly and\\nreliably by ensuring that the outputs of each layer are\\ncentered around 0 and have a standard deviation of 1.\\nAdd regularization, such as dropout. Dropout helps prevent\\noverﬁtting to the training data, which is essential when\\nworking with smaller datasets.\\nAdd attention. Introducing self-attention layers allows the\\nmodel to focus on diﬀerent parts of the image at diﬀerent\\ntimes, which can help the UNet learn more complex\\nfunctions. Adding transformer-like attention layers also lets\\nus increase the number of learnable parameters. The\\ndownside is that attention layers are much more expensive\\nto compute than regular convolutional layers at higher\\nresolutions, so we typically only use them at lower\\nresolutions (e.g., the lower-resolution blocks in the UNet).'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='For comparison, Figure 4-23 shows the results on MNIST when\\nusing the UNet implementation in the diﬀusers library, which\\nfeatures all of the above improvements:\\nFigure 4-8. Figure 4-23. Loss and generations from the diﬀusers UNet, with several\\nimprovements over the basic architecture.\\nAlternative Architectures\\nFigure 4-9. Figure 4-24. Comparison of UNet with UVit and RIN.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='More recently, several alternative architectures have been\\nproposed for diﬀusion models (Figure 4-24). These include:\\nTransformers. The Diﬀusion Transformers paper\\nshowed that a transformer-based architecture can train a\\ndiﬀusion model with excellent results. However, the\\ncompute and memory requirements of the transformer\\narchitecture remain a challenge for very high resolutions.\\nUViT. The UViT architecture from the Simple Diﬀusion\\npaper aims to get the best of both worlds by replacing the\\nmiddle layers of the UNet with a large stack of transformer\\nblocks. A key insight of this paper is that focusing most of\\nthe compute at the lower resolution blocks of the UNet\\nallows for more eﬃcient training of high-resolution\\ndiﬀusion models. For very high resolutions, they do some\\nadditional pre-processing using something called a wavelet\\ntransform to reduce the spatial resolution of the input\\nimage while keeping as much information as possible\\nthrough additional channels, again reducing the amount of\\ncompute spent on the higher spatial resolutions.\\nRecurrent Interface Networks. The RIN paper  takes a\\nsimilar approach, ﬁrst mapping the high-resolution inputs\\nto a more manageable and lower-dimensional latent\\nrepresentation, which is then processed by a stack of\\ntransformer blocks before being decoded back out to an\\n2 0 \\n2 1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='image. Additionally, the RIN paper introduces the idea of\\nrecurrence where information is passed to the model from\\nthe previous processing step. This can beneﬁt the iterative\\nimprovement that diﬀusion models are designed to\\nperform.\\nSome high-quality diﬀusion transformers models include Flux,\\nStable Diﬀusion 3, PixArt-Σ, and the text-to-video Sora. It\\nremains to be seen whether transformer-based approaches\\ncompletely supplant UNets as the go-to architecture for\\ndiﬀusion models or whether hybrid approaches like the UViT\\nand RIN architectures will be the most eﬀective.\\nIn Depth: Diﬀusion Objectives\\nWe’ve discussed diﬀusion models taking a noisy input and\\nlearning to denoise it. At ﬁrst glance, you might assume that the\\nnetwork’s natural prediction target is the image’s denoised\\nversion, which we’ll call x0. However, we compared the model\\nprediction in the code with the unit-variance noise used to\\ncreate the noisy version (often called the epsilon objective,\\neps). The two appear mathematically identical since if we\\nknow the noise and the timestep, we can derive x0 and vice\\nversa. While this is true, the objective choice has some subtle'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='eﬀects on how large the loss is at diﬀerent timesteps and, thus,\\nwhich noise levels the model learns best to denoise. Predicting\\nnoise is easier for the model than directly predicting the target\\ndata. This is because the noise follows a known distribution at\\neach step, and predicting the diﬀerence between two steps is\\noften simpler than predicting the absolute values of the target\\ndata.\\nTo gain some intuition, let’s visualize some diﬀerent objectives\\nacross diﬀerent timesteps. The input image and the random\\nnoise in Figure 4-25 are the same (ﬁrst two rows in the\\nillustration), but the noised images in the third row have\\ndiﬀerent amounts of added noise depending on the timestep.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 4-25. Comparing eps vs x0 vs v objectives. eps tries\\nto predict the noise added at each timestep, x0 predicts the\\ndenoised image and v uses a mixture of the two.\\nAt extremely low noise levels, the x0 objective is trivially easy\\n(the noised image is almost the same as the input), while\\npredicting the noise accurately is almost impossible. Likewise,\\nat extremely high noise levels, the eps objective is\\nstraightforward (the noised image is almost equal to the pure\\nnoise added), while predicting the denoised image accurately is\\nalmost impossible. If we use the x0 objective, our training will\\nput less weight on lower noise levels. Neither case is ideal, and'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='so additional objectives have been introduced that have the\\nmodel predict a mix of x0 and eps at diﬀerent timesteps. The\\nvelocity (v) objective, shown in the last row of the illustration,\\nis one such objective, which is deﬁned as\\nv=√α⋅ϵ+√1−α⋅x0. The eps objective remains one of\\nthe most preferred approaches, but it’s important to be aware\\nof its disadvantages and the existence of other objectives.\\nNOTE\\nA group of researchers at NVIDIA worked to unify the diﬀerent formulations of\\ndiﬀusion models into a consistent framework with a clear separation of design\\nchoices. This allowed them to identify changes in the sampling and training\\nprocesses, resulting in better performance, leading to what is known as k-diﬀusion. If\\nyou’re interested in learning more about the diﬀerent objectives, scalings, and\\nnuances of the diﬀusion model formulations, we recommend reading the EDM paper\\nfor a more in-depth discussion.\\nProject Time: Train Your Diﬀusion\\nModel\\nOk, that’s enough theory. It’s now time for you to train your\\nunconditional diﬀusion model. As before, you’ll train a model to\\ngenerate new images. The main challenge of this project will be\\ncreating or ﬁnding a good dataset you can use for this.\\n2 2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='In case you want to use an existing dataset, a good starting\\npoint is to ﬁlter for image classiﬁcation datasets on the Hugging\\nFace Hub and pick one of your liking. One of the main questions\\nyou will want to answer is which part of the dataset you want\\nto use for training. Will you use the whole dataset, as before, so\\nthe model generates digits? Or will you use a speciﬁc class, e.g.,\\ncats, so we get a cats expert model? Or will you use a subset\\nof the dataset, e.g., only images with a certain resolution?\\nIf you want to upload a new dataset instead, the ﬁrst step will\\nbe to ﬁnd and access the data. To share a dataset, the most\\nstraightforward approach is to use the ImageFolder feature of\\nthe datasets library. You can then upload the dataset to the\\nHugging Face Hub and use it in your project.\\nOnce you have the data, think about the pre-processing steps,\\nthe model deﬁnition, and the training loop. You can use the\\ncode from the chapter as a starting point and modify it to ﬁt\\nyour dataset.\\nSummary\\nWe started the chapter using high-level pipelines to run\\ninference of diﬀusion models. We ended up training our'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='diﬀusion model from scratch and diving into each component.\\nLet’s do a brief recap.\\nThe goal is to train a model, usually a UNet, that receives noisy\\nimages as input and can predict the noise part of that image.\\nWhen training our model, we add noise in diﬀerent magnitudes\\naccording to a random number of timesteps. One of the\\nchallenges we saw was that to add noise at a high number of\\nsteps, 900, for example, we would need to do a high number of\\nnoise iterations. To ﬁx this, we use the reparameterization trick,\\nwhich allows us to obtain the noisy input at a speciﬁc timestep\\ndirectly. The model is trained to minimize the diﬀerence\\nbetween the noise predictions and the actual input noise. For\\ninference, we do an iterative reﬁnement process in which the\\nmodel reﬁnes the initial random input. Rather than keeping the\\nﬁnal prediction of a single diﬀusion step, we iteratively modify\\nthe input x by a small amount in the direction of that\\nprediction. This, of course, is one of the reasons why doing\\ninference with diﬀusion models tends to be slow and becomes\\none of its main disadvantages compared to models like GANs.\\nThe diﬀusion world is fast-moving, so many advances exist (e.g.,\\nthe scheduler, the model, the training techniques, and so on).\\nThis chapter focused on foundations that will allow us to jump\\nto conditional generation (e.g., generating an image conditioned'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='on an input prompt) and provide a background for you to dive\\ndeeper into the diﬀusion world. Some of the readings through\\nthis chapter can help you dive deeper.\\nFor additional readings, we suggest to review:\\nThe Annotated Diﬀusion Model: This blog post does a\\ntechnical write-up of the DDPM paper. It can be accessed at\\nhttps://huggingface.co/blog/annotated-diﬀusion\\nLilian Weng’s write-up is excellent for a deeper dive into\\nthe math. It can be accessed at\\nhttps://lilianweng.github.io/posts/2021-07-11-diﬀusion-\\nmodels/\\nThe Denoising Diﬀusion Probabilistic Models paper itself at\\nhttps://arxiv.org/abs/2006.11239\\nKarras work on unifying the formulations of diﬀusion\\nmodels at http://arxiv.org/abs/2206.00364\\nSimple diﬀusion, which explains how to adjust the sample\\nschedule for diﬀerent sizes. Check out the paper at\\nhttps://arxiv.org/abs/2301.11093\\nExercises\\n1. Explain the diﬀusion inference algorithm.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='2. What’s the role of the noise scheduler?\\n3. When creating a training dataset of images, which\\ncharacteristics are important to watch?\\n4. Why do we randomly ﬂip training images?\\n5. How can we evaluate the generations of diﬀusion models?\\n6. How do the values of beta_end impact the diﬀusion\\nprocess?\\n7. Why do we use UNets rather than VAEs as the main model\\nfor diﬀusion?\\n8. What beneﬁts and challenges are faced when incorporating\\ntechniques from transformers (like attention layers or a\\ntransformer-based architecture) to diﬀusion?\\nYou can ﬁnd the solutions to these exercises in the GitHub\\nrepository of the book.\\nChallenges\\n9. Show that\\nxt =√1−βtxt−1 +√βtϵ\\nis equivalent to\\nxt =√αtx0 +√1−αtϵ'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Note that this is not a trivial example and is not required to use\\ndiﬀusion models. We recommend reviewing A Beginner’s Guide\\nto Diﬀusion Models: Understanding the Basics and Beyond or for\\nguidance. An important thing to know is how to merge two\\nGaussians: if you have two Gaussians with diﬀerent variance,\\nN(μ1,σ21) and N(μ2,σ22), the resulting Gaussian is\\nN(μ1 +μ2,σ21 +σ22).\\n10. This chapter uses the DDPM scheduler, sometimes\\nrequiring hundreds or thousands of steps to achieve high-\\nquality results. Recent research has explored achieving\\ngood generations with as few steps as possible, down to\\neven one or two. The diﬀusers library contains multiple\\nschedulers such as the DDIMScheduler from the Denoising\\nDiﬀusion Implicit Models paper  Create some images using\\nthe DDIMScheduler. This chapter’s sampling section\\nrequired 1000 steps with the DDPMScheduler. How many\\nsteps are you required to generate images with similar\\nquality? Experiment switching the scheduler for the\\ngoogle/ddpm-celebahq-256 and compare both\\nschedulers.\\n2 3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='References\\n1. Bansal, Arpit, et al.\\xa0Cold Diﬀusion: Inverting Arbitrary Image\\nTransforms Without Noise. arXiv, 19 Aug.\\xa02022. arXiv.org,\\nhttp://arxiv.org/abs/2208.09392\\n2. Chen, Ting. On the Importance of Noise Scheduling for\\nDiﬀusion Models. arXiv, 21 May 2023. arXiv.org,\\nhttp://arxiv.org/abs/2301.10972\\n3. Ho, Jonathan, et al.\\xa0Denoising Diﬀusion Probabilistic Models.\\narXiv, 16 Dec.\\xa02020. arXiv.org,\\nhttp://arxiv.org/abs/2006.11239\\n4. Hoogeboom, Emiel, Jonathan Heek, and Tim Salimans.\\nSimple diﬀusion: End-to-end diﬀusion for high resolution\\nimages. In International Conference on Machine Learning,\\npp.\\xa013213-13232. PMLR, 2023.\\nhttp://arxiv.org/abs/2301.11093.\\n5. Jabri, Allan, et al.\\xa0Scalable Adaptive Computation for\\nIterative Generation. arXiv, 13 June 2023. arXiv.org,\\nhttp://arxiv.org/abs/2212.11972\\n6. Karras, Tero, et al.\\xa0Elucidating the Design Space of Diﬀusion-\\nBased Generative Models. arXiv, 11 Oct.\\xa02022. arXiv.org,\\nhttp://arxiv.org/abs/2206.00364\\n7. Lee, Tony, et al.\\xa0Holistic Evaluation of Text-To-Image Models.\\nAdvances in Neural Information Processing Systems 36'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='(2024).\\n8. Lin, Shanchuan, et al.\\xa0Common Diﬀusion Noise Schedules\\nand Sample Steps are Flawed. In Proceedings of the\\nIEEE/CVF Winter Conference on Applications of Computer\\nVision, pp.\\xa05404-5411. 2024. https://arxiv.org/abs/2305.08891\\n9. Peebles, William, and Saining Xie. Scalable Diﬀusion Models\\nwith Transformers. arXiv, 2 Mar.\\xa02023. arXiv.org,\\nhttp://arxiv.org/abs/2212.09748\\n10. Rogge, Niels, and Rasul, Kashif, The Annotated Diﬀusion\\nModel. Hugging Face Blog,\\nhttps://huggingface.co/blog/annotated-diﬀusion\\n11. Ronneberger, Olaf, et al.\\xa0U-Net: Convolutional Networks for\\nBiomedical Image Segmentation. arXiv, 18 May 2015.\\narXiv.org, http://arxiv.org/abs/1505.04597\\n12. Song, Jiaming, et al.\\xa0Denoising Diﬀusion Implicit Models.\\narXiv, 5 Oct.\\xa02022. arXiv.org, https://arxiv.org/abs/2010.02502\\n There’s a lot of research about reducing the number of diﬀusion steps in inference,\\nplease check exercise 10 at the end of the chapter for an initial glimpse on the area.\\n A subset of a dataset compiled by Ceyda Cinarel with butterﬂies extracted from the\\nSmithsonian Institute and can be accessed at\\nhttps://huggingface.co/datasets/huggan/smithsonian_butterﬂies_subset\\n torchvision is a PyTorch library that provides a wide range of tools for working with\\nimages. In the book, we’ll use this library only for data pre-processing\\n1 \\n2 \\n3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='transformations.\\n We used images larger than 64x64 to print beautiful butterﬂies in the book instead\\nof pixelated ones.\\n Ho, Jonathan, et al.\\xa0Denoising Diﬀusion Probabilistic Models. arXiv, 16 Dec.\\xa02020.\\narXiv.org, http://arxiv.org/abs/2006.11239\\n Ronneberger, Olaf, et al.\\xa0U-Net: Convolutional Networks for Biomedical Image\\nSegmentation. arXiv, 18 May 2015. arXiv.org, http://arxiv.org/abs/1505.04597\\n The images were generated by the model we trained at a resolution of 64x64 and\\nupscaled, so they’ll look pixelated\\n ImageNet is one of the most popular Computer Vision benchmarks. It contains\\nmillions of images in thousands of categories, making it a popular dataset for\\ntraining and benchmarking base models\\n For a practical deep dive into evaluating diﬀusion models, we suggest to review the\\ndiﬀusers library Evaluating Diﬀusion Models documentation at\\nhttps://huggingface.co/docs/diﬀusers/en/conceptual/evaluation\\n The Gaussian noise is added with torch.rand_like().\\n Bansal, Arpit, et al.\\xa0Cold Diﬀusion: Inverting Arbitrary Image Transforms Without\\nNoise. arXiv, 19 Aug.\\xa02022. arXiv.org, http://arxiv.org/abs/2208.09392\\n The Annotated Diﬀusion Model is an excellent blog post\\n(https://huggingface.co/blog/annotated-diﬀusion) that explains and implements the\\nwhole diﬀusion process from scratch.\\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n 0 \\n 1 \\n 2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Lin, Shanchuan, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diﬀusion noise\\nschedules and sample steps are ﬂawed. In Proceedings of the IEEE/CVF winter\\nconference on applications of computer vision, pp.\\xa05404-5411. 2024., arXiv\\nhttps://arxiv.org/abs/2305.08891\\n The diﬀusers documentation page on schedulers can be a good place to get started\\nwith the multiple schedulers variants.\\n Hoogeboom, Emiel, et al.\\xa0Simple Diﬀusion: End-to-End Diﬀusion for High Resolution\\nImages. arXiv, 26 Jan.\\xa02023. arXiv.org, http://arxiv.org/abs/2301.11093\\n Chen, Ting. On the Importance of Noise Scheduling for Diﬀusion Models. arXiv, 21\\nMay 2023. arXiv.org, http://arxiv.org/abs/2301.10972\\n In this regime, input images are normalized before being passed to the model to not\\nreduce variance so drastically.\\n Pooling is the method to choose the information to preserve when downsampling\\nthe output from a previous layer. Common strategies include average pooling, which\\nreduces a patch to its average value, or max pooling, which selects the maximum\\nvalue in a given patch. Pooling is applied independently to all the channels of the\\ninput tensor.\\n For visualization purposes, we show MNIST as black numbers on white background,\\nbut the training dataset uses the opposite\\n Peebles, William, and Saining Xie. Scalable Diﬀusion Models with Transformers.\\narXiv, 2 Mar.\\xa02023. arXiv.org, http://arxiv.org/abs/2212.09748\\n Jabri, Allan, et al.\\xa0Scalable Adaptive Computation for Iterative Generation. arXiv, 13\\nJune 2023. arXiv.org, http://arxiv.org/abs/2212.11972\\n 3 \\n 4 \\n 5 \\n 6 \\n 7 \\n 8 \\n 9 \\n 0 \\n 1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Karras, Tero, et al.\\xa0Elucidating the Design Space of Diﬀusion-Based Generative Models.\\narXiv, 11 Oct.\\xa02022. arXiv.org, http://arxiv.org/abs/2206.00364\\n Song, Jiaming, et al.\\xa0Denoising Diﬀusion Implicit Models. arXiv, 5 Oct.\\xa02022. arXiv.org,\\nhttps://arxiv.org/abs/2010.02502.\\n 2 \\n 3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Chapter 5. Stable Diﬀusion and\\nConditional Generation\\nA NOTE FOR EARLY RELEASE READERS\\nWith Early Release ebooks, you get books in their earliest form\\n—the authors’ raw and unedited content as they write—so you\\ncan take advantage of these technologies long before the oﬃcial\\nrelease of these titles.\\nThis will be the ﬁfth chapter of the ﬁnal book. Please note that\\nthe GitHub repo will be made active later on.\\nIf you have comments about how we might improve the content\\nand/or examples in this book, or if you notice missing material\\nwithin this chapter, please reach out to the editor at\\njleonard@oreilly.com.\\nIn the previous chapter, we introduced diﬀusion models and\\nthe underlying idea of iterative reﬁnement. By the end of the\\nchapter, we could generate images, but training the model was\\ntime-consuming, and we had no control over the generated\\nimages. In this chapter, we’ll see how to go from this to text-\\nconditioned models that can eﬃciently generate images based'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='on text descriptions, with a model called Stable Diﬀusion (SD)\\nas a case study. Before we get to SD, though, we’ll look at how\\nconditional models work and review some of the innovations\\nthat led to the text-to-image models we have today.\\nAdding Control: Conditional Diﬀusion\\nModels\\nBefore we tackle the challenge of generating images from text\\ndescriptions, let’s start with something slightly easier. We’ll\\nexplore how we can steer our model outputs towards speciﬁc\\ntypes or classes of images. We can use a method called\\nconditioning, where the idea is to ask the model to generate not\\njust any image but an image belonging to a pre-deﬁned class. In\\nthis context, conditioning refers to guiding the model’s output\\nby providing additional information, such as a label or prompt,\\nduring the generation process.\\nModel conditioning is a simple but eﬀective concept. We’ll start\\nfrom the diﬀusion model we used in Chapter 4 and introduce a\\nfew changes. First, rather than using the butterﬂies dataset,\\nwe’ll switch to a dataset that has classes. We’ll use Fashion\\nMNIST, a dataset with thousands of images of clothes associated\\nwith labels from 10 diﬀerent classes. Then, crucially, we’ll'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='provide the model with two inputs: the images –just like\\nbefore–, but also the class label each image belongs to. By doing\\nso, we expect the model to learn the associations between\\nimages and labels, helping it understand the distinctive features\\nof sweaters, boots, and other clothing items.\\nNote that we are not interested in solving a classiﬁcation\\nproblem – we don’t want the model to tell us which class the\\nimage belongs to. We still want it to perform the same task as in\\nChapter 4: Please generate plausible images that look like they\\ncame from this dataset. The only diﬀerence is that we are giving\\nit additional information about those images. We’ll use the\\nsame loss function and training strategy, as it’s the same task as\\nbefore.\\nPreparing the Data\\nWe need a dataset with distinct groups of images. Datasets\\nintended for computer vision classiﬁcation tasks are ideal for\\nthis purpose. We could start with something like the ImageNet\\ndataset, which contains millions of images across 1000 classes.\\nHowever, training models on this dataset would take an\\nextremely long time. When approaching a new problem,\\nstarting with a smaller dataset is a good idea to ensure\\neverything works as expected. This keeps the feedback loop'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='short so we can iterate quickly and ensure we’re on the right\\ntrack.\\nWe could choose MNIST for this example, as in Chapter 4. To\\nmake things just a little bit diﬀerent, we’ll choose Fashion\\nMNIST instead. Fashion MNIST, developed and open-sourced by\\nZalando, is a replacement for MNIST that shares similar\\ncharacteristics: a compact size, black-and-white images, and ten\\nclasses. The main diﬀerence is that classes correspond to\\ndiﬀerent types of clothing instead of being digits, and the\\nimages contain more detail than simple handwritten digits, as\\nshown in Figure 5-1.\\nJust as in Chapter 3, we’ll need to conﬁgure matplotlib to use\\nreversed gray colors to match the Fashion MNIST dataset. Let’s\\ncheck out at some examples.\\nimport matplotlib as mpl\\nfrom datasets import load_dataset\\nfrom genaibook.core import show_images\\nmpl.rcParams[\"image.cmap\"] = \"gray_r\"\\nfashion_mnist = load_dataset(\"fashion_mnist\")'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='clothes = fashion_mnist[\"train\"][\"image\"][:8]\\nclasses = fashion_mnist[\"train\"][\"label\"][:8]\\nshow_images(clothes, titles=classes, figsize=\\n(4, 2.5))\\nFigure 5-1. Examples from the Fashion MNIST dataset with their\\ncorresponding labels.\\nSo class 0 corresponds to a t-shirt, 2 is a sweater, and 9 is a\\nboot . We prepare our dataset and DataLoader similarly to\\nhow we did it in Chapter 4, with the main diﬀerence that we’ll\\nalso include the class information as input. Instead of resizing\\nas we did in Chapter 4, we’ll pad our image inputs (28x28\\npixels) to 32x32. This will preserve the original image quality,\\nwhich will help the UNet make higher quality predictions.\\nPadding helps avoid issues where the operations might crop out\\nor distort edge information.\\n1 \\n2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='import torch\\nfrom torchvision import transforms\\npreprocess = transforms.Compose(\\n    [\\n        transforms.RandomHorizontalFlip(),  # \\nRandomly flip (data augmentation)\\n        transforms.ToTensor(),  # Convert to \\ntensor (0, 1)\\n        transforms.Pad(2),  # Add 2 pixels on \\nall sides\\n        transforms.Normalize([0.5], [0.5]),  \\n# Map to (-1, 1)\\n    ]\\n)  \\ndef transform(examples):  \\n    images = [preprocess(image) for image in \\nexamples[\"image\"]]\\n    return {\"images\": images, \"labels\": \\nexamples[\"label\"]}  \\ntrain_dataset = \\nfashion_mnist[\"train\"].with_transform(transform)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='train_dataloader = \\ntorch.utils.data.DataLoader(\\n    train_dataset, batch_size=256, \\nshuffle=True\\n)  \\nDeﬁne a series of transformations (ﬂipping, converting to\\ntensor, padding, and normalizing) that will be applied to the\\nimages in the dataset.\\nProcess a batch of images using the transformations.\\nReturn a dictionary containing the processed images and\\ntheir corresponding labels.\\nLoad the dataset’s train split. By using with_transform,\\nyou ensure that items are returned after applying the\\ntransformation.\\nCreate a DataLoader that will build the batches and shuﬄe\\nthe data, simplifying our code.\\nCreating a Class-Conditioned Model\\nThe UNet from the diﬀusers library allows providing custom\\nconditioning information. Here, we create a similar model to'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='the one we used in Chapter 4, but we add a\\nnum_class_embeds argument to the UNet constructor. This\\nargument tells the model we’d like to use class labels as\\nadditional conditioning. We’ll use ten as that’s the number of\\nclasses in Fashion MNIST.\\nfrom diffusers import UNet2DModel\\nmodel = UNet2DModel(\\n    in_channels=1,  # 1 channel for grayscale \\nimages\\n    out_channels=1,\\n    sample_size=32,\\n    block_out_channels=(32, 64, 128, 256),\\n    num_class_embeds=10,  # Enable class \\nconditioning\\n)\\nTo make predictions with this model, we must pass in the class\\nlabels as additional inputs to the forward() method:\\nx = torch.randn((1, 1, 32, 32))\\nwith torch.inference_mode():\\nwith torch.inference_mode():\\n    out = model(x, timestep=7,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='class_labels=torch.tensor([2])).sample\\nout.shape\\ntorch.Size([1, 1, 32, 32])\\nNOTE\\nWe also pass something else to the model as conditioning: the timestep. That’s right,\\neven the model from Chapter 4 can be considered a conditional diﬀusion model. We\\ncondition it on the timestep, expecting that knowing how far we are in the diﬀusion\\nprocess will help it generate more realistic images.\\nInternally, the timestep and the class label are turned into\\nembeddings that the model uses during its forward pass. At\\nmultiple stages throughout the UNet, these embeddings are\\nprojected onto a dimension that matches the number of\\nchannels in a given layer. The embeddings are then added to\\nthe outputs of that layer. This means the conditioning\\ninformation is fed to every block of the UNet, giving the model\\nample opportunity to learn how to use it eﬀectively.\\nEmbeddings are eﬀective in diﬀusion models because they\\nprovide a compact and dense representation of conditioning\\ninformation, such as timesteps and class labels, which the\\nmodel can easily integrate throughout the UNet architecture.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The ﬂexibility of embeddings also allows for eﬀective handling\\nof diﬀerent types of conditioning inputs, whether they are\\ncontinuous (like timesteps), categorical (like class labels), or\\neven text-based (like prompts).\\nTraining the Model\\nAdding noise works just as well on grayscale images as on the\\nbutterﬂies from Chapter 4. Let’s look at the impact of noise as\\nwe do more noising timesteps, as shown in Figure 5-2.\\nfrom diffusers import DDPMScheduler\\nscheduler = DDPMScheduler(\\n    num_train_timesteps=1000, \\nbeta_start=0.001, beta_end=0.02\\n)\\ntimesteps = torch.linspace(0, 999, 8).long()\\nbatch = next(iter(train_dataloader))\\n# We load 8 images from the dataset and\\n# add increasing amounts of noise to them\\nx = batch[\"images\"][0].expand([8, 1, 32, 32])\\nnoise = torch.rand_like(x)\\nnoised_x = scheduler.add_noise(x, noise, \\ntimesteps)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='show_images((noised_x * 0.5 + 0.5).clip(0, \\n1))\\nFigure 5-2. Noising Fashion MNIST images at diﬀerent\\ntimesteps.\\nOur training loop will be almost the same as in Chapter 4,\\nexcept that we now pass the class labels for conditioning. Note\\nthat this is just additional information for the model, but it'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='doesn’t aﬀect how we deﬁne our loss function in any way. We’ll\\nalso use tqdm to display progress bars during training.\\nThis is a great moment to kick oﬀ the training and get a tea,\\ncoﬀee, or drink of your choice.\\nDo not be intimidated by the following code: it’s similar to what\\nwe’ve done with unconditional generation. We strongly suggest\\nlooking at this code side-by-side with the code in the previous\\nchapter. Can you ﬁnd all diﬀerences? .\\nThe script:\\n1. Loads a batch of images and their corresponding labels\\n(using train_dataloader and tqdm to iterate through it)\\n2. Adds noise to the images based on their timestep (using\\nscheduler.add_noise())\\n3. Feeds the noisy images into the model, alongside the class\\nlabels for conditioning (using model()).\\n4. Calculates the loss.\\n5. Backpropagates the loss and updates the model weights\\nwith the optimizer.\\nfrom torch.nn import functional as F\\nfrom tqdm import tqdm\\n3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from genaibook.core import get_device\\n# Initialize the scheduler\\nscheduler = DDPMScheduler(\\n    num_train_timesteps=1000, \\nbeta_start=0.0001, beta_end=0.02\\n)\\nnum_epochs = 25\\nlr = 3e-4\\noptimizer = \\ntorch.optim.AdamW(model.parameters(), lr=lr, \\neps=1e-5)\\nlosses = []  # To store loss values for \\nplotting\\ndevice = get_device()\\nmodel = model.to(device)\\n# Train the model (this takes a while!)\\nfor epoch in (progress := \\ntqdm(range(num_epochs))):\\n    for step, batch in (\\n        inner := tqdm(\\n            enumerate(train_dataloader),\\n            position=0,\\n            leave=True,\\n            total=len(train_dataloader),'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=')\\n    ):\\n        # Load the input images and classes\\n        clean_images = \\nbatch[\"images\"].to(device)\\n        class_labels = \\nbatch[\"labels\"].to(device)\\n        # Sample noise to add to the images\\n        noise = \\ntorch.randn(clean_images.shape).to(device)\\n        # Sample a random timestep for each \\nimage\\n        timesteps = torch.randint(\\n            0,\\n            \\nscheduler.config.num_train_timesteps,\\n            (clean_images.shape[0],),\\n            device=device,\\n        ).long()\\n        # Add noise to the clean images \\naccording\\n        # to the noise magnitude at each \\ntimestep\\n        noisy_images = \\nscheduler.add_noise(clean_images, noise,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='timesteps)\\n        # Get the model prediction for the \\nnoise\\n        # Note the use of class_labels\\n        noise_pred = model(\\n            noisy_images,\\n            timesteps,\\n            class_labels=class_labels,\\n            return_dict=False,\\n        )[0]\\n        # Compare the prediction with the \\nactual noise\\n        loss = F.mse_loss(noise_pred, noise)\\n        # Update loss display\\n        inner.set_postfix(loss=f\"\\n{loss.cpu().item():.3f}\")\\n        # Store the loss for later plotting\\n        losses.append(loss.item())\\n        # Backward pass and optimization\\n        loss.backward()\\n        optimizer.step()\\n        optimizer.zero_grad()'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Once the training is complete, we can plot the training loss to\\nsee how the model performed, as shown in Figure 5-3.\\nimport matplotlib.pyplot as plt\\nplt.plot(losses)\\nFigure 5-3. Training loss curve.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Sampling\\nWe now have a model that expects two inputs when making\\npredictions: the image and the class label. We can create\\nsamples by beginning with random noise and then iteratively\\ndenoising, passing in whatever class label we’d like to generate:\\ndef generate_from_class(class_to_generate, \\nn_samples=8):\\n    sample = torch.randn(n_samples, 1, 32, \\n32).to(device)\\n    class_labels = [class_to_generate] * \\nn_samples\\n    class_labels = \\ntorch.tensor(class_labels).to(device)\\n    for _, t in \\ntqdm(enumerate(scheduler.timesteps)):\\n        # Get model prediction\\n        with torch.inference_mode():\\n            noise_pred = model(sample, t, \\nclass_labels=class_labels).sample\\n        # Update sample with step\\n        sample = scheduler.step(noise_pred, \\nt, sample).prev_sample'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='return sample.clip(-1, 1) * 0.5 + 0.5\\nWe can generate some t-shirts (class 0), as shown in Figure 5-4.\\nimages = generate_from_class(0)\\nshow_images(images, nrows=2)\\n1000it [00:13, 75.05it/s]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 5-4. Samples generated by the class-conditioned\\ndiﬀusion model for the class 0.\\nNow, we can generate some sneakers (class 7), as shown in\\nFigure 5-5.\\nimages = generate_from_class(7)\\nshow_images(images, nrows=2)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='1000it [00:13, 76.91it/s]\\nFigure 5-5. Samples generated by the class-conditioned\\ndiﬀusion model for the class 7.\\nOr, ﬁnally, we can generate some boots (class 9), as shown in\\nFigure 5-6.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='images = generate_from_class(9)\\nshow_images(images, nrows=2)\\n1000it [00:13, 76.29it/s]\\nFigure 5-6. Samples generated by the class-conditioned\\ndiﬀusion model for the class 9.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='As you can see, the generated images still contain some noise.\\nThey could get better if we explored the model architecture, do\\nhyperparameter tuning, and trained for longer. In any case, it’s\\namazing that the model learned the shapes of diﬀerent types of\\nclothing and realized that shape 9 looks diﬀerent than shape\\n0 just by sending this information alongside the training data.\\nTo put it slightly diﬀerently, the model is used to seeing the\\nnumber 9 accompanying boots. When we ask it to generate an\\nimage and provide the 9, it responds with a boot.\\nImproving Eﬃciency: Latent\\nDiﬀusion\\nNow that we can train a conditional model, we just need to\\nscale it up and condition it on text instead of class labels,\\nright?… Right? Well, not quite. As image size grows, so does the\\ncomputational power required to work with those images. This\\nis especially pronounced in self-attention, where the amount of\\noperations grows quadratically with the number of inputs. A\\n128x128 image has four times as many pixels as a 64x64 image,\\nrequiring 16 times the memory and computing in a self-\\nattention layer. This is a problem for anyone who’d like to\\ngenerate high-resolution images.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 5-1. Figure 5-7. The Latent Diﬀusion process. Note the VAE encoder and\\ndecoder on the left for translating between pixel and latent space.\\nLatent Diﬀusion tries to mitigate this issue using a separate\\nVariational Auto-Encoder. As we saw in Chapter 2, VAEs can\\ncompress images to a smaller spatial dimension. The rationale\\nis that images tend to contain a large amount of redundant\\ninformation. Given enough training data, a VAE can learn to\\nproduce a much smaller representation of an input image and\\nthen reconstruct the image with high ﬁdelity based on this\\nsmall latent representation. The VAE used in Stable Diﬀusion\\ntakes in 3-channel images and produces a 4-channel latent\\nrepresentation with a reduction factor of 8 for each spatial\\ndimension. A 512x512 input image (3x512x512=786,432 values)\\nwill be compressed down to a 4x64x64 latent (16,384 values).'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='By applying the diﬀusion process on these smaller latent\\nrepresentations rather than on full-resolution images, we can\\nget many of the beneﬁts that would come from using smaller\\nimages (lower memory usage, fewer layers needed in the UNet,\\nfaster generation times, etc.) and still decode the result back to\\na high-resolution image once we’re ready to view it. This\\ninnovation dramatically lowers the cost to train and run these\\nmodels. The paper that introduced this idea, Latent Diﬀusion\\nModels , demonstrated the power of this technique by training\\nmodels conditioned on segmentation maps, class labels, and\\ntext. The impressive results led to further collaboration\\nbetween the authors and partners such as LAION, StabilityAI,\\nRunwayML and EleutherAI to train a more powerful model\\nversion, which became Stable Diﬀusion.\\nStable Diﬀusion: Components in\\nDepth\\nStable Diﬀusion is a Latent Diﬀusion model that can generate\\nimages conditioned on text prompts. It can also be used to do\\nmany other things, such as modifying images, which we’ll learn\\nmore about in Chapter 9.\\n4 \\n5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Thanks to its popularity, hundreds of websites and apps let you\\nuse it to create images with no technical knowledge required.\\nIt’s also very well-supported by libraries like diﬀusers, which let\\nus sample an image with SD using a user-friendly pipeline as\\nwe did in the book introduction. In this case, we’ll use a model\\ncreated by a community contributor, ﬁne-tuned from Stable\\nDiﬀusion 1.5 using the techniques we’ll cover in Chapter 7. .\\nfrom diffusers import StableDiffusionPipeline\\npipe = \\nStableDiffusionPipeline.from_pretrained(\\n    \"Lykon/dreamshaper-8\",\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\",\\n).to(device)\\npipe(\"Watercolor illustration of a \\nrose\").images[0]\\n  0%|          | 0/50 [00:00<?, ?it/s]\\n6'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 5-2. Figure 5-8. An image generated with DreamShaper 8.\\nThis section will explore all the components that make this\\npossible.\\nNOTE\\nJust as diﬀusers has StableDiffusionPipeline, it also provides access to dozens\\nof other models from diﬀerent families (Würstchen, AuraFlow, Flux, etc.).\\nAdditionally, it oﬀers similar pipelines for other tasks, such as inpainting\\n(StableDiffusionInpaint) and super-resolution\\n(StableDiffusionLatentUpscale). The Creative Applications Chapter will further\\nexplore some of these models and pipelines. We’ll ﬁrst focus on understanding the\\nwhole SD pipeline.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The Text Encoder\\nSo, how does Stable Diﬀusion understand text? Earlier on, we\\nexplained how feeding additional information to the UNet\\nallows us to have some control over the types of images\\ngenerated. Given a noisy version of an image, the model is\\ntasked with predicting the denoised version based on additional\\nclues such as a class label. In the case of SD, the additional clue\\nis the text prompt. At inference time, we feed in the description\\nof an image we’d like to generate and some pure noise as a\\nstarting point, and the model does its best to denoise the\\nrandom input into something that matches the caption.\\nFor this to work, we need to create a numeric representation of\\nthe text that captures relevant information about what it\\ndescribes. To accomplish this, we’ll use a text encoder which\\nturns an input string into text embeddings, which are then fed\\ninto the UNet along with the timestep and the noisy latents, as\\nshown in Figure 5-9.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='To do this, SD leverages a pre-trained transformer model based\\non CLIP, introduced in Chapter 3. The text encoder is a\\ntransformer model that takes in a sequence of tokens and\\nproduces a vector for each token. In the case of the ﬁrst\\nversions of Stable Diﬀusion (SD 1 to 1.5), where they used the\\noriginal CLIP from OpenAI, the text encoder maps to a 768-\\ndimensional vector. As the original dataset of CLIP is unknown,\\nthe community trained an open-source version called\\nOpenCLIP. Stable Diﬀusion 2 uses the text encoder from\\nOpenCLIP, which generates 1024-dimensional vectors for each\\ntoken.\\nInstead of combining the vectors of all tokens into a single\\nrepresentation, we keep them separate and use them as\\nconditioning for the UNet. This allows the UNet to use the\\ninformation in each token separately rather than just the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='overall meaning of the prompt. Because we’re extracting these\\ntext embeddings from the internal representation of the CLIP\\nmodel, they are often called the encoder hidden states.Let’s dive\\ndeeper into how the text encoder works under the hood. This is\\nthe same process as the encoder models we discussed in\\nChapter 2.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 5-3. Figure 5-10. The text encoding process transforms the input prompt into a\\nset of text embeddings (the encoder_hidden_states) which can then be fed in as\\nconditioning to the UNet\\nThe ﬁrst step to encode text is to perform tokenization, which\\nconverts a sequence of characters into a sequence of numbers,\\nas we learned in the transformers chapter. In the following\\nexample, we see how the tokenization of a phrase works with'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Stable Diﬀusion’s tokenizer. Each token in the prompt is\\nassigned a unique token number (for example, \"photograph\"\\nhappens to be 8853 in the tokenizer’s vocabulary). There are\\nalso special tokens that provide additional context, such as\\nwhere the sentence ends.\\nprompt = \"A photograph of a puppy\"\\n# Turn the text into a sequence of tokens:\\ntext_input = pipe.tokenizer(\\n    prompt,\\n    return_tensors=\"pt\",\\n)\\n# Output each token and its corresponding ID\\nfor t in text_input[\"input_ids\"][0]:\\n    print(t, \\npipe.tokenizer.decoder.get(int(t)))\\ntensor(49406) <|startoftext|>\\ntensor(320) a</w>\\ntensor(8853) photograph</w>\\ntensor(539) of</w>\\ntensor(320) a</w>\\ntensor(6829) puppy</w>'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='tensor(49407) <|endoftext|>\\ntensor(49407) <|endoftext|>\\nOnce the text is tokenized, we can pass it through the text\\nencoder to get the ﬁnal text embeddings that will be fed into the\\nUNet:\\ntext_embeddings = \\npipe.text_encoder(text_input.input_ids.to(device)\\n[0]\\nprint(\"Text embeddings shape:\", \\ntext_embeddings.shape)\\nText embeddings shape: torch.Size([1, 77, \\n768])\\nThe Variational AutoEncoder\\nThe VAE is tasked with compressing images into a smaller latent\\nrepresentation and reconstructing them again. The VAE, shown\\nin Figure 5-11, is a crucial component of the Stable Diﬀusion\\nmodel and is truly impressive. We won’t go into the training\\ndetails here, but in addition to the usual reconstruction loss and\\nKL divergence described in Chapter 3, the VAE uses an'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='additional patch-based discriminator loss to help the model\\ngenerate plausible details and textures. This helps avoid the\\nslightly blurry outputs typical in previous VAEs. Like the text\\nencoder, the VAE is usually trained separately and used as a\\nfrozen component during the diﬀusion model training and\\nsampling process.\\nFigure 5-4. Figure 5-11. The VAE architecture\\nLet’s load an image, in Figure 5-11, and see what it looks like\\nafter being compressed and decompressed by the VAE. First,\\nlet’s check out the original image:\\nfrom genaibook.core import load_image, \\nshow_image, SampleURL\\nim = load_image(\\n    SampleURL.LlamaExample\\n    size=(512, 512),'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=')\\nshow_image(im);\\nFigure 5-12. An example input image.\\nNow, let’s pass the image through the VAE.\\nwith torch.inference_mode():\\n    # Process image\\n    tensor_im = transforms.ToTensor()\\n(im).unsqueeze(0).to(device) * 2 - 1  \\n    tensor_im = tensor_im.half()'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='# Encode the image\\n    latent = pipe.vae.encode(tensor_im)  \\n    # Sample from the latent distribution\\n    latents = latent.latent_dist.sample()  \\n    latents = latents * 0.18215  \\nlatents.shape\\nThe image is transformed to match the VAE’s input\\nexpectations. We make it a tensor and add a dimension with\\nunsqueeze(0) to make it a batch of size 1. Finally, we\\nnormalize the image to the range [-1, 1] to match the VAE’s\\ninput range.\\nThe image is converted from +float32+ to +float16+,\\nbecause we loaded the pipeline with +torch.float16+\\nprecision.\\nThe image is passed through the VAE’s encoder. As discussed\\nin Chapter 3, VAEs output a distribution from which we can\\nsample.\\nWe can access the distribution object produced by the VAE\\nencoder to generate a sample from it.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Scale the latent vector by a ﬁxed size of 0.18215. SD authors\\nintroduced this scaling factor so that the latent space has\\napproximately unit variance, like the noise added during the\\ndiﬀusion process. It can be accessed in\\nvae.config.scaling_factor.\\ntorch.Size([1, 4, 64, 64])\\nThe original image as a 3-channel image of size 512x512\\n(786432 values). The VAE compresses this image into a 4-\\nchannel latent representation of size 64x64 (16384 values). We\\ncan plot each of the channels in the latent representation, as\\nshown in Figure 5-13.\\nshow_images(\\n    [l for l in latents[0]],\\n    titles=[f\"Channel {i}\" for i in \\nrange(latents.shape[1])],\\n    ncols=4,\\n)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 5-13. The image in diﬀerent channels of the latent\\nrepresentation.\\nNow that we’ve encoded the image into a latent representation,\\nwe can decode it back into an image. In an ideal world, the\\ndecoded image would be identical to the original image. In\\npractice, the VAE can potentially introduce some noise and\\nartifacts. Let’s see how the decoded image looks in Figure 5-14.\\nwith torch.inference_mode():\\n    image = pipe.vae.decode(latents / \\n0.18215).sample\\nimage = (image / 2 + 0.5).clamp(0, 1)\\nshow_image(image[0].float())'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 5-14. The reconstructed image.\\nWhen generating images from scratch, we create a random set\\nof latents as the starting point. We iteratively reﬁne these noisy\\nlatents to generate a sample, and then the VAE decoder is used\\nto decode these ﬁnal latents into an image we can view. The\\nencoder is only used if we’d like to start the process from an\\nexisting image, something we’ll explore in Chapter 8.\\nThe UNet\\nThe UNet used in Stable Diﬀusion is similar to the one in\\nChapter 4 for generating images. Instead of taking in a 3-'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='channel image as the input, we take in a 4-channel latent. The\\ntimestep embedding is fed the same way as the class\\nconditioning was in the example at the start of this chapter. But\\nthis UNet also needs to accept the text embeddings as additional\\nconditioning. Scattered throughout the UNet are cross-attention\\nlayers. Each spatial location in the UNet can attend to diﬀerent\\ntokens in the text conditioning, bringing in relevant\\ninformation from the prompt. Figure 5-15 shows how this text\\nconditioning (as well as the timestep-based conditioning) is fed\\nin at diﬀerent points.\\nFigure 5-5. Figure 5-15. Conditioned UNet architecture. At the left, you can ﬁnd the\\nmodel inputs: the noisy x, which would be the 4-channel latent in this case. The\\ntimestep and prompt are also fed to the model (in the form of embeddings) at\\ndiﬀerent points of the architecture. As with all previous UNets, the model has a series\\nof layers that downsample the input and then upsample it back to the original size.\\nThe network also has skip connections that allow the model to access information\\nfrom earlier blocks.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The UNet for Stable Diﬀusion versions 1 and 2 has around 860\\nmillion parameters. The UNet in the more recent Stable\\nDiﬀusion XL (SDXL)  has even more, at about 2.6 billion, and it\\nuses additional conditioning information.\\nStable Diﬀusion XL, 3 and FLUX\\nDuring the Summer of 2023, a new and better version of Stable\\nDiﬀusion was released: Stable Diﬀusion XL. It uses the same\\nprinciples described in this chapter, with various improvements\\nacross all system components. Some of the most exciting\\nchanges include:\\nA larger text encoder to capture better prompt\\nrepresentations. It uses the output from two text encoders\\nand concatenates the embeddings.\\nCondition on everything. In addition to the timestep (that\\ncarries information about the amount of noise) and the text\\nembeddings, SDXL uses the following additional\\nconditioning signals:\\nOriginal image size. Instead of discarding small images\\nin the training set (they account for almost 40% of the\\ntotal training data used to train SDXL), small images\\nare upscaled and used during training. However, the\\nmodel also receives information about the image sizes\\n7'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='it’s receiving. This way, it learns that upscaling artifacts\\nare not supposed to be part of large images and is\\nencouraged to produce better quality during inference.\\nCropping coordinates. Input images are usually\\nrandomly cropped during training because all the\\nimages in a batch must have the same size. Random\\ncrops may produce undesired eﬀects, such as cutting\\nsubject heads or completely removing subjects from\\nthe image, even though they may be described in the\\ntext prompt. After the model is trained, if we request\\nan uncropped image (by setting the crop coordinates to\\n(0, 0)), the model is more likely to produce subjects\\ncentered in the frame.\\nTarget aspect ratio. After initial pre-training on square\\nimages, SDXL was ﬁne-tuned on various aspect ratios,\\nand the information about the original aspect ratio was\\nused as another conditioning signal. As in the other\\nconditioning cases, this enables the generation of much\\nmore realistic landscape and portrait images with\\nfewer artifacts than before.\\nLarger resolution. SDXL is designed to produce images with\\na resolution of 1024x1024 pixels (or non-square images\\nwith a total number of pixels of approximately 10242). Like'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='the aspect ratio, this feature was achieved during a ﬁne-\\ntuning phase.\\nThe UNet is about three times as large. The cross-attention\\ncontext is larger to account for the increase in the amount\\nof conditioning.\\nImproved VAE. It uses the same architecture as the original\\nStable Diﬀusion, but it’s trained on a larger batch size and\\nuses the EMA (exponential moving average) technique to\\nupdate the weights.\\nReﬁner model. In addition to the base model, SDXL includes\\nan additional reﬁner model that works on the same latent\\nspace as the base model. However, this model was trained\\non high-quality images only during the ﬁrst 20% of the\\nnoise schedule. This means it knows how to take an image\\nwith a small amount of noise and create high-quality\\ntextures and details.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 5-6. Figure 5-16. Images generated with SDXL\\nOther researchers and the open-source community had already\\nexplored many of these techniques, thanks to the original Stable\\nDiﬀusion being open-sourced. SDXL combines many of these\\nideas to achieve an impressive improvement in image quality,\\nwith the cost of running the model being slower and using\\nmore memory.\\nFLUX, SD3 and Video\\nThe development in the diﬀusion models space doesn’t show a\\nsign of slowdown. In June 2024, Stability AI released Stable\\nDiﬀusion 3, and in August 2024, Black Forest Labs released the\\nFlux family of models. The principles behind these models are'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='the same as what we learned in this chapter. However, they\\ncontain a diﬀerent type of scheduler (rectiﬁed ﬂow mathcing\\nschedulers) and a diﬀerent architecture: a diﬀusion\\ntransformer instead of the UNet. Check out the Scaling Rectiﬁed\\nFlow Transformers for High-Resolution Image Synthesis paper\\nfor more information.\\nBoth model families are available in various sizes and support\\ndiﬀerent resolutions, and they have improved prompt\\nunderstanding and text rendering capabilities. The same\\narchitecture is also scalable to support video generation.\\nCogVideoX is a family of models that apply those ideas to the\\nvideo generation space. These models demonstrate that the\\nprinciples we covered (conditioning, in particular) are great\\ngeneral tools to guide the behavior of generative models and\\nthat open-source releases can make exploration faster.\\nClassiﬁer-Free Guidance\\nDespite all the eﬀorts to make the text conditioning as helpful\\nas possible, the model still tends to default to relying primarily\\non the noisy input image rather than the prompt when making\\nits predictions. In a way, this makes sense - many captions are\\nonly loosely related to their associated images, so the model\\nlearns not to rely too heavily on the descriptions. However, this'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='is undesirable when generating new images - if the model\\ndoesn’t follow the prompt, we may get images that don’t relate\\nto our description.\\nTo mitigate this, we introduce guidance. Guidance is any\\nmethod that provides more control over the sampling process.\\nOne option we could apply is to modify the loss function to\\nfavor a speciﬁc direction. For example, if we wanted to bias the\\ngenerations towards a particular color, we could change the loss\\nfunction to measure how far we are, on average, from the\\ntarget color. Another alternative is to use models, such as CLIP\\nor a classiﬁer, to evaluate the result and include their loss signal\\nas part of the generation process. For example, using CLIP, we\\ncould compare the diﬀerence between the prompt text and the\\ngenerated image embeddings and guide the diﬀusion process to\\nminimize this diﬀerence. The exercise section will show how to\\nuse this technique.\\nAnother alternative is to use a trick called classiﬁer-free\\nguidance (CFG), which combines the generations of conditional\\nand unconditional diﬀusion models. During training, text\\nconditioning is sometimes kept blank, forcing the model to\\nlearn to denoise images with no text information whatsoever\\n(unconditional generation). Then, we make two predictions at\\ninference time: one with the text prompt as conditioning and'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='one without. We can then use the diﬀerence between these two\\npredictions to create a ﬁnal combined prediction that pushes\\neven further in the direction indicated by the text-conditioned\\nprediction according to some scaling factor (the guidance scale),\\nhopefully resulting in an image that better matches the prompt.\\nTo incorporate the guidance, we can modify the noise\\nprediction by doing something like noise_pred =\\nnoise_pred_uncond + guidance_scale *\\n(noise_pred_text - noise_pred_uncond). This small\\nchange works surprisingly well and allows us to have much\\nbetter control of the generations. We’ll dive into the\\nimplementation details later in the chapter, but let’s take a look\\nat how to use it, and check out the results in Figure 5-17.\\nimages = []\\nprompt = \"An oil painting of a collie in a \\ntop hat\"\\nfor guidance_scale in [1, 2, 4, 12]:\\n    torch.manual_seed(0)\\n    image = pipe(prompt, \\nguidance_scale=guidance_scale).images[0]\\n    images.append(image)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from genaibook.core import image_grid\\nimage_grid(images, 1, 5)\\nFigure 5-17. Images generated from the prompt \"An oil\\npainting of a collie in a top hat\" with CFG scale 1, 2,\\n4 and 12 (left to right)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='As you can see, higher values result in images that better match\\nthe description, but going too high may start to oversaturate the\\nimage. Google’s Imagen paper found that this was because the\\nin-progress predictions were exceeding the [-1, 1] bounds\\nthe model was trained with. Because the diﬀusion model runs\\niteratively, it has to deal with inputs whose values had not been\\nseen during training, which can result in excessive saturation,\\nposterization and even generation failures. They proposed a\\nmethod called dynamic thresholding to contain values within\\nrange and greatly improve quality at high CFG scales. .\\nPutting it All Together: Annotated\\nSampling Loop\\nNow that we know what each component does, let’s combine\\nthem to generate an image without relying on the pipeline.\\nHere are the settings we’ll use:\\n# Some settings\\nprompt = [\\n    \"Acrylic palette knife painting of a \\nflower\"\\n]  # What we want to generate\\nheight = 512  # default height of SD\\n8'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='width = 512  # default width of SD\\nnum_inference_steps = 30  # Number of \\ndenoising steps\\nguidance_scale = 7.5  # Scale for classifier-\\nfree guidance\\nseed = 42  # Seed for random number generator\\nThe ﬁrst step is to encode the text prompt. Because we plan to\\ndo classiﬁer-free guidance, we’ll create two sets of text\\nembeddings: one with the prompt embedding and one\\nrepresenting an empty string, which is the unconditional input.\\nAlthough we’ll go with unconditional input here, this setup\\nprovides lots of ﬂexibility. For example, we can:\\nencode a negative prompt instead of the empty string.\\nAdding a negative prompt allows us to guide the model in\\navoiding going in a certain direction. In Exercise 6 of this\\nchapter, you’ll play with negative prompts\\ncombine multiple prompts with diﬀerent weights. Prompt\\nweighting allows us to emphasize or de-emphasize certain\\nparts of a prompt. We’ll learn more about prompt\\nweighting in Chapter 8.\\n# Tokenize the input\\ntext_input = pipe.tokenizer(\\n    prompt,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='padding=\"max_length\",  # Pad to max \\nlength to ensure both inputs have the same \\nshape\\n    return_tensors=\"pt\",\\n)\\n# Do the same for the unconditional input (a \\nblank string)\\nuncond_input = pipe.tokenizer(\\n    \"\",\\n    padding=\"max_length\",\\n    return_tensors=\"pt\",\\n)\\n# Feed both embeddings through the text \\nencoder\\nwith torch.inference_mode():\\n    text_embeddings = \\npipe.text_encoder(text_input.input_ids.to(device)\\n[0]\\n    uncond_embeddings = \\npipe.text_encoder(uncond_input.input_ids.to(devic\\n[0]\\n# Concatenate the two sets of text embeddings \\nembeddings\\ntext_embeddings ='),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='torch.cat([uncond_embeddings, \\ntext_embeddings])\\nNext, we create our random initial latents and set up the\\nscheduler to use the desired number of inference steps:\\n# Prepare the scheduler\\npipe.scheduler.set_timesteps(num_inference_steps)\\n# Prepare the random starting latents\\nlatents = (\\n    torch.randn(\\n        (1, pipe.unet.config.in_channels, \\nheight // 8, width // 8),\\n    )\\n    .to(device)\\n    .half()\\n)\\nlatents = latents * \\npipe.scheduler.init_noise_sigma\\nNow we loop through the sampling steps, getting the model\\nprediction at each stage and using this to update the latents:'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='for t in pipe.scheduler.timesteps:\\n    # Create two copies of the latents to \\nmatch the two\\n    # text embeddings (unconditional and \\nconditional)\\n    latent_input = torch.cat([latents] * 2)\\n    latent_input = \\npipe.scheduler.scale_model_input(latent_input, \\nt)\\n    # Predict noise residuals for both \\nunconditional and conditional latents\\n    with torch.inference_mode():\\n        noise_pred = pipe.unet(\\n            latent_input, t, \\nencoder_hidden_states=text_embeddings\\n        ).sample\\n    # Split the prediction into unconditional \\nand conditional versions\\n    noise_pred_uncond, noise_pred_text = \\nnoise_pred.chunk(2)\\n    # Perform classifier-free guidance\\n    noise_pred = noise_pred_uncond + \\nguidance_scale * (\\n        noise_pred_text - noise_pred_uncond\\n    )'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='# Update latents for the next timestep\\n    latents = pipe.scheduler.step(noise_pred, \\nt, latents).prev_sample\\nNotice the classiﬁer-free guidance step. Our ﬁnal noise\\nprediction is noise_pred_uncond + guidance_scale *\\n(noise_pred_text - noise_pred_uncond), pushing the\\nprediction away from the unconditional prediction towards the\\nprediction based on the prompt. Try changing the guidance\\nscale to explore how this aﬀects the output.\\nBy the end of the loop, the latents should represent a plausible\\nimage that matches the prompt. The ﬁnal step is to decode the\\nlatents into an image using the VAE so that we can see the\\nresult, shown in\\n# Scale and decode the image latents with the \\nVAE\\nlatents = 1 / pipe.vae.config.scaling_factor \\n* latents\\nwith torch.inference_mode():\\n    image = pipe.vae.decode(latents).sample\\nimage = (image / 2 + 0.5).clamp(0, 1)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='show_image(image[0].float());\\nFigure 5-18. The image generated from the prompt \"Acrylic\\npalette knife painting of a flower\" with our\\nannotated sampling loop.\\nIf you explore the source code for the\\nStableDiffusionPipeline, you’ll notice that the code above\\nclosely matches the call() method used by the pipeline.\\nHopefully, this annotated version shows that nothing too\\nmagical is happening behind the scenes. Use this as a reference'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='when we encounter additional pipelines that add tricks to this\\nfoundation.\\nOpen Data, Open Models\\nThe LAION-5B dataset  was a dataset comprised of over 5\\nbillion image URLs and their respective associated captions\\n(image-caption pairs). The dataset was created by ﬁrst taking all\\nimage URLs found in CommonCrawl (an open repository of\\nweb-crawled data, similar to how Google indexes the internet\\nfor its search) and then using CLIP to keep only the image-\\ncaption pairs with high similarity between text and image.\\nThis dataset was created by and for the open ML community,\\nwhich saw the need for an open-access dataset of this kind.\\nBefore the LAION initiative, only a handful of research labs at\\nlarge companies had access to image-text pair datasets. These\\norganizations kept their datasets’ details to themselves, making\\ntheir results impossible to validate or replicate. By creating a\\npublicly available source of URL and caption indexes, LAION\\nenabled a wave of smaller communities and organizations to\\ntrain models and perform research that would otherwise have\\nbeen impossible.\\n9'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The ﬁrst latent diﬀusion was one such model, trained on a\\nprevious version of the LAION dataset with 400M image-text\\npairs by CompVis . The release of the LAION-trained latent\\ndiﬀusion model marked the ﬁrst time a robust text-to-image\\nmodel was available for all the research community.\\nThe success of latent diﬀusion showed the potential of this\\napproach, which was realized by the follow-up work, Stable\\nDiﬀusion. Training a model like SD required a signiﬁcant\\namount of GPU time. Even leveraging the freely available\\nLAION dataset, only a few could aﬀord the GPU-hours\\ninvestment. This is why the public release of the model weights\\nand code was such a big deal - it marked the ﬁrst time a\\npowerful text-to-image model with similar capabilities to the\\nbest closed-source alternatives was available to all.\\nStable Diﬀusion’s public availability has made it the go-to\\nchoice for researchers and developers exploring this technology\\nover the past years. Hundreds of papers build upon the base\\nmodel, adding new capabilities or ﬁnding innovative ways to\\nimprove its speed and quality. Apart from research papers, a\\ndiverse community not necessarily from a Machine Learning\\nbackground has been hacking with the models to enable new\\ncreative workﬂows, optimize for faster inference, and so much\\nmore. Innumerable startups have found ways to integrate these\\n1 0'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='rapidly improving tools into their products, spawning an entire\\necosystem of new applications.\\nFigure 5-19. An explosion of creativity in the text-to-image\\nspace.\\nThe months after the introduction of Stable Diﬀusion\\ndemonstrated the impact of sharing these technologies in the\\nopen, with many further quality improvements and\\ncustomization techniques that we will explore in chapters 7 and\\n8. SD was competitive in quality with the commercial\\nalternatives of the time, such as DALL-E and MidJourney, and\\nthousands of people have spent their time making it better and\\nbuilding upon that open foundation. We hope this example'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='encourages others to follow suit and share their work with the\\nopen-source community in the future.\\nNOTE\\nApart from being used to train Stable Diﬀusion, LAION-5B has been used by many\\nother research eﬀorts. One example is OpenCLIP, an eﬀort from the LAION\\ncommunity to train high-quality (state-of-the-art) open-source CLIP models and\\nreplicate similar quality to the original one. A high-quality open-source CLIP model\\nbeneﬁts many tasks, such as image retrieval and zero-shot image classiﬁcation.\\nHaving transparency in the data used to train the model also enables researching the\\nimpact of scaling up the models, correctly reproducing results, and making research\\nmore accessible.\\nChallenges and the Sunset of LAION 5B\\nHowever, the huge success of text-to-image generative models\\nand downstream commercial applications based on such\\nmodels have raised concerns about the data source and content\\nin those datasets.\\nBecause the dataset comprises links to images crawled from the\\ninternet, it contains millions of URLs pointing to images that\\nmay contain copyrighted material, such as photographs, works\\nof art, comics, illustrations, etc. Research has also found that\\nsuch dataset also includes private sensitive information, such as'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='personally identiﬁable medical imagery, that was publicly\\navailable online .\\nUsing such a dataset to train generative AI models can also\\ninject the model with the capability of producing content that\\nreinforces or exacerbates societal biases  and be used to\\nproduce explicit adult content. Additionally, these models can\\nfrequently produce content that is very close to the training\\ndata, which can lead to the generation of content extremley\\nsimilar to copyrighted material. However, those open models\\nare trained on open datasets, so such biases and problematic\\ncontent can be studied, analyzed, and mitigated.\\nMore recent research  works showcase that the LAION 2B\\ndataset, having been scraped from the internet, failed to ﬁlter\\nout explicitly illegal material regarding violence and child\\nsafety, which led to the dataset deactivation.\\nAlternatives\\nWith the deactivation of LAION datasets on the grounds of child\\nsafety, open alternatives such as COYO-700M and DataComp 1B\\nﬁll in the void as open datasets that follow a similar formula of\\ninternet-level scrapping of images; but while they don’t contain\\ncontent that brought to its immediate deactivation, it still\\n1 1 \\n1 2 \\n1 3 \\n1 4'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='contains the same challenges of biases, copyrighted material\\nand personality rights that were brought up just above.\\nCommonCanvas is a smaller scale (70M image-text pairs)\\ndataset but contains exclusively openly Creative Commons\\nlicensed images.\\nFair and Commercial Use\\nWhile some countries have fair-use exceptions regarding\\ncopyright law for research usage, and others have precedents\\nthat seem favorable regarding using scraped data to train\\nMachine Learning models, what happens when a research\\nmodel trained on such materials is used commercially and at\\nscale for generative AI? This complex subject is currently being\\nlitigated in courts in diﬀerent jurisdictions in the United States\\nand Europe, with angles that relate to copyright law, fair use for\\nresearch applications, privacy, the economic impact of AI tools\\non creative jobs, and others. We don’t claim to have an answer\\nfor such complex matters, but such a legal gray area is moving\\nthe research and open-source community away from using\\nopen datasets; for Stable Diﬀusion XL, the dataset used to train\\nit was not disclosed, despite the open-source model weights.\\nThe construction of a new large-scale text-image dataset that\\nputs consent, safety, and licensing in the center stage would also'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='be an excellent resource for the research and open-source\\ncommunities and legal certainty for commercial downstream\\napplications. The CommonCanvas datasets show a path in this\\ndirection.\\nProject Time: Build an interactive ML\\ndemo with Gradio\\nUntil now, we’ve focused on running transformer and diﬀusion\\nmodels using open-source libraries. This gives us lots of\\nﬂexibility and control over the models but also requires much\\nwork to set up and run. The reality is that most people don’t\\nknow how to code but might be interested in exploring models\\nand their capabilities.\\nIn this project, we’ll build a simple ML demo that allows users\\nto generate images from text prompts using Stable Diﬀusion.\\nDemos allow you to easily showcase a model to a broad\\naudience and make your work and research more accessible.\\nThere are many ways to build ML demos. You could use HTML,\\nJavaScript, and CSS. However, this requires some web\\ndevelopment experience and is not a straightforward process.\\nAlternatively, there are open-source libraries such as streamlit'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='and gradio that make it easy to build interactive ML demos\\nusing Python. We’ll use gradio in this chapter, which is a very\\nsimple and minimal library.\\ngradio can be run anywhere - in a Python IDE, Jupyter\\nnotebook, Google Colab, or a cloud environment such as\\nHugging Face Spaces. The easiest way to build gradio demos is\\nusing its Interface class, which has three key aspects:\\ninputs: the expected input types of the demo, such as text\\nprompts or images\\noutputs: the expected output types of the demo, such as\\ngenerated images\\nfn: the function that will be called when the user interacts\\nwith the demo. This is where the magic happens. You can\\nrun any code here, including running models with\\ntransformers or diﬀusers.\\nLet’s look at an example:\\nimport gradio as gr\\ndef greet(name):\\n    return \"Hello \" + name'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='demo = gr.Interface(fn=greet, inputs=\"text\", \\noutputs=\"text\")\\ndemo.launch()\\nFigure 5-7. Figure 5-20. Example Gradio application\\nNow it’s your turn! Build a simple demo that allows users to\\ngenerate images from text prompts using Stable Diﬀusion. You\\ncan use the code from the previous section as a starting point.\\nOnce you get a demo running, we suggest adding more features\\nto make it interactive and fun. For example, you could:\\nadd a slider to control the guidance scale\\nadd an additional text ﬁeld to add a negative prompt\\nadd a title and a description so users understand what the\\ndemo is about'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='If you need help, remember to look at the oﬃcial\\ndocumentation and the quick start.\\nSummary\\nThis chapter showed how conditioning gives us new ways to\\ncontrol the images generated by diﬀusion models. We’ve seen\\nhow a text encoder can condition a diﬀusion model on a text\\nprompt, enabling powerful text-to-image capabilities. And\\nwe’ve explored how all of this comes together in the Stable\\nDiﬀusion model by digging into the sampling loop and seeing\\nhow the diﬀerent components work together.\\nIn Chapter 7 of the book, you’ll learn how to ﬁne-tune Stable\\nDiﬀusion to add new knowledge or capabilities to the model.\\nFor example, we’ll see how, by showing pictures of your pet,\\nStable Diﬀusion can learn the concept of \"your pet\" and\\ngenerate novel images in new scenarios, such as \"your pet on\\nthe moon“.\\nLater, in Chapter 8, we’ll show some of the capabilities we can\\nadd to diﬀusion models to take them beyond simple image\\ngeneration. For example, we’ll explore inpainting, which allows'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='us to mask a part of the image and then ﬁll that part. Chapter 8\\nalso explores techniques to edit images based on a prompt.\\nExercises\\n1. How does the training process of a class-conditioned\\ndiﬀusion model diﬀer from a non-conditioned model,\\nparticularly in terms of the input data and the loss function\\nused?\\n2. How does the timestep embedding inﬂuence the quality\\nand evolution of the images during the diﬀusion process?\\n3. Explain the diﬀerence between latent diﬀusion and normal\\ndiﬀusion. What are the trade-oﬀs of using latent diﬀusion?\\n4. How is the text prompt incorporated into the model?\\n5. What is the diﬀerence between model-based and classiﬁer-\\nfree guidance? What is the beneﬁt of classiﬁer-free\\nguidance?\\n6. What is the eﬀect of using a negative prompt? Experiment\\nwith it using pipe(…, negative_prompt=““). How are\\nyou able to guide the image generation using Stable\\nDiﬀusion?\\n7. Let’s say you want to remove white hats from any\\ngenerated image. How can you use negative prompts for\\nthis? First try implementing this using the high-level'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='pipeline. Then, try adapting the end-to-end inference\\nexample (hint: it only requires modifying the random part\\nof the classiﬁer-free conditioning).\\n8. What happens in SDXL if you use (256, 256) instead of\\n(1024, 1024) as the \"original size\" conditioning\\nsignal? What happens if you use crop coordinates other\\nthan (0, 0)? Can you explain why?\\nYou can ﬁnd the solutions to these exercises in the GitHub\\nrepository of the book.\\nChallenges\\n9. Blue Guidance. Let’s say we want to bias generated images\\nto a speciﬁc color, such as blue. How can we do that? The\\nﬁrst step is to deﬁne a conditioning function we’d like to\\nminimize, which, in this case, will be a color loss.\\ndef color_loss(images, target_color=(0.1, \\n0.5, 0.9)):\\n    \"\"\"Given a target color (R, G, B) return \\na loss for how far away on average\\n    the images\\' pixels are from that \\ncolor.\"\"\"\\n    # Map target color to (-1, 1)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='target = \\ntorch.tensor(target_color).to(images.device) \\n* 2 - 1\\n    # Get shape right to work with the images \\n(b, c, h, w)\\n    target = target[None, :, None, None]\\n    # Mean absolute difference between the \\nimage pixels and the target color\\n    error = torch.abs(images - target).mean()\\n    return error\\nGiven this loss function, write a sampling loop (no training is\\nneeded) that modiﬁes x in the direction of the loss function.\\nReferences\\n1. Esser, Patrick, et al.\\xa0Scaling Rectiﬁed Flow Transformers for\\nHigh-Resolution Image Synthesis. arXiv, 2024. arXiv.org,\\nhttps://arxiv.org/abs/2403.03206\\n2. Ho, Jonathan, and Tim Salimans. _Classiﬁer-Free Diﬀusion\\nGuidanc_e. arXiv, 25 July 2022. arXiv.org,\\nhttp://arxiv.org/abs/2207.12598\\n1 5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='3. Luccioni, Alexandra Sasha, et al.\\xa0Stable Bias: Analyzing\\nSocietal Representations in Diﬀusion Models. arXiv, 20\\nMar.\\xa02023. arXiv.org, http://arxiv.org/abs/2303.11408\\n4. Peebles, William, and Saining Xie. Scalable Diﬀusion Models\\nwith Transformers. arXiv, 2023. arXiv.org,\\nhttps://arxiv.org/abs/2212.09748\\n5. Podell, Dustin, et al.\\xa0SDXL: Improving Latent Diﬀusion\\nModels for High-Resolution Image Synthesis. arXiv, 4 July\\n2023. arXiv.org, http://arxiv.org/abs/2307.01952\\n6. Rombach, Robin, et al.\\xa0High-Resolution Image Synthesis with\\nLatent Diﬀusion Models. arXiv, 13 Apr.\\xa02022. arXiv.org,\\nhttp://arxiv.org/abs/2112.10752\\n7. Saharia, Chitwan, et al.\\xa0Photorealistic Text-to-Image\\nDiﬀusion Models with Deep Language Understanding.\\nAdvances in Neural Information Processing Systems 35\\n(2022): 36479-36494. arXiv https://arxiv.org/abs/2205.11487.\\n8. Schramowski, Patrick, et al.\\xa0Safe Latent Diﬀusion: Mitigating\\nInappropriate Degeneration in Diﬀusion Models. arXiv, 26\\nApr.\\xa02023. arXiv.org, http://arxiv.org/abs/2211.05105\\n9. Schuhmann, Christoph, et al.\\xa0LAION-5B: An Open Large-\\nScale Dataset for Training next Generation Image-Text\\nModels. arXiv, 15 Oct.\\xa02022. arXiv.org,\\nhttp://arxiv.org/abs/2210.08402'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='10. Xiao, Han, et al.\\xa0Fashion-MNIST: A Novel Image Dataset for\\nBenchmarking Machine Learning Algorithms. arXiv, 15\\nSept.\\xa02017. arXiv.org, http://arxiv.org/abs/1708.07747.\\n11. Yang, Zhuoyi, et al.\\xa0CogVideoX: Text-to-Video Diﬀusion\\nModels with An Expert Transformer. arXiv, 2024. arXiv.org,\\nhttps://arxiv.org/abs/2408.06072\\n Here’s a list of the ten categories in Fashion MNIST:\\nhttps://www.kaggle.com/datasets/zalando-research/fashionmnist\\n In the diﬀusion chapter, we resized the butterﬂy images as they were very large\\n(512x283). We resized them to a lower size to speed up training. In this section, our\\nimages are small and don’t require resizing, but we pad them to 32x32 to use\\nmultiples of 2, which usually play better with the cascaded UNet layers.\\n Diﬀerent number of epochs and learning rate, a diﬀerent epsilon for AdamW, usage\\nof tqdm for data loading, loading labels, and passing the labels to the model. The most\\nimportant part, the conditioning, is a 2-line diﬀ.\\n Rombach, Robin, et al.\\xa0High-Resolution Image Synthesis with Latent Diﬀusion Models.\\narXiv, 13 Apr.\\xa02022. arXiv.org, http://arxiv.org/abs/2112.10752\\n LAION and EleutherAI are non-proﬁt organizations focused in open ML. StabilityAI\\nis one of the companies that has pushed the most for open access ML. RunwayML is a\\ncompany building AI-powered tools for creative applications.\\n Stable Diﬀusion 1.5, by RunwayML, is no longer available\\n1 \\n2 \\n3 \\n4 \\n5 \\n6'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Podell, Dustin, et al.\\xa0SDXL: Improving Latent Diﬀusion Models for High-Resolution\\nImage Synthesis. arXiv, 4 July 2023. arXiv.org, http://arxiv.org/abs/2307.01952.\\n Saharia, Chitwan, et al.\\xa0Photorealistic text-to-image diﬀusion models with deep\\nlanguage understanding. Advances in Neural Information Processing Systems 35\\n(2022): 36479-36494, arXiv https://arxiv.org/abs/2205.11487\\n Find the oﬃcial blog post for more info https://laion.ai/blog/laion-5b/\\n At the time, the Computer Vision Group at Heidelberg University. Currently, it is a\\nresearch group at LMU Munich. https://github.com/CompVis\\n An article about this came out in 2022 soon after the release of Stable Diﬀusion -\\nhttps://arstechnica.com/information-technology/2022/09/artist-ﬁnds-private-medical-\\nrecord-photos-in-popular-ai-training-data-set/\\n Luccioni, Alexandra Sasha, et al.\\xa0Stable Bias: Analyzing Societal Representations in\\nDiﬀusion Models. arXiv, 20 Mar.\\xa02023. arXiv.org, http://arxiv.org/abs/2303.11408\\n Schramowski, Patrick, et al.\\xa0Safe Latent Diﬀusion: Mitigating Inappropriate\\nDegeneration in Diﬀusion Models. arXiv, 26 Apr.\\xa02023. arXiv.org,\\nhttp://arxiv.org/abs/2211.05105.\\n Thiel, David, et al.\\xa0Stanford Digital Repository. 21 Dec.\\xa02023. purl.stanford.edu,\\nhttps://purl.stanford.edu/kh752sm9123\\n To simplify things, we recommend using the unconditional DDPMPipeline from\\nChapter 4.\\n7 \\n8 \\n9 \\n 0 \\n 1 \\n 2 \\n 3 \\n 4 \\n 5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Chapter 6. Fine-Tuning Language\\nModels\\nA NOTE FOR EARLY RELEASE READERS\\nWith Early Release ebooks, you get books in their earliest form\\n—the authors’ raw and unedited content as they write—so you\\ncan take advantage of these technologies long before the oﬃcial\\nrelease of these titles.\\nThis will be the sixth chapter of the ﬁnal book. Please note that\\nthe GitHub repo will be made active later on.\\nIf you have comments about how we might improve the content\\nand/or examples in this book, or if you notice missing material\\nwithin this chapter, please reach out to the editor at\\njleonard@oreilly.com.\\nIn Chapter 2, we explored how language models work and how\\nto use them for diﬀerent tasks, such as text generation and\\nsequence classiﬁcation. We saw that language models could be\\nhelpful in many tasks without further training, thanks to\\nproper prompting and the zero-shot capabilities of these\\nmodels. We also explored some of the hundreds of thousands of'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='pre-trained models by the community. In this chapter, we’ll\\ndiscuss how we can improve the performance of language\\nmodels on speciﬁc tasks by ﬁne-tuning them on our data.\\nWhile pre-trained models showcase remarkable capabilities,\\ntheir general-purpose training may not be suited for certain\\ntasks or domains. Fine-tuning is frequently used to tailor the\\nmodel’s understanding to the nuances of their dataset or task.\\nFor instance, in the ﬁeld of medical research, a language model\\npre-trained on general web text will not perform great out of\\nthe box, so we can ﬁne-tune it on a dataset of medical literature\\nto enhance its ability to generate relevant medical text or assist\\nin information extraction from healthcare documents. Another\\nexample is for making conversational models. Although large\\npre-trained models can generate coherent text, they usually\\ndon’t work well for generating high-quality conversational text\\nor following instructions. We can ﬁne-tune this model on a\\ndataset with everyday conversations and informal language\\nstructures, adapting the model to output engaging,\\nconversational text, as the one you would expect in interfaces\\nsuch as ChatGPT.\\nThe goal of this chapter is to build strong foundations in ﬁne-\\ntuning LLMs, and hence, we’ll cover the following:'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Classifying the topic of a text using a ﬁne-tuned encoder\\nmodel\\nUnderstanding the role of encoder-based models in the\\nmodern LLM era\\nGenerating text in a particular style using a decoder model\\nSolving multiple tasks with a single model via instruction\\nﬁne-tuning\\nParameter-eﬃcient ﬁne-tuning techniques that allow us to\\ntrain models with smaller GPUs\\nTechniques that will allow us to run inference of the\\nmodels with less compute\\nClassifying Text\\nBefore jumping into the land of generative models, it’s a good\\nidea to understand the general ﬂow of ﬁne-tuning a pre-trained\\nmodel. We’ll begin with sequence classiﬁcation, where a model\\nassigns a class to a given input. Sequence classiﬁcation is one of\\nthe classical Machine Learning problems. With it, you can\\ntackle challenges such as spam detection, sentiment\\nrecognition, intent classiﬁcation, and fake content detection,\\namong many others. Although it’s a simple task that people\\nfrequently solve via prompting a general-purpose language'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='model, it’s a good starting point to understand the ﬁne-tuning\\nprocess and the steps involved.\\nWe’ll ﬁne-tune a model to classify the topic of short news article\\nabstracts. As we’ll ﬁnd out soon, ﬁne-tuning requires much less\\ncompute and data than training a model from scratch. The\\nusual process is:\\n1. Identify a dataset for the task\\n2. Deﬁne which model type is needed (encoder, decoder, or\\nencoder-decoder)\\n3. Select a good base model that meets your requirements\\n4. Pre-process the dataset\\n5. Deﬁne evaluation metrics\\n6. Train and share'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 6-1. Figure 6-1. The usual steps of a ﬁne-tuning workﬂow.\\nIdentify a Dataset\\nOur goal is to adapt a general-purpose pre-trained language\\nmodel to work as a text classiﬁer, and we need to teach it the\\ncategories it needs to detect. This leads to us needing a labeled\\ndataset for sequence classiﬁcation. Depending on your task and\\nuse case, you can use a public or private dataset (e.g., a dataset\\nfrom your company). Some good places to ﬁnd public datasets\\nare Hugging Face Datasets, Kaggle, Zenodo and Google Dataset\\nSearch. With hundreds of thousands of datasets out there, we\\nneed help ﬁnding a suitable dataset for our use case. One\\napproach can be to ﬁlter for text classiﬁcation datasets on\\nHugging Face.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Among the most downloaded datasets is the AG News dataset, a\\nwell-known non-commercial dataset used for benchmarking\\ntext classiﬁcation models and researching data mining,\\ninformation retrieval, and data streaming.\\nNOTE\\nSometimes, you will want to share a dataset with the community. To do that, you can\\nupload it as a dataset repository. The datasets library has out-of-the-box support for\\ncommon data types (audio, images, text, csv, json, pandas, etc.).\\nThe ﬁrst instinct should be to explore the dataset. As shown\\nbelow, the dataset contains two columns: one with the text and\\none with the label. The dataset provides 120,000 training\\nsamples, more than enough data to ﬁne-tune a model. Fine-\\ntuning requires very little data compared to pre-training a\\nmodel, and just using a few thousand examples should be\\nenough to get a good baseline model.\\nfrom datasets import load_dataset\\nraw_datasets = load_dataset(\"ag_news\")\\nraw_datasets\\n1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='DatasetDict({\\n    train: Dataset({\\n        features: [\\'text\\', \\'label\\'],\\n        num_rows: 120000\\n    })\\n    test: Dataset({\\n        features: [\\'text\\', \\'label\\'],\\n        num_rows: 7600\\n    })\\n})\\nLet’s explore how a speciﬁc example looks like:\\nraw_train_dataset = raw_datasets[\"train\"]\\nraw_train_dataset[0]\\n{\\'label\\': 2,\\n \\'text\\': \\'Wall St. Bears Claw Back Into the \\nBlack (Reuters) Reuters \\'\\n         \"- Short-sellers, Wall Street\\'s \\ndwindling\\\\\\\\band of \"\\n         \\'ultra-cynics, are seeing green \\nagain.\\'}\\nThe ﬁrst sample contains the text and a label, which is… 2? To\\nwhich class does 2 refer? To ﬁgure this out, we can inspect the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"dataset .features and its label ﬁeld.\\nprint(raw_train_dataset.features)\\n{'label': ClassLabel(names=['World',\\n                            'Sports',\\n                            'Business',\\n                            'Sci/Tech'],\\n                     id=None),\\n 'text': Value(dtype='string', id=None)}\\nSo a label of 0 means news about the world, 1 about sports,\\n2 about business, and 3 about science and tech. With this\\nﬁgured out, let’s decide which model to use.\\nDeﬁne Which Model Type to Use\\nLet’s re-cap Chapter 2. We can use three types of transformers\\ndepending on which type of task we’re trying to solve:\\nEncoder Models: They obtain rich semantic\\nrepresentations from sequences. They produce embeddings\\nthat capture the meaning of the input, which can be used\\nfor various tasks relying on the input’s semantic\\ninformation (e.g., identifying entities in the text or\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='classifying the sequence). A small network can be added on\\ntop of these embeddings to train for a speciﬁc downstream\\ntask.\\nDecoder Models: These models are designed to generate\\nnew sequences, such as text. They take an input (often an\\nembedding or context) and produce coherent output\\nsequences, making them ideal for text generation tasks.\\nEncoder-Decoder Models: These models are well-suited\\nfor tasks that require transforming an input sequence into\\na diﬀerent output sequence, such as machine translation or\\nsummarization. The encoder processes the input while the\\ndecoder generates the corresponding output.\\nConsidering the task of topic classiﬁcation for short news article\\nabstracts, we have three possible approaches:\\n1. Zero or Few-Shot Learning: We can use a high-quality pre-\\ntrained model, explain the task (e.g., \"classify into\\nthese four categories“), and let the model do the rest.\\nThis approach does not require any ﬁne-tuning and is very\\ncommon nowadays with powerful pre-trained models—a\\nsingle model can solve many diﬀerent tasks by formulating\\nthem as text generation problems.\\n2. Text Generation Model: Fine-tune a text generation model\\nto generate the label (e.g., \"business“) given an input'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='news article. We could use either a decoder or an encoder-\\ndecoder model here.\\n3. Encoder Model with Classiﬁcation Head: Fine-tune an\\nencoder model by adding a simple classiﬁcation network\\n(called head) to the embeddings. This approach provides a\\nspecialized and eﬃcient model tailored to our use case,\\nmaking it a favorable choice for our topic classiﬁcation\\ntask.\\nBased on the above, we’ll choose the third approach.\\nSelect a Good Base Model\\nOur requirements are a model that:\\nhas an encoder-based architecture.\\nis small enough so we can ﬁne-tune in a few minutes on a\\nGPU.\\nhas solid pre-training results\\ncan process a small number of tokens.\\nBERT, although old, is a great base encoder architecture for ﬁne-\\ntuning. Given that we want to train the model quickly and with\\nlittle computing power, we can use DistilBERT,  which is 40%\\nsmaller and 60% faster while retraining 97% of BERT\\n2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='capabilities. Given the base model, we can ﬁne-tune it for\\nmultiple downstream tasks, such as answering questions or\\nclassifying text.\\nApart from the original BERT and DistilBERT, many other\\nmodels can be used as base models for ﬁne-tuning. We won’t\\ndive into each of them, but knowing they exist is important.\\nSome examples are RoBERTa, ALBERT, Electra, DeBERTa,\\nLongformer, LuKE, MobileBERT, and Reformer. Each model has\\nits own training procedure and builds upon the original BERT\\nmodel. Which one to choose depends on your speciﬁc\\nrequirements, but using DistilBERT is a good starting point\\ngiven our computing requirements. DeBERTa is among the\\nSOTA at the time of writing.\\nPre-Process the Dataset\\nAs explained in Chapter 2, each language model comes with its\\ntokenizer. To ﬁne-tune DistilBERT, we must ensure the whole\\ndataset is tokenized with the same tokenizer that was used to\\npre-train the model. We can use AutoTokenizer to load the\\nappropriate one, and then we can deﬁne a function that will\\ntokenize a batch of samples. transformers expect all inputs in a\\nbatch to be the same length: by adding padding=True, we add'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='zeros to the samples so they all have the same size as the\\nlongest input sample.\\nIt’s important to note that transformers models have a\\nmaximum context size - the maximum number of tokens a\\nlanguage model can use when making predictions. For\\nDistilBERT, this limit is 512 tokens, so don’t try to use it for\\nentire books. Fortunately, most of our samples are small\\nabstracts, but some may still exceed this token limit. To handle\\nthis, we can use truncation=True, which will truncate all\\nsamples to ﬁt within the model’s context length. However, this\\napproach comes with a trade-oﬀ: truncating the text means that\\nsome potentially useful information might be lost.\\nHandling long contexts with transformers is an active research\\narea. For scenarios involving encoder-based models and long\\ncontexts, you can try out several strategies:\\nUse a specialized long-context transformer model, such as\\nLongformer.\\nDivide the text into smaller segments and process them\\nseparately.\\nUse a sliding window approach to process the text in\\nchunks.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Summarize the text as a pre-processing step, then feed the\\nsummary to the model.\\nWhich strategy to pick depends on the task and model. For\\nexample, if you want to analyze the sentiment of a book, use\\nchunking and analyze diﬀerent parts of the book separately. If\\nyou want to classify the topic of a long article, you could\\nsummarize the article and then classify the summary.\\nNOTE\\nAlthough models come with an out-of-the-box context length, which refers to the\\nnumber of tokens the model can consider at a time, there are techniques such as\\nrotary embeddings that allow us to use longer or even inﬁnite context lengths. We’ll\\ndiscuss more about this later on.\\nLet’s tokenize two samples to inspect the output:\\nfrom transformers import AutoTokenizer\\ncheckpoint = \"distilbert-base-uncased\"\\ntokenizer = \\nAutoTokenizer.from_pretrained(checkpoint)\\ndef tokenize_function(batch):\\n    return tokenizer('),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='batch[\"text\"], truncation=True, \\npadding=True, return_tensors=\"pt\"\\n    )\\ntokenize_function(raw_train_dataset[:2])\\n{\\'attention_mask\\': tensor([[1, 1, 1, 1, 1, 1, \\n1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \\n1, 1, 1, 1, 1, 1, 1, 1,\\n         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \\n0],\\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \\n1, 1, 1, 1, 1, 1, 1, 1,\\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \\n1, 1, 1, 1, 1, 1, 1, 1,\\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \\n1]]),\\n \\'input_ids\\': tensor([[  101,  2813,  2358,  \\n1012,  6468, 15020,  2067,  2046,\\n          1996,  2304,  1006, 26665,  1007, \\n26665,  1011,  2460,\\n          1011, 19041,  1010,  2813,  2395,  \\n1005,  1055,  1040,\\n         11101,  2989,  1032,  2316,  1997, \\n11087,  1011, 22330,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='8713,  2015,  1010,  2024,  3773,  \\n2665,  2153,  1012,\\n           102,     0,     0,     0,     0,     \\n0,     0,     0,\\n             0,     0,     0,     0,     0],\\n        [  101, 18431,  2571,  3504,  2646,  \\n3293, 13395,  1006,\\n         26665,  1007, 26665,  1011,  2797,  \\n5211,  3813, 18431,\\n          2571,  2177,  1010,  1032,  2029,  \\n2038,  1037,  5891,\\n          2005,  2437,  2092,  1011, 22313,  \\n1998,  5681,  1032,\\n          6801,  3248,  1999,  1996,  3639,  \\n3068,  1010,  2038,\\n          5168,  2872,  1032,  2049, 29475,  \\n2006,  2178,  2112,\\n          1997,  1996,  3006,  1012,   \\n102]])}\\nIn this example, tokenize_function() takes a batch of\\nsamples, tokenizes them using the DistilBERT tokenizer, and\\nensures uniform length by padding and truncating as needed.\\nAs you can check out, the ﬁrst element was shorter than the\\nsecond, so it has some additional tokens with an ID of 0 at the\\nend. The zeros correspond to the [PAD] token, which will be\\nignored during inference. Note that the attention mask for this'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"sample also has 0 at the end - this ensures that the model only\\npays attention to the actual tokens.\\nNow that we understand the tokenization, we can use the\\nmap() method to tokenize the whole dataset. map() applies a\\nfunction to each element of the dataset in parallel.\\ntokenized_datasets = \\nraw_datasets.map(tokenize_function, \\nbatched=True)\\ntokenized_datasets\\nDatasetDict({\\n    train: Dataset({\\n        features: ['text', 'label', \\n'input_ids', 'attention_mask'],\\n        num_rows: 120000\\n    })\\n    test: Dataset({\\n        features: ['text', 'label', \\n'input_ids', 'attention_mask'],\\n        num_rows: 7600\\n    })\\n})\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Deﬁne Evaluation Metrics\\nIn addition to monitoring the loss during training, it’s usually a\\ngood idea to deﬁne some downstream metrics to better\\nevaluate and monitor the model’s performance. We’ll leverage\\nthe evaluate library, a handy tool with a standardized interface\\nfor various metrics. The choice of metrics depends on the task.\\nFor sequence classiﬁcation, suitable candidates can be:\\nAccuracy: Represents the proportion of correct predictions\\nout of all predictions, providing a high-level overview of\\nthe model’s overall performance. It’s a good metric for\\nbalanced datasets and is easy to interpret. However, it can\\nbe misleading for imbalanced datasets.\\nPrecision: This is the ratio of correctly labeled positive\\ninstances to all instances predicted as positive. It helps us\\nunderstand how accurate the model’s positive predictions\\nare. Precision should be used when the cost of false\\npositives is high, such as spam detection.\\nRecall: This metric indicates the proportion of actual\\npositive instances that were correctly predicted by the\\nmodel. It reﬂects the model’s ability to capture all positive\\ninstances, and it will be lower if there are many false\\nnegatives. Recall should be used when the cost of false\\nnegatives is high, such as in medical diagnosis.\\n3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='F1 Score: The F1 score is the harmonic mean  of precision\\nand recall, oﬀering a balanced measure that considers both\\nfalse positives and false negatives and penalizes strong\\ndiscrepancies between precision and recall. F1 is often used\\nfor imbalanced datasets and is a good default metric for\\nclassiﬁcation tasks.\\nMetrics in evaluate provide a description attribute and a\\ncompute() method to obtain the metric given the labels and\\nmodel predictions.\\nimport evaluate\\naccuracy = evaluate.load(\"accuracy\")\\nprint(accuracy.description)\\nprint(accuracy.compute(references=[0, 1, 0, \\n1], predictions=[1, 0, 0, 1]))\\n(\\'Accuracy is the proportion of correct \\npredictions among the total \\'\\n \\'number of cases processed. It can be \\ncomputed with:\\\\n\\'\\n \\'Accuracy = (TP + TN) / (TP + TN + FP + \\nFN)\\\\n\\'\\n \\' Where:\\\\n\\'\\n \\'TP: True positive\\\\n\\'\\n3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='\\'TN: True negative\\\\n\\'\\n \\'FP: False positive\\\\n\\'\\n \\'FN: False negative\\\\n\\')\\n{\\'accuracy\\': 0.5}\\nLet’s deﬁne a compute_metrics() function that, given a\\nprediction instance (which contains both the label and\\npredictions), returns a dictionary with the accuracy and the F1\\nscore. When we evaluate the model during the training, we will\\nautomatically use this function to monitor its progress.\\nf1_score = evaluate.load(\"f1\")\\ndef compute_metrics(pred):  \\n    labels = pred.label_ids\\n    preds = pred.predictions.argmax(-1)  \\n    # Compute accuracy and F1 Score\\n    acc_result = \\naccuracy.compute(references=labels, \\npredictions=preds)\\n    acc = acc_result[\"accuracy\"]  \\n    f1_result = f1_score.compute(\\n        references=labels, predictions=preds, \\naverage=\"weighted\"'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=')\\n    f1 = f1_result[\"f1\"]  \\n    return {\"accuracy\": acc, \"f1\": f1}  \\ncompute_metrics() expects an EvalPrediction\\ninstance. An EvalPrediction is a utility class used by the\\nTrainer that contains the labels and model predictions for\\na sample.\\nUse argmax to get the predicted class with the highest\\nprobability.\\nUse the loaded accuracy to compute the accuracy score\\nbetween labels and predictions. Recall that accuray\\noutputs a dictionary with the accuracy key.\\nRepeat with the F1 score. As we have multiple classes, we\\nuse the weighted=True argument. This means that we\\ncalculate F1 for each class and then average them weighted\\nby the number of true instances for each class.\\nFinally, return both metrics by building a dictionary.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Train the Model\\nTime to train. Recall that DistilBERT is an encoder model. If we\\nuse the raw model as is, we’ll get the embeddings, as we did in\\nChapter 2, so we cannot use this model directly. For classifying\\ntext sequences, we feed these embeddings to a classiﬁcation\\nhead. When ﬁne-tuning, we won’t use ﬁxed embeddings: all the\\nmodel parameters, the original weights, and the classiﬁcation\\nhead are trainable. This requires the head to be diﬀerentiable\\nand leads us to use a neural network on top of the base\\ntransformer. This head will take the embeddings as input and\\noutput class probabilities. Why do we train all the weights? By\\ntraining all the parameters, we help make the embeddings\\nmore useful for this speciﬁc classiﬁcation task.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 6-2. Figure 6-2. BERT with a classiﬁcation head. In practice, the embedding\\ncorresponding to CLS is used as the pooled embedding and can be used for\\nclassiﬁcation tasks.\\nAlthough we’ll use a simple feed-forward network, we can use\\nmore complex networks as the head or even classic models,\\nsuch as Logistic Regression or Random Forests (in which case\\nwe use the model as a feature extractor and freeze the weights).\\nUsing a simple layer works well, is computationally eﬃcient,\\nand is the most common approach.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='NOTE\\nIf you’ve done transfer learning in Computer Vision, you might be familiar with the\\nconcept of freezing the weights of the base model. This is frequently not done in NLP,\\nas our goal is to make the internal language representations more useful for the\\ndownstream task. In Computer Vision, it’s frequent to freeze some layers as the\\nfeatures learned by the base model are more general and useful for many tasks. For\\nexample, some layers capture generic features like edges or textures, which are\\nbroadly applicable across vision tasks. Whether to freeze or unfreeze layers depends\\non the context, including the dataset size, the amount of computing, and the\\nsimilarity between the pre-training and ﬁne-tuning tasks. Later in the chapter, we’ll\\nlearn about a technique called adapters, which allow us to work with frozen LLMs.\\nTo train the model with the classiﬁcation head, we can load the\\nmodel with AutoModelForSequenceClassification. This\\nwill does two things:\\nLoad the speciﬁed model (DistilBERT in this case) without\\nits masked language model head. This is the encoder part of\\nthe model, which outputs an embedding for each token. It\\nadditionally outputs a pooled embedding, which captures\\ninformation for the whole sequence.\\nAdd a randomly-initialized classiﬁcation head on top of the\\nmodel. This head is only a linear layer that receives the\\npooled embedding and outputs the class probabilities.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='import torch\\nfrom transformers import \\nAutoModelForSequenceClassification\\nfrom genaibook import get_device\\ndevice = get_device()\\nnum_labels = 4\\nmodel = \\nAutoModelForSequenceClassification.from_pretraine\\n    checkpoint, num_labels=num_labels\\n).to(device)\\n(\\'Some weights of \\nDistilBertForSequenceClassification were not \\n\\'\\n \\'initialized from the model checkpoint at \\ndistilbert-base-uncased \\'\\n \"and are newly initialized: \\n[\\'classifier.bias\\', \"\\n \"\\'classifier.weight\\', \\'pre_classifier.bias\\', \\n\"\\n \"\\'pre_classifier.weight\\']\\\\n\"\\n \\'You should probably TRAIN this model on a \\ndown-stream task to be \\''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"'able to use it for predictions and \\ninference.')\\nYou will get a warning about some weights being newly\\ninitialized. This makes sense - we have a new head suitable for\\nour classiﬁcation task and need to train it.\\nWith our model initialized, we can ﬁnally train it. There are\\ndiﬀerent approaches we can take to train the model. If you’re\\nfamiliar with PyTorch, you can write your training loop.\\nAlternatively, transformers provides a high-level class called\\nTrainer, which streamlines much of the training loop\\ncomplexity.\\nThe ﬁrst step before creating our Trainer is to deﬁne\\nTrainingArguments, which speciﬁes the hyperparameters\\nused for training, such as learning rate and weight decay,\\ndetermining the number of samples per batch, setting\\nevaluation intervals, and deciding whether we want to share\\nour model with the ecosystem by pushing it to the Hub. We\\nwon’t modify the hyperparameters as the defaults provided by\\nthe TrainingArguments generally perform well. Still, we\\nencourage you to explore and experiment with them. The\\nTrainer class is a robust and ﬂexible tool.4\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from transformers import TrainingArguments\\nbatch_size = 32  # You can change this if you \\nhave a big or small GPU\\ntraining_args = TrainingArguments(\\n    \"classifier-chapter4\",\\n    push_to_hub=True,  \\n    num_train_epochs=2,  \\n    eval_strategy=\"epoch\",  \\n    per_device_train_batch_size=batch_size,  \\n    per_device_eval_batch_size=batch_size,\\n)\\nWhether or not to push the model to the Hugging Face Hub\\nevery time the model is saved. You can change how often the\\nmodel is saved with save_strategy, which is done every\\nfew hundred steps by default.\\nTotal number of epochs to perform; an epoch is a full pass\\nthrough the training data.\\nWhen to evaluate the model on the validation set. It’s done\\nevery 500 steps by default, but by specifying epoch, the\\nevaluation happens at the end of each epoch.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The batch size per core for training. You can reduce this if\\nyour GPU is running out of memory. Alternatively, you can\\nuse auto_find_batch_size=True to ﬁnd the largest batch\\nsize that ﬁts on your GPU.\\nWe now have all the pieces we need:\\nA pre-trained model with a proper head ready to be ﬁne-\\ntuned\\nThe training arguments\\nA function that will compute metrics\\nA training and evaluation dataset\\nA tokenizer, which we add to ensure it’s pushed with the\\nmodel to the Hub\\nThe AG News dataset contains 120,000 samples, more than we\\nneed to get good initial results. To make an initial quick training\\nrun, we’ll use 10,000 samples, but feel free to play with this\\nnumber - more data should yield better results. Note that we’ll\\nstill evaluate with the whole test set.\\nfrom transformers import Trainer\\n# Shuffle the dataset and pick 10,000 \\nexamples for training\\nshuffled_dataset = \\n5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='tokenized_datasets[\"train\"].shuffle(seed=42)\\nsmall_split = \\nshuffled_dataset.select(range(10000))\\n# Initialize the Trainer\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    compute_metrics=compute_metrics,\\n    train_dataset=small_split,\\n    eval_dataset=tokenized_datasets[\"test\"],\\n    tokenizer=tokenizer,\\n)\\nWith everything ready and the Trainer initialized, it’s time to\\ntrain.\\ntrainer.train()\\nThe training will report the loss, the evaluation metrics, and\\ntraining speed details. Table 6-1 provides a summarized view.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Metric Epoch 1 Value Epoch 2 Value\\neval_loss 0.2624 0.2433\\neval_accuracy 0.9117 0.9184\\neval_f1 0.9118 0.9183\\neval_runtime 15.2709 14.5161\\neval_samples_per_second 497.678 523.557\\neval_steps_per_second 15.585 16.396\\ntrain_runtime - 213.9327\\ntrain_samples_per_second - 93.487\\ntrain_steps_per_second - 2.926\\ntrain_loss - 0.2714\\nTable 6-1: Training and evaluation metrics for DistilBERT ﬁne-\\ntuning on the AG News dataset.\\nHopefully, that took just a little bit of time. The ﬁnal evaluation\\naccuracy and F1 score were close to 92%, which is okay,\\nespecially given we’re using less than 10% of the available\\ntraining data. The evaluation loss decreases between epochs,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='which is exactly what we were aiming for. If you want to share\\nthe ﬁnal model for others to access, you need to make a call to\\npush_to_hub at the end. You can ﬁnd our model on the Hub.\\ntrainer.push_to_hub()\\nAlthough using the Trainer might appear like a black box,\\nunder the hood, it’s just making regular PyTorch training loops\\nas we did to train simple diﬀusion models in the diﬀusion\\nchapter. Writing such a loop from scratch would look like\\nsomething along these lines:\\nfrom transformers import AdamW, get_scheduler\\noptimizer = AdamW(model.parameters(), lr=5e-\\n5) \\nlr_scheduler = get_scheduler(\"linear\", ...) \\nfor epoch in range(num_epochs): \\n    for batch in train_dataloader: \\n        batch = {k: v.to(device) for k, v in \\nbatch.items()} \\n        outputs = model(**batch)\\n        loss = outputs.loss \\n        loss.backward()'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='optimizer.step() \\n        lr_scheduler.step()\\n        optimizer.zero_grad()\\nThe optimizer holds the current state of the model and will\\nupdate the parameters based on the gradients.\\nA learning rate scheduler that deﬁnes how the learning rate\\nchanges through training.\\nIterate over all data for a number of epochs.\\nIterate over all batches in the training data.\\nMove the batch to the device and run the model.\\nCompute the loss and backpropagate.\\nUpdate the model parameters, adjust the learning rate, and\\nreset the gradients to zero.\\nThe Trainer takes care of this, from doing evaluations and\\npredictions, pushing the models to the Hub, training on\\nmultiple GPUs, saving instant checkpoints, logging, and many\\nother things.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='If you pushed the model to the Hub, others can now access it\\nusing AutoModel or pipeline(). Let’s try out an example.\\n# Use a pipeline as a high-level helper\\nfrom transformers import pipeline\\npipe = pipeline(\\n    \"text-classification\",\\n    model=\"genaibook/classifier-chapter4\",\\n    device=device,\\n)\\npipe(\\n    \"\"\"The soccer match between Spain and\\nPortugal ended in a terrible result for \\nPortugal.\"\"\"\\n)\\n[{\\'label\\': \\'Sports\\', \\'score\\': \\n0.8631355166435242}]\\nThe prediction appears to be correct. In your ﬁrst try, you might\\nget LABEL_1 instead of Sports. This is because the model\\ndoesn’t have a knowledge of the intrinsic label names. To\\nupdate them, you can update the conﬁg.json ﬁle by adding the\\nid2label and label2id mappings. This will make the\\npredictions more human-readable and interpretable.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Let’s do a deep dive into the metrics. You can either use\\nTrainer.predict or pipe.predict to get the predictions.\\nThe Trainer.predict method returns a PredictionOutput\\nobject, which contains the predictions, label IDs, and metrics,\\nwhile pipe.predict returns a list of dictionaries with the\\npredictions and the corresponding labels. Let’s conﬁrm that\\nthings make sense by looking at the ﬁrst three sample texts with\\ntheir corresponding predictions and labels. Running some\\nsamples through the network is always important to ensure\\nthat things work correctly.\\n# Get prediction for all samples\\nmodel_preds = \\npipe.predict(tokenized_datasets[\"test\"]\\n[\"text\"])\\n# Get the dataset labels\\nreferences = tokenized_datasets[\"test\"]\\n[\"label\"]\\n# Get the list of label names\\nlabel_names = \\nraw_train_dataset.features[\"label\"].names\\n# Print results of the first 3 samples\\nsamples = 3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='texts = tokenized_datasets[\"test\"][\"text\"]\\n[:samples]\\nfor pred, ref, text in \\nzip(model_preds[:samples], \\nreferences[:samples], texts):\\n    print(f\"Predicted {pred[\\'label\\']}; Actual \\n{label_names[ref]};\")\\n    print(text)\\n(\\'Predicted Business; Actual Business; Fears \\nfor T N pension after \\'\\n \\'talks Unions representing workers at Turner   \\nNewall say they are \\'\\n \"\\'disappointed\\' after talks with stricken \\nparent firm Federal \"\\n \\'Mogul.\\\\n\\'\\n \\'\\\\n\\'\\n \\'Predicted Sci/Tech; Actual Sci/Tech; The \\nRace is On: Second \\'\\n \\'Private Team Sets Launch Date for Human \\nSpaceflight (SPACE.com) \\'\\n \\'SPACE.com - TORONTO, Canada -- A \\nsecond\\\\team of rocketeers \\'\\n \\'competing for the  #36;10 million Ansari X \\nPrize, a contest \\'\\n \\'for\\\\\\\\privately funded suborbital space \\nflight, has officially \\''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"'announced the first\\\\\\\\launch date for its \\nmanned rocket.\\\\n'\\n '\\\\n'\\n 'Predicted Sci/Tech; Actual Sci/Tech; Ky. \\nCompany Wins Grant to '\\n 'Study Peptides (AP) AP - A company founded \\nby a chemistry '\\n 'researcher at the University of Louisville \\nwon a grant to develop '\\n 'a method of producing better peptides, \\nwhich are short chains of '\\n 'amino acids, the building blocks of \\nproteins.')\\nThe prediction is aligned with the reference, and the label\\nmakes sense. Let’s now dive into the metrics.\\nIn Machine Learning classiﬁcation tasks, a confusion matrix\\nserves as a table summarizing a model’s performance, depicting\\ncounts of true positive, true negative, false positive, and false\\nnegative predictions. For multi-class classiﬁcation, the matrix\\nbecomes a square with dimensions equal to the number of\\nclasses, where each cell represents the counts of instances for\\nthe combination of labels and predicted classes. Rows indicate\\nactual (ground truth) classes, while columns indicate predicted\\nclasses. We can normalize the matrix so that each row adds up\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='to 1, making it easier to interpret the model’s performance\\nacross diﬀerent classes. Analyzing this matrix provides insights\\ninto the model’s strengths and weaknesses in distinguishing\\nbetween speciﬁc classes.\\nWe’ll use evaluate to load and compute the confusion matrix\\nand the ConfusionMatrixDisplay from sklearn to visualize\\nit. The confusion matrix will help us understand where the\\nmodel is making mistakes and which classes are more\\nchallenging to predict. For example, by looking at the confusion\\nmatrix in Figure 6-3, we can check that business articles are\\noften mislabeled as Sci/Tech articles.\\nimport matplotlib.pyplot as plt\\nfrom sklearn.metrics import \\nConfusionMatrixDisplay\\n# Convert predicted labels to ids\\nlabel_to_id = {name: i for i, name in \\nenumerate(label_names)}\\npred_labels = [label_to_id[pred[\"label\"]] for \\npred in model_preds]\\n# Compute confusion matrix\\nconfusion_matrix = \\nevaluate.load(\"confusion_matrix\")'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='cm = confusion_matrix.compute(\\n    references=references, \\npredictions=pred_labels, normalize=\"true\"\\n)[\"confusion_matrix\"]\\n# Plot the confusion matrix\\nfig, ax = plt.subplots(figsize=(6, 6))\\ndisp = \\nConfusionMatrixDisplay(confusion_matrix=cm, \\ndisplay_labels=label_names)\\ndisp.plot(cmap=\"Blues\", values_format=\".2f\", \\nax=ax, colorbar=False)\\nplt.title(\"Normalized confusion matrix\")\\nplt.show()'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 6-3: Confusion matrix for our ﬁne-tuned DistilBERT\\nmodel on the AG News dataset.\\nStill Relevant?\\nTraining an encoder model on text classiﬁcation might appear\\nalmost quaint in the post-ChatGPT era. Can’t we prompt a cheap\\nmodel to \"Classify the text into one of the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='following categories…“? While it is tough to beat the\\nconvenience of fast generalist models, there are still cases\\nwhere small, custom classiﬁers prove helpful—particularly in\\napplications where speed and eﬃciency are key.\\nFor example, one domain in which these models shine is the\\npreparation of training data for today’s largest LLMs, which\\ninvolves processing vast quantities of text. In the paper \"The\\nLlama 3 Herd of Models\" , the authors apply something\\nthey call \"Model-based quality filtering“, in which a\\n\"quality classifier\" is trained and then used to score\\ndocuments for quality ﬁltering. The authors state, \"We use\\nDistilRoberta to generate quality scores for each\\ndocument for efficiency reasons.\" The training data for\\nthis quality model is predictions from Llama 2; it would have\\nbeen extremely costly to use Llama 2 directly on the many\\ntrillions of tokens worth of data that had to be ﬁltered. A similar\\napproach was used by the team behind Phi 3[Abdin, Marah, et\\nal.\\xa0Phi-3 Technical Report: A Highly Capable Language Model\\nLocally on Your Phone. arXiv, 23 May 2024. arXiv.org,\\nhttps://arxiv.org/abs/2404.14219], and by the FineWeb authors to\\ncurate the most educational content for their FineWeb Edu\\nsubset. The authors wrote an excellent blog post on their\\nrecipes and the importance of quality ﬁltering.\\n6'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Another helpful use of small and fast encoder-based models is\\nobtaining embeddings for retrieval systems, where the goal is to\\nﬁnd the most similar documents to a given query. A ﬁnal\\nexample of their usage is guard railing, where you use a small\\nmodel to check the input to a model to detect if it’s malicious or\\nto check the model’s output to ensure it’s not harmful. In all\\nthese cases, the speed and eﬃciency of a small model are key.\\nWith all that said, today’s trend is away from these small,\\nspecialized models and toward more capable generalist models.\\nStill, there is room for customization (hence this book), so let’s\\ndiscuss how we can apply a similar process to the one outlined\\nabove to generate text: ﬁnd (or create) a dataset, pick a model,\\ndeﬁne evaluation metrics, and train the model.\\nGenerating Text\\nWe’ve just ﬁne-tuned an encoder-based model for text\\nclassiﬁcation. Now, let’s dive into training a model for text\\ngeneration. While in text classiﬁcation, our labels are a list of\\ndiscrete options (World, Sports, Business, and Sci/Tech), training\\nand ﬁne-tuning a generative model means doing the next token\\nprediction task, where the labels are text outputs.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='For instance, if our goal is to generate code, we can gather a\\nsubstantial dataset of permissible code (such as The Stack ) and\\ntrain a model from scratch. Although this is interesting, it would\\nrequire a lot of compute to get decent results (leading to\\nmultiple training weeks or months).\\nInstead of training a model from scratch for open-ended text\\ngeneration, we can ﬁne-tune an existing model to generate text\\nin a speciﬁc style. This approach allows us to beneﬁt from the\\nmodel’s pre-existing knowledge about the language, drastically\\nreducing the need for extensive data and computing power. For\\nexample, you could employ a few hundred tweets to generate\\nnew ones in your distinctive writing style.\\nWITH LABELS OR WITHOUT LABELS?\\nWith the next token prediction task, we do not need to explicitly label the data as we\\ndid for classiﬁcation—the model will learn to predict the next token based on the\\ninput text. This has allowed us to build large-scale datasets from the web.\\nOn the other hand, a new family of techniques, Reinforcement Learning with Human\\nFeedback (RLHF), allows us to steer the model’s output by providing feedback. This is\\nparticularly useful for conversational models, where you can provide preferences or\\ncorrections to the model’s output. This is why many chatbots have a thumbs-\\nup/thumbs-down button or provide side-by-side generated text for users to select the\\nbest one. Even then, preference optimization is just one component of the training\\nprocess, and the model still needs to learn from data unsupervised. We’ll discuss\\nmore about RLHF in the ﬁnal chapter.\\n7'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Let’s continue the news theme and ﬁne-tune a model to\\ngenerate news in a speciﬁc style, such as business news. We can\\nuse the same AG News dataset. Let’s start by ﬁltering all\\nsamples labeled as business (where the label is 2) and\\nremoving the unnecessary label column.\\nfiltered_datasets = \\nraw_datasets.filter(lambda example: \\nexample[\"label\"] == 2)\\nfiltered_datasets = \\nfiltered_datasets.remove_columns(\"label\")\\nPicking the Right Generative Model\\nThe second question is which base model to use. As our goal is\\nto do text generation, we need a decoder model. With\\nthousands of models available, we need to choose one that ﬁts\\nour requirements. Let’s discuss some of the key factors that\\nmight inﬂuence our decision:\\nModel Size: Deploying a model with 60 billion parameters\\nlocally on your computer won’t be practical. The choice of\\nmodel size depends on factors like expected inference time,\\nhardware capacity, and deployment requirements. Later in\\nthis chapter, we’ll explore techniques that enable running'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='models with more parameters using the same computing\\nresources.\\nTraining data: The performance of your ﬁne-tuned model\\ncorrelates with how closely the training data of the base\\nmodel aligns with your inference data. For instance, ﬁne-\\ntuning a model to generate code in your codebase style is\\nmore eﬀective when starting with a model pre-trained on\\ncode. Consider the speciﬁcity of data sources, especially if\\nnot all models disclose their training data. Similarly, you\\nwill want to use something other than a predominantly\\nEnglish-based model for Korean text generation (such as\\nmultilingual or a Korean-trained model). Not all models\\ndisclose which were the data sources, which can make it\\nchallenging to identify this.\\nContext length: As discussed before, diﬀerent models have\\ndiﬀerent context length limits. For example, if the context\\nlength is 1024, the model can use the last 1024 tokens to\\nmake predictions. To generate long-form text, you will need\\na model with a large context length. We’ll explore ways to\\nwork with longer contexts later in the book.\\nLicense: The licensing aspect is crucial when selecting a\\nbase model. Consider whether the model aligns with your\\nusage requirements. Models may have commercial or non-\\ncommercial licenses, and there’s a distinction between'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='open-source and open-access licenses. Understanding these\\nlicenses is essential to ensure compliance with legal and\\nusage restrictions. For example, although some models may\\npermit commercial use, they can specify permissive use\\ncases and scenarios where the model should not be used. In\\nother cases, the license may limit how the model’s output\\ncan be used (e.g.\\xa0prohibiting using the output of a model to\\ntrain another model).\\nAssessing generation models remains a challenge, with various\\nbenchmarks evaluating speciﬁc aspects. Benchmarks such as\\nARC for science questions, HellaSwag for common sense\\ninference, and others serve as proxies for diﬀerent capabilities.\\nHugging Face Open LLM Leaderboard collects benchmark\\nresults for thousands of models and allows ﬁltering according\\nto model size and type. However, it’s essential to note that these\\nbenchmarks are tools for systematic comparison, and the ﬁnal\\nmodel choice should always be based on its performance in\\nyour real-world task. Many of the benchmarks used in the Open\\nLLM Leaderboard are not focused on conversation, and hence,\\nthey should not be used as the main criteria for picking a\\nconversational model. The choice of model depends on your use\\ncase, and selecting a model based on a single metric is not\\nrecommended.\\n8'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The Leaderboard considers a set of challenging benchmarks:\\nMMLU-Pro (knowledge): a knowledge dataset containing\\n12 thousand multi-choice questions. Each question includes\\na passage and ten answer choices. The questions are about\\nmath, physics, economics, psychology, business, and more\\ndisciplines.\\nGPQA (complex knowledge): It’s a small dataset of\\nchallenging graduate-level multiple-choice physics,\\nchemistry, and biology questions. The questions are\\ndesigned by domain experts (having or pursuing a PhD in\\nthe respective ﬁeld) and are expected to be challenging\\neven for non-experts (skilled humans from other ﬁelds with\\naccess to the internet - the GP in GPQA stands for \"Google-\\nPoof“).\\nMuSR (multi-step reasoning): It contains complex\\nproblems with around a thousand words each, which\\npresents challenges for short-context models. The problems\\ncan include murder mysteries, team allocation, and object\\nplacements.\\nMATH (problem-solving): It contains over 12,000\\nproblems from high school math exams. There are diﬀerent\\nlevels of diﬃculty, and the LLM Leaderboard only uses the\\nhardest, level 5.\\n8 \\n9'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='BBH (mix): This benchmark contains a suite of 23\\nchallenging tasks that require some multi-step reasoning.\\nThe tasks cover algorithmic and arithmetic reasoning (e.g.,\\nboolean expressions, geometric shapes, and navigation),\\nnatural language understanding (e.g., sarcasm detection\\nand adjective ordering), world knowledge (e.g.,\\nunderstanding sports and recommending movies), and\\nreasoning (translation error detection). This benchmark is\\ncorrelated with human preference.\\nIFEval (instruction-following): This dataset evaluates\\nwhether a model can follow instructions such as \"mention\\nthe keyword Y at least three times\" or \"write in\\nless than ten words.\"\\nOut of the six benchmarks used by the LLM Leaderboard, only\\nIFEval is speciﬁcally targeted towards conversational models.\\nWe’ll discuss conversational models later in the chapter. Our\\ncurrent goal is generating text in a speciﬁc style, so we’ll focus\\non that. Table 6-2 shows a couple of popular open-access pre-\\ntrained LLMs.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Model Creator Size Training D\\nGPT-2 OpenAI 117M380M812M1.6B Unreleased\\n40GB of tex\\na web scrap\\nGPT-Neo EleutherAI 125M1.3B2.7B The Pile300\\ntokens380B\\ntokens420B\\nFalcon TII UAE 7B11B40B 180B Partially re\\nReﬁnedWe\\non top of\\nCommonCr\\ntokens1T\\ntokens3.5T\\nLlama 2 Meta 7B13B70B Unreleased\\ntokens\\nLlama 3 Meta 8B70B Unreleased\\ntokens.\\nLlama 3.1 Meta 8B70B405B Unreleased\\ntokens.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Model Creator Size Training D\\nMistral Mistral 7B Unreleased\\nMixtral Mistral 8x7B8X22B* Unreleased\\nQwen 2 Alibaba 500M1.5B7B72B110B Unreleased\\nPhi (1, 1.5,\\n2)\\nMicrosoft 1.42B1.42B2.78 54B tokens\\ntokens1.4T\\nTable 6-2. A selection of popular open-access pre-trained LLMs\\nand their performance on the Open LLM Leaderboard.\\nNOTE\\nWhat does “8x7B” mean in the “Mixtral” model? This means the model is a Mixture\\nof Experts (MoE), a special model architecture we’ll learn more about in the last\\nchapter. In short, it’s a model with multiple smaller models, and an internal\\nmechanism decides which models to use for each token. Comparing a number of\\nparameters between MoE models and regular dense models is not straightforward,\\nas we’ll learn more about it later.\\nThis table is not exhaustive; there are many other open LLMs,\\nsuch as Mosaic MPT and Cohere Command R+, and when this\\nbook is published, there will be likely many others. Similarly,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='this table does not cover code models. For those, you might\\nwant to review the Big Code Models Leaderboard, where you\\ncan ﬁnd models such as CodeLlama (a popular model from\\nMeta) and BigCode’s model (a model trained with permissively-\\nlicensed code).\\nAdditionally, it’s worth noting that this table is biased towards\\nmodels trained on mostly English data. However, powerful\\nChinese models such as InternLM, ChatGLM, and Baichuan are\\nalso noteworthy contributors to the expanding landscape of\\npre-trained language models. This information serves as a\\nguide on what to consider when choosing a model for\\nexperimentation rather than an exhaustive list of open-source\\nmodels.\\nTraining a Generative Model\\nGiven that we want to do a quick training with very little data\\nthat can run in an environment without a powerful GPU, we’ll\\nﬁne-tune SmolLM’s smallest variant. We encourage you to\\nexperiment with larger models and diﬀerent datasets. Later in\\nthe chapter, we’ll explore techniques for using larger models for\\ninference and training.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Just as before, we’ll begin by loading the model and the\\ntokenizer. One particular thing about SmolLM is that it does not\\nspecify a padding token, but we require one when tokenizing,\\nas it’s used to ensure all samples have the same length. We can\\nset the padding token to be the same as the end-of-text token.\\nfrom transformers import AutoModelForCausalLM\\nmodel_id = \"HuggingFaceTB/SmolLM-135M\"\\ntokenizer = \\nAutoTokenizer.from_pretrained(model_id)\\ntokenizer.pad_token = (\\n    tokenizer.eos_token\\n)  # Needed as SmolLM does not specify \\npadding token.\\nmodel = \\nAutoModelForCausalLM.from_pretrained(model_id).to\\nWe’ll tokenize the dataset (but using SmolLM’s tokenizer).\\ndef tokenize_function(batch):\\n    return tokenizer(batch[\"text\"], \\ntruncation=True)\\ntokenized_datasets = filtered_datasets.map('),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='tokenize_function,\\n    batched=True,\\n    remove_columns=[\"text\"],  # We only need \\nthe input_ids and attention_mask\\n)\\ntokenized_datasets\\nDatasetDict({\\n    test: Dataset({\\n        features: [\\'input_ids\\', \\n\\'attention_mask\\'],\\n        num_rows: 1900\\n    })\\n    train: Dataset({\\n        features: [\\'input_ids\\', \\n\\'attention_mask\\'],\\n        num_rows: 30000\\n    })\\n})\\nIn the topic classiﬁcation example, we padded and truncated all\\nsamples to ensure they were the same length. Apart from doing\\nit in the tokenization stage, we can do it using data collators.\\nData collators are utilities that assemble samples into a batch.\\ntransformers provides some out-of-the-box collators for tasks'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='(such as language modeling). The collator will dynamically pad\\nthe examples in a batch to the maximum length. Apart from the\\npadding, the language modeling collator structures the inputs\\nfor the language modeling task, which is slightly more complex\\nthan before. In language modeling, we shift the inputs by one\\nelement and use that as a label. For example, if the input is I\\nlove Hugging Face, the label is love Hugging Face. The\\nmodel aims to predict the next token given the previous ones.\\nIn practice, the data collator will create a label column with a\\ncopy of the inputs. Later, the model will take care of shifting the\\ninputs and labels.\\nfrom transformers import \\nDataCollatorForLanguageModeling\\n# mlm corresponds to masked language modeling\\n# and we set it to False as we are not \\ntraining a masked language model\\n# but a causal language model\\ndata_collator = \\nDataCollatorForLanguageModeling(tokenizer=tokeniz\\nmlm=False)\\nLet’s check out how this works for three samples. As shown\\nbelow, each sample has a diﬀerent length (37, 55, and 51).'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='samples = [tokenized_datasets[\"train\"][i] for \\ni in range(3)]\\nfor sample in samples:\\n    print(f\"input_ids shape: \\n{len(sample[\\'input_ids\\'])}\")\\ninput_ids shape: 37\\ninput_ids shape: 55\\ninput_ids shape: 51\\nThanks to the collator, the samples are padded to the maximum\\nlength in the batch (55) and a label column is added.\\nout = data_collator(samples)\\nfor key in out:\\n    print(f\"{key} shape: {out[key].shape}\")\\ninput_ids shape: torch.Size([3, 55])\\nattention_mask shape: torch.Size([3, 55])\\nlabels shape: torch.Size([3, 55])\\nFinally, we need to deﬁne the training arguments. In this\\nexample, we modify a couple to showcase that'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='TrainingArguments provides some control and ﬂexibility.\\nLet’s brieﬂy discuss a couple of key parameters, showcasing the\\nsigniﬁcant inﬂuence they can have on model training:\\nWeight Decay: Weight decay is a regularization technique\\nthat prevents model overﬁtting by adding a penalty term to\\nthe loss function. It discourages the learning algorithm\\nfrom assigning large weights. Adjusting the weight decay\\nparameter in the TrainingArguments allows you to\\nadjust this regularization eﬀect, inﬂuencing the model’s\\ngeneralization capabilities.\\nLearning Rate: The learning rate is a key hyperparameter\\ndetermining the optimization step size. In the context of the\\nTrainingArguments, you can specify the learning rate,\\ninﬂuencing the convergence speed and stability of the\\ntraining process. Careful tuning of the learning rate can\\nsigniﬁcantly impact the model’s generation quality.\\nLearning Rate Scheduler Type: The learning rate\\nscheduler dictates how the learning rate evolves during\\ntraining. Diﬀerent tasks and model architectures may\\nbeneﬁt from speciﬁc scheduling strategies. The\\nTrainingArguments provides options to deﬁne the\\nlearning rate scheduler type, enabling you to experiment\\nwith various schedules such as constant learning rates,\\ncosine annealing, or others.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='training_args = TrainingArguments(\\n    \"business-news-generator\",\\n    push_to_hub=True,\\n    per_device_train_batch_size=8,\\n    weight_decay=0.1,\\n    lr_scheduler_type=\"cosine\",\\n    learning_rate=5e-4,\\n    num_train_epochs=2,\\n    eval_strategy=\"steps\",\\n    eval_steps=200,\\n    logging_steps=200,\\n)\\nAfter all this setup, just as in the classiﬁcation example, the\\nﬁnal step is creating a Trainer instance with all the\\ncomponents. The main diﬀerences are that we’re using a data\\ncollator this time and that we’re using 5000 samples.\\ntrainer = Trainer(\\n    model=model,\\n    tokenizer=tokenizer,\\n    args=training_args,\\n    data_collator=data_collator,\\n    \\ntrain_dataset=tokenized_datasets[\"train\"].select'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='eval_dataset=tokenized_datasets[\"test\"],\\n)\\ntrainer.train()\\nTable 6-3 summarizes the training and evaluation loss during\\nthe ﬁne-tuning process.\\nepoch step loss grad_norm lear\\n0.32 200 3.2009 2.99705 0.00\\n0.64 400 2.8833 2.46037 0.00\\n0.96 600 2.7102 2.35531 0.00\\n1.28 800 1.722 2.55815 0.00\\n1.6 1000 1.5371 1.89922 4.77\\n1.92 1200 1.4841 2.78178 1.97\\nTable 6-3. Training results for SmolLM ﬁne-tuning on the AG\\nNews dataset.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='trainer.push_to_hub()\\nAs before, we pushed the model to the Hub and can use\\npipeline() and specify the task (text-generation) to load\\nthe model and run inference.\\nfrom transformers import pipeline\\npipe = pipeline(\\n    \"text-generation\",\\n    model=\"genaibook/business-news-\\ngenerator\",\\n    device=device,\\n)\\nprint(\\n    pipe(\"Q1\", do_sample=True, \\ntemperature=0.1, max_new_tokens=30)[0][\\n        \"generated_text\"\\n    ]\\n)\\nprint(\\n    pipe(\"Wall\", do_sample=True, \\ntemperature=0.1, max_new_tokens=30)[0][\\n        \"generated_text\"\\n    ]\\n)\\nprint('),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='pipe(\"Google\", do_sample=True, \\ntemperature=0.1, max_new_tokens=30)[0][\\n        \"generated_text\"\\n    ]\\n)\\n(\\'Q1: China #39;s Airline Pilots Union Says \\nUnions May Block Planes \\'\\n \\'(Update1) China #39;s Air\\')\\n(\\'Wall Street Seen Flat After Jobless Data  \\nNEW YORK (Reuters) - \\'\\n \\'Wall Street was expected to see a  slightly \\nlower open on Friday\\')\\n(\\'Google IPO Imminent Google #39;s long-\\nawaited stock sale is \\'\\n \\'imminent, and the company is already \\nconsidering whether to sell \\'\\n \\'its\\')\\nAs you can notice, the generated text follows a similar structure\\nto the AG News business slice. However, the generated content\\nmay not always exhibit coherence, which is ﬁne considering\\nthat we used a small base model that doesn’t have great quality\\nand used little training data. Using Mistral 7B or a very large\\nmodel such as the 70B variant of Llama 3.1 would no doubt'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='yield much more coherent text while preserving the same\\nformat.\\nInstructions\\nIn the ﬁrst part of the chapter, we discussed ﬁne-tuning an\\nencoder-based model for speciﬁc text classiﬁcation tasks such\\nas topic classiﬁcation. However, this approach requires training\\na new model for each distinct task. If we encounter an unseen\\ntask, such as identifying if a text corresponds to spam, we won’t\\nhave a pre-trained model readily available, and we’ll need to\\nﬁne-tune a model for it. This leads us to explore other\\ntechniques, so let’s brieﬂy discuss the beneﬁts, limitations, and\\nuses of diﬀerent approaches:\\n1. Fine-tuning multiple models: We can pick and ﬁne-tune a\\nbase model for each task to build a specialized model. All\\nthe model weights are updated during ﬁne-tuning, which\\nimplies that if we want to solve ﬁve diﬀerent tasks, we’ll\\nend up with ﬁve model ﬁne-tunes.\\n2. Adapters: We can freeze the base model and train a small\\nauxiliary model called an adapter rather than modifying all\\nthe model weights. We would still need a diﬀerent adapter\\nfor every new task, but they are signiﬁcantly smaller,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='meaning we can easily have multiple without adding\\noverhead. There’s active research to manage hundreds or\\neven thousands of adapters in production, and they are\\nwidely popular and used both by practitioners and in\\nindustry. We’ll learn about adapters in the following\\nsection.\\n3. Prompting: As learned in the ﬁrst chapter, we can use a\\nrobust pre-trained model’s zero-shot and few-shot\\ncapabilities to solve diﬀerent tasks. With zero-shot, we\\nwrite a prompt that explains a task in detail. With a few-\\nshot approach, we add examples of solving the task and\\nimproving the model’s performance. The performance of\\nthese capabilities hinges on the strength of the base model.\\nA very strong model such as Llama 3.1 may yield\\nimpressive zero-shot results, which is great for tackling all\\nkinds of tasks, such as writing those long emails or\\nsummarizing a book chapter.\\n4. Supervised ﬁne-tuning (SFT) SFT, also known as instruct-\\ntuning,  is an alternative and simple way to improve the\\nzero-shot performance of LLMs. instruct-tuning formulates\\ntasks as instructions such as \"Is the topic of this\\npost business or sports?\" or \"Translate \\'`how\\nare you\\' to Spanish`”. This approach mainly involves\\nconstructing a dataset of instructions for many tasks and\\n1 0'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='then ﬁne-tuning a pre-trained language model with this\\nmixture of instruction datasets. Creating datasets for\\ninstruct-tuning is a task of manageable complexity; for\\ninstance, one could utilize AG News and structure the\\ninputs and labels as instructions by building a prompt like\\nthis:\\nTo which of the \"World\", \"Sports, \"Business\" \\nor \"Sci/Tech\" categories\\ndoes the text correspond to? Answer with a \\nsingle word:\\nText: Wall St. Bears Claw Back Into the Black \\n(Reuters)\\nReuters - Short-sellers, Wall Street\\'s \\ndwindling\\\\\\\\band of\\nultra-cynics, are seeing green again.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 6-3. Figure 6-4. The supervised ﬁne-tuning ﬂow.\\nBy building a large enough dataset of diverse instructions, we\\ncan end up with a general instruct-tuned model that can solve\\nmany tasks, even new ones, thanks to cross-task generalization.\\nThis idea is the foundation behind Flan, a model that can solve\\n62 tasks out of the box. This concept has been further expanded\\nby the Flan T5 model , an open-source family of instruct-tuned\\nT5 models that can solve over 1000 tasks. Something to note\\nhere is that the model is trained with input (instruction) and\\noutput (answer) texts; unlike the SmolLM ﬁne-tune example,\\nthis is a supervised training technique. Instruct-tuning has been\\nvery popular with encoder-decoder architectures such as T5 or\\n1 1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='BART due to the input-output structure of the dataset. The idea\\nhas then been extended to most LLMs.\\nWhen should you use ﬁne-tuning vs.\\xa0instruct-tuning vs.\\xa0prompt\\nengineering? Once again, it depends on the task, available\\nresources desired experimentation speed, and more. Usually,\\nﬁne-tuned models speciﬁc to a task or domain will perform\\nbetter. On the other hand, it won’t allow you to tackle tasks out\\nof the box. Instruct-tune is more versatile, but deﬁning the\\ndataset and structure requires additional work. Prompt\\nengineering is the most ﬂexible approach for quick\\nexperimentation, as it won’t require you to train a model out of\\nthe box. Still, it requires a more powerful base model, and there\\nis limited control over the generation.\\nWe won’t build an end-to-end example of instruct-tuning as it’s\\nmostly a dataset task rather than a modeling task, but let’s\\ndiscuss some excellent papers if it’s a topic you want to dive\\ndeeper into.\\nFinetuned Language Models Are Zero-Shot Learners\\n(February 2022)  trains a model called Flan using\\ninstruction tuning and outperforming the base model’s\\nzero-shot performance and the few-shot performance of\\nother models.\\n1 2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='After Flan, there was a new wave of dataset papers. Cross-\\nTask Generalization via Natural Language Crowdsourcing\\nInstructions (March 2022)  introduces Natural Instructions,\\na dataset of 61 tasks with human instructions and 193\\nthousand input-output pairs generated by mapping existing\\nNLP datasets to a uniﬁed schema. The premise of doing this\\nis that humans can follow instructions to solve novel\\nproblems by learning (in a supervised fashion) from\\ninstances of other tasks. The authors instruct-tuned BART,\\nan encoder-decoder model, leading to a 19% gain in cross-\\ntask generalization compared to not using instructions. The\\nmore tasks the model is trained on, the better it performs.\\nMultitask Prompted Training Enables Zero-Shot Task\\nGeneralization (March 2022)  follows a similar concept of\\nuniﬁed data schemas for diﬀerent tasks. The authors ﬁne-\\ntune T5 to build T0, an encoder-decoder model trained on a\\nmultitask mixture that generalizes to more tasks. One of the\\nexciting highlights is that the more tasks represented in the\\ndata, the higher median performance the model achieves\\nwhile not decreasing variability.\\nThis was later expanded with Super-NaturalInstructions:\\nGeneralization via Declarative Instructions on 1600+ NLP\\nTasks (October 2022) , a new dataset of over 1600 tasks\\nwith 5 million examples. The diﬀerence in these projects is\\n1 3 \\n1 4 \\n1 5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='how the datasets were generated. T0 retroactively builds\\ninstructions based on already-available task instances,\\nwhile Natural Instructions had NLP researchers make\\ninstructions and crowd workers built dataset instances.\\nFigure 6-4. Figure 6-5. With instruction-tuning, we can format many labeled datasets\\nas generation tasks.\\nAn alternative approach is to generate outputs using LLMs:\\nUnnatural Instructions (December 2022)  is a dataset of\\nautomatically generated examples based on seed examples\\nand asking for a fourth. The dataset is augmented by asking\\nthe model to rephrase each instruction.\\n1 6 \\n1 7'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Self-Instruct (May 2023)  bootstraps oﬀ the language\\nmodels’ own generation. The idea is to have a model that\\ngenerates the instruction, then the input (conditioned on\\nthe instruction), and ﬁnally the output.  Synthetically\\ngenerated datasets tend to contain more noise. They can\\nlead to a model that is less robust than a model trained with\\nless but better curated human-generated data.\\nLIMA (May 2023)  is a much smaller English instruction\\ndataset. Although it only has a thousand instances, the\\nauthors were able to ﬁne-tune a robust LLaMA model. This\\nwas achieved thanks to a strong pretrained model and by\\nvery careful curation of training data.\\nThese are just some of the massive explosion of instruct-tuned\\nmodels. Flan-T5 is a ﬁne-tuned T5 model using the FLAN\\ndataset. Alpaca is an LLaMA ﬁne-tune on an instruction dataset\\ngenerated by InstructGPT. WizardLM is a LLaMA instruct-tune\\non the Evol-Instruct dataset. ChatGLM2 is a ﬁne-tuned bilingual\\nmodel trained on English and Chinese instructions. We keep\\nﬁnding the same formula of combining a strong base model\\nwith a diverse dataset of instruction data (that can be human or\\nmodel-generated).\\nLearning to Generate Task-Speciﬁc Adapters from Task\\nDescription (June 2021)  is a diﬀerent approach to improving\\n1 7 \\n1 8 \\n1 9 \\n2 0'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='generalization abilities. Rather than aiming for a general\\nnetwork for all tasks, the authors generate task-speciﬁc\\nparameters called adapters. Although adapters have existed for\\nyears, their adoption has recently become widespread in\\nnatural language and image generation. In language models\\nwith billions of parameters, many people want to ﬁne-tune for\\ntheir domain or task. The next section is all about adapters.\\nTo re-cap this section, the two main components for instruct-\\ntuning are a robust base model and a high-quality instructions\\ndataset. The quality of the instructions dataset is,\\nunsurprisingly, key for the model. This dataset can be either\\nsynthetically generated (e.g., using self-instruct), manually\\ngenerated, or a mix of both. Consistently, research has shown\\nthat the more tasks represented in the training data, the better\\nthe model is. Finally, the instruction template can impact the\\nﬁnal performance a lot. Existing datasets end up trading oﬀ\\nbetween quantity and diversity of tasks.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 6-5. Figure 6-6. Instruct tuning examples.\\nA Quick Introduction to Adapters\\nLet’s now dive into the fourth approach: adapters. So far, we’ve\\nexplored ﬁne-tuning DistilBERT for text classiﬁcation and\\nSmolLM to generate text in our speciﬁc style. In both cases, all\\nweights of the model were modiﬁed during ﬁne-tuning. Fine-\\ntuning is much more eﬃcient than pre-training as we don’t\\nneed too much data or compute power. However, as the trend of\\nlarger models keeps growing, doing traditional ﬁne-tuning\\nbecomes infeasible on consumer hardware. Additionally, if we\\nwant to ﬁne-tune an encoder model for diﬀerent tasks, we’ll'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='end up with multiple models, multiplying the storage and\\ncompute requirements.\\nWelcome: PEFT! Parameter-eﬃcient ﬁne-tuning, called PEFT, is\\na group of techniques that enables adapting the pre-trained\\nmodels without ﬁne-tuning all the model parameters. Typically,\\nwe add a small number of extra parameters, called adapter, and\\nthen ﬁne-tune them while freezing the original pre-trained\\nmodel. What eﬀects does this have?\\nFaster training and lower hardware requirements:\\nWhen doing traditional ﬁne-tuning, we update many\\nparameters. With PEFT, we only update the adapter, which\\nhas a small percentage of parameters compared to the base\\nmodel. Hence, training is completed much faster and can be\\ndone with smaller GPUs.\\nLower storage costs: After ﬁne-tuning the model, we only\\nneed to store the adapter instead of the whole model for\\neach ﬁne-tuning. When some models can take over 100Gb\\nto store, it won’t scale well if each downstream model\\nrequires saving all the parameters again. An adapter could\\nbe 1% of the size of the original model. If we have 100 ﬁne-\\ntunes of a 100Gb model, traditional ﬁne-tuning would take\\n10,000Gb of storage while PEFT would take 200Gb (the\\noriginal model and 100 adapters of 1Gb each).'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Comparable performance: The performance of the PEFT\\nmodels tends to be comparable to the performance of fully\\nﬁne-tuned models\\nNo latency hit: As we’ll discuss soon, after training, the\\nadapter can be merged into the pre-trained model, meaning\\nthe ﬁnal size and inference latency will be the same.\\nThis sounds too good to be true. How does it work? There are\\nmultiple PEFT methods. Amongst the most popular ones are\\npreﬁx tuning, prompt tuning, and low-rank adaptation (LoRA),\\nwhich we’ll focus on in this chapter. LoRA represents the weight\\nupdates with two smaller matrices called update matrices using\\nlow-rank decomposition. Although this can be applied to all\\nblocks in the transformers models, we usually apply them only\\nto attention blocks.\\nPEFT is a simple library to use these techniques with\\ntransformers and diﬀusers. To start, let’s discuss how to build an\\nadapter of the SmolLM model from the previous section. In the\\ncase of LoRA, we can control multple things, such as:\\nThe rank r: This controls the size of the update matrices.\\nA larger rank allows the adapter to learn more complex\\npatterns but requires more parameters.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='lora_alpha: it scales the update matrices. For example, if\\nlora_alpha is 32 and r is 8, the gradient updates will be\\nscaled by 4. This is similar to using a higher learning rate\\nduring training.\\nlora_dropout: The dropout probability for LoRA layers,\\nwhich can help with overﬁtting.\\ntask_type: The task type, such as SEQ_CLS (sequence\\nclassiﬁcation) or CAUSAL_LM (causal language model). This\\nwill determine the adapter’s architecture.\\nuse_dora: DoRA is a variant of LoRA that works\\nparticularly well to match the performance of full ﬁne-\\ntuning. We won’t use it in this example, but it’s good to\\nknow it exists.\\nThe update matrix in LoRA isn’t just any matrix. It’s a special\\nkind called a low-rank matrix. Imagine you have a huge matrix\\nwith lots of information. The idea behind low-rank matrices is\\nto summarize it using fewer rows and columns without losing\\nimportant information. For those interested in the (very high-\\nlevel) math behind LoRA, the update matrix is represented with\\na low-rank decomposition\\nh=W0x+ΔW=W0x+ α\\nrBAx\\nwhere W0 is the original weight and x is the input.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='In LoRA, the update ΔW is expressed as the product of two\\nlow-rank matrices B and A, with B having fewer rows and A\\nhaving fewer columns than the original weight matrix. The\\nscaling factor αr controls the magnitude of this update. The\\ndimensions of B is d×r and the dimensions of A is r×k,\\nwhere d and k are the rows and columns of the original\\nweights.\\nLet’s go into code. The ﬁrst step is creating a conﬁguration of a\\nPEFT method\\nfrom peft import LoraConfig, get_peft_model\\npeft_config = LoraConfig(\\n    r=8, lora_alpha=32, lora_dropout=0.05, \\ntask_type=\"CAUSAL_LM\"\\n)\\nmodel = \\nAutoModelForCausalLM.from_pretrained(\"HuggingFace\\n135M\")\\npeft_model = get_peft_model(model, \\npeft_config)\\npeft_model.print_trainable_parameters()'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='trainable params: 460,800 || all params: \\n134,975,808 || trainable%: 0.3414\\nThe initial model has almost 135 million parameters, but only\\nabout 460 thousand would be trained. That’s just 0.34% of the\\nsize of the original model. The idea behind PEFT is that we can\\ntrain this small adapter and get similar performance to the 300\\ntimes larger model.\\nHow does it work under the hood? When you ﬁne-tune a base\\nmodel, you’re updating all the layers. As discussed, LoRA\\napproximates these update matrices with two smaller ones. For\\nexample, let’s assume there’s a single update matrix with 10,000\\nrows and 20,000 columns. That means it contains 200 million\\nvalues. Let’s assume we do LoRA with a rank of 8. The ﬁrst\\nmatrix, A, would have 10,000 rows and 8 columns, while matrix\\nB would have 8 rows and 20,000 columns (to ensure same\\ninput and output sizes). A has 80,000 values and B has 160,000.\\nWe went from 200 million values to 240,000 values. That’s 800\\ntimes smaller! LoRA assumes that these matrices can\\napproximate well enough the weight update matrices.\\nWe talked about the r parameter. As mentioned, it controls the\\ndimension of the LoRA matrices, which leads to a trade-oﬀ\\nbetween capabilities and overﬁtting. A rank that is too high will\\n2 1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='lead to adapters that are too complex and prone to overﬁtting.\\nA rank that is too low may result in underperformance because\\nthe model won’t be able to capture enough complexity. The\\nsecond key parameter is alpha, which controls how much the\\nadapters impact the original model. A higher alpha gives\\nmore importance to the adapter. Picking the values of r and\\nalpha depends on the problem and model. A good starting\\npoint for LLMs is to use a rank of 8 and consistently use an\\nalpha twice as large as the rank.\\nAfter ﬁne-tuning, we can merge the LoRA weights back into the\\noriginal model, as shown in Figure 6-7. This means that the\\nlatency and the amount of compute needed to run inference\\nwith a model is exactly the same with or without a merged\\nLoRA adapter.\\nweight=weight+scaling×(B×A)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 6-6. Figure 6-7. LoRA reduces the number of trainable weights. Once trained,\\nthe LORA weights can be merged back into the original model.\\nOne exciting thing about LoRAs being so small is that they\\nbecome very portable and practical for production. Imagine a\\nuse case in which the users expect a chatbot or an image\\ngenerator to generate in 10 diﬀerent styles unknown by the\\ninitial model. Rather than ﬁne-tuning the initial model ten\\ntimes and loading the models ad-hoc, we can load and unload\\nthe adapter as needed. Recent techniques, such as LoRAX ,\\nallow to serve over a hundred ﬁne-tuned adapters on a single\\nGPU.\\nThere are also use cases where you might want to merge the\\nadapters. Just as we updated a single adapter, we can keep\\ndoing so with multiple.\\nweight=weight+scaling1×(B1×A1)\\n2 2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='weight=weight+scaling2×(B2×A2)\\nweight=weight+scaling3×(B3×A3)\\nOne last question is which parameters to update with LoRA.\\nUsing LoRA in more blocks often leads to slightly better\\nperformance at the expense of more extensive memory\\nrequirements during training, which may be a worthwhile\\ntrade-oﬀ and can be done with the target_modules\\nparameter. We can use LoRA in the attention blocks for quick\\nexperimentation, which is usually the default of the PEFT\\nlibrary.  You can also use target_modules=\"all-linear\"\\nto choose all the linear modules, excluding the output blocks.\\nAlthough in this chapter we’re focusing on text generation ﬁne-\\ntuning, PEFT is widely used in other domains such as image\\ngeneration (which we’ll explore in Chapter 7), image\\nsegmentation, and more.\\nA Light Introduction to Quantization\\nPEFT allows us to ﬁne-tune models with less compute and disk\\nspace. However, the size of the model during inference is not\\ndecreased. If you’re doing inference of a model with 30 billion\\nparameters, you will still need a powerful GPU to run it. For\\nexample, a 405B model such as Llama would require more than\\n2 3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='8 A100 GPUs, which are pretty powerful and expensive (each\\none costs over $15k). In this section, we’ll discuss a technique\\nthat will allow us to run the models with smaller GPUs in a way\\nthat does not degrade their performance.\\nEach of those parameters has a data type or precision. For\\nexample, the float32 (FP32, also called full precision) type\\nstores a ﬂoat number with 32 bits. FP32 allows the\\nrepresentation of a wide range of numbers with high precision,\\nwhich is important for pre-training models. In many cases,\\nthough, such a wide range is not required. In those cases, we\\ncan use float16 (or FP16, also called half-precision). FP16\\nhas less precision and a lower range of numbers (the largest\\nnumber possible is 64,000), which introduces new risks: a\\nmodel can overﬂow (if a number is not within the range of\\nrepresentable numbers). During inference, though, using FP16\\nis ﬁne; the risks of half precision are just signiﬁcant during\\ntraining. A third data type is Brain Floating-Point, or\\nbfloat16. BF16 uses 16 bits just like FP16, but allocates\\nthose bits in a diﬀerent way in order to gain more precision for\\nsmaller numbers (like those typically found in neural network\\nweights) while still covering the same total range as FP32.\\nLet’s say we have a model of 7 billion parameters. 7 billion\\nparameters, each being 32 bits, leads to 224 billion bits. 224'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='billion bits is 28 billion bytes, or ~26 Gigabytes. If we used hald\\nprecision, we would only need 13 Gigabytes. This is a signiﬁcant\\nreduction in memory usage, which can lead to faster inference\\nand lower costs. Appendix B of the book discusses memory\\nrequirements for diﬀerent models and precisions.\\nTest your knowledge:\\nHow much memory would a 135M model in half precision\\ntake?\\nHow much memory would a 405B model in half precision\\ntake?\\nFigure 6-7. Figure 6-8. Diﬀerent precisions.\\n2 4 \\n2 5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Using full precision for training and inference usually leads to\\nthe best results, but it’s signiﬁcantly slower. For training, people\\nhave found ways to do mixed-precision training, which oﬀers a\\nsigniﬁcant speedup. In mixed-precision training, the weights\\nare maintained in full precision as reference, but the operations\\nare done in half-precision. The half-precision updates are used\\nto update the full-precision weights.\\nThe precision does not signiﬁcantly impact inference, so we can\\nload the model with half-precision. PyTorch loads all models in\\nfull precision by default, so we need to specify the type when\\nloading a model by passing the torch_dtype if we want to use\\nfloat16 or bfloat16.\\nmodel = \\nAutoModelForCausalLM.from_pretrained(\"gpt2\", \\ntorch_dtype=torch.float16)\\nLoading the 7B model with 16 bits rather than 32 bits per\\nparameter will require 13 Gigabytes of GPU for a 7B model,\\nwhich might work well for some consumer GPUs. ~7B models\\nsuch as Llama, Mistral, and Gemma have become popular\\nsolutions for consumer GPUs, but there are compelling models\\nwith even more parameters. For example, if we want to use a\\n34B model in half-precision, we would need a 68GB GPU, far'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from any consumer GPU. Is there anything we can do to use\\nthese models?\\nIntuitively, we could think of naively reducing the range or the\\nprecision of the numbers to reach a quarter-precision (using a\\nsingle byte, 8 bits per parameter). Unfortunately, doing so\\nwould lead to signiﬁcant performance degradation. We can\\nachieve quarter precision thanks to 8-bit quantization. The idea\\nbehind 8-bit quantization techniques is to map a value from\\none type (e.g., fp16) into an int8, which would represent\\nvalues in the [-127, 127] or [0, 256] range.\\nThere are diﬀerent 8-bit quantization techniques. Let’s explore\\nthe simple absmax quantization. Given a vector, we ﬁrst\\ncompute its maximum absolute value. We then divide 127 (the\\nlargest possible value) by this maximum value. This leads to a\\nquantization factor - when we multiply the vector by this factor,\\nwe guarantee that the largest value will be 127. We can\\ndequantize the array to retrieve the original numbers, but some\\ninformation will be lost. This is better understood by running\\nsome code:\\nimport numpy as np'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='def scaling_factor(vector):\\n    # Get largest value of vector\\n    m = np.max(np.abs(vector))\\n    # Return scaling factor\\n    return 127 / m\\narray = [1.2, -0.5, -4.3, 1.2, -3.1, 0.8, \\n2.4, 5.4, 0.3]\\nalpha = scaling_factor(array)\\nquantized_array = np.round(alpha * \\nnp.array(array)).astype(np.int8)\\ndequantized_array = quantized_array / alpha\\nprint(f\"Scaling factor: {alpha}\")\\nprint(f\"Quantized array: {quantized_array}\")\\nprint(f\"Dequantized array: \\n{dequantized_array}\")\\nprint(f\"Difference: {array - \\ndequantized_array}\")\\nScaling factor: 23.518518518518515\\nQuantized array: [  28  -12 -101   28  -73   \\n19   56  127    7]\\n(\\'Dequantized array: [ 1.19055118 -0.51023622 \\n-4.29448819  1.19055118 \\'\\n \\'-3.10393701  0.80787402\\\\n\\''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"'  2.38110236  5.4         0.2976378 ]')\\n('Difference: [ 0.00944882  0.01023622 \\n-0.00551181  0.00944882  0.00393701 '\\n '-0.00787402\\\\n'\\n '  0.01889764  0.          0.0023622 ]')\\nThese diﬀerences will lead to performance degradation.\\nBecause of this, classic quantization techniques have failed at\\nscale with models of billions of parameters. LLM.int8() is a\\ntechnique allowing us to do 8-bit quantization without\\ndegradation. The idea behind this technique is to extract\\noutliers, i.e.\\xa0values beyond some bounds, and compute matrix\\nmultiplication of those outliers in FP16 while using int8 for\\nthe rest. This mixed-precision structure allows us to manage\\n99.9% of the values in 8-bit and 1% in full or half-precision, and\\nhave no performance degradation. What’s the catch? The main\\ngoal of LLM.int8() is to reduce the requirement of huge GPUs\\nto run model inference. Given the additional conversion\\noverhead, doing inference will be slower (15-30% slower) than\\nusing fp16. One additional thing to note is that although all\\nthe GPUs from recent years provide tensor cores for int8,\\nsome older GPUs might not have good support for this.\\nThe boundaries of low-precision inference are being pushed\\nwith new 4-bit and 2-bit quantization techniques. There are\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='even explorations of using sub-1-bit quantization. Achieving\\nquantization with no degradation is a research area of\\ntremendous interest, given the trends of models becoming\\nlarger and larger. At the beginning of this section, we needed\\n28GB GPU to load a model with 7B parameters. We can now\\nload the same model with 7GB and no quality degradation at\\nthe cost of inference speed (but not too much).\\nThe transformers library has integration with diﬀerent\\nquantization methods such as AWQ, GPTQ, and 4-bit and 8-bit\\nwith bitsandbytes. .\\nLoading the model in 8 bits is as easy as creating a\\nBitsAndBytesConfig and specifying load_in_8bit. You\\ncan then pass this to the model when loading it.\\nfrom transformers import \\nAutoModelForCausalLM, BitsAndBytesConfig\\nquantization_config = \\nBitsAndBytesConfig(load_in_8bit=True)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"gpt2\", \\nquantization_config=quantization_config\\n)\\n2 6'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Apart from quantization, we can do a couple of other things to\\nwork with very large models. One popular inference technique\\nis called oﬄoading. If a model is too large to ﬁt into your GPU,\\nyou can split it into multiple checkpoint shards, which are\\nautomatically handled by transformers. What beneﬁts does it\\nhave? If a model is too large, we can load only the layers or\\nshards that ﬁt and oﬄoad the other operations into your CPU\\nRAM, which is much slower. This allows us to work with any\\nmodel size but at an inference speed cost that is not usable for\\nmany large models. If a model is so large that it won’t ﬁt into\\nyour CPU RAM, you can oﬄoad the model to disk, which is even\\nslower but should allow you to work with any model size (as\\nlong as it ﬁts your disk).'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 6-8. Figure 6-9. Model oﬄoading.\\nAll Together\\nLet’s review PEFT and quantization.\\nPEFT allows us to ﬁne-tune models using much less\\ncompute by adding adapters and freezing the base model\\nweights. This accelerates training, given only a few weights\\nare updatable.\\nQuantization allows us to load a model using fewer bits\\nthan those used for storage. This reduces the GPU\\nrequirements to load and run inference with a model.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Why not both? Let’s imagine we train a model in 8-bits.\\nUnfortunately, as discussed in the previous section, having high\\nprecision is important when pre-training or ﬁne-tuning large\\nmodels. On the other hand, PEFT freezes the base model and\\nonly uses a small adapter, so we could aim to use lower\\nprecision here while achieving the same performance.\\nQLoRA allows us to ﬁne-tune large models with smaller GPUs.\\nThis technique is very similar to LoRA but with quantization.\\nFirst, the base model is quantized into 4-bits and frozen. Then,\\nthe LoRA adapters (the two matrices) are added and kept in\\nbfloat16. When ﬁne-tuning, QLoRA uses the 4-bit storage\\nbase model and the half-precision 16-bit model to perform\\ncomputations.\\nLoading a model in 4 bits just requires changing to the\\nload_in_4bit parameter when you create the\\nBitsAndBytesConfig. Let’s try this with Mistral 7B, which is a\\nsolid base model choice. Due to this model being quite large\\ncompared to the previous ones, we’ll also specify\\ndevice_map=\"auto\". It will automatically try to ﬁll all the\\nspace in your GPUs and then oﬄoad weights to the CPU if the\\nmodel does not ﬁt in the GPU (which would be much slower to\\nrun, but the model would still load).'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='One ﬁnal detail before using Mistral: this model repository is\\ngated, which means that the authors require explicit consent of\\ntheir license terms. To use it, you have to visit the model page in\\nHugging Face while being logged in to your account, read the\\nlicense, and click the button to accept it if you agree to the\\nterms. To access the model programmatically, like in the code\\nsnippet that follows, you have to be authenticated with the\\nsame account. The easiest way to do it is to install the\\nhuggingface_hub Python package (it comes with\\n+transformers+) and run huggingface-cli login in a\\nterminal session. You’ll be asked for an access token that you\\ncan create on your settings page. If you are downloading the\\nmodel from a Google Colab session, you can set up the\\nHF_TOKEN secret and give permission to your notebook to use\\nit.\\nquantization_config = \\nBitsAndBytesConfig(load_in_4bit=True)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"mistralai/Mistral-7B-v0.3\",\\n    quantization_config=quantization_config,\\n    device_map=\"auto\",\\n)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='BitsAndBytesConfig allows more ﬁne-grained control of the\\nquantization techniques by using additional arguments to\\nchange the compute type, apply nested quantization, and more.\\nQLoRA is just a tool in our toolbox, not a golden bullet. It\\nsigniﬁcantly reduces the GPU requirements while maintaining\\nthe same performance, but it also increases the training time it\\nwill take to train the model. All beneﬁts of the PEFT section\\nhold, making QLoRA a popular technique in the community to\\nquickly ﬁne-tune 7B models.\\nLet’s do a QLoRA ﬁne-tune to make a generative model that can\\ndo simple conversations. Let’s go over each component:\\nThe base model: We’ll use the Mistral model. Mistral is a\\nvery high-quality 7B model. We load the model with\\nload_in_4bit and device_map=\"auto\" to do 4-bit\\nquantization.\\nThe dataset: We’ll use the Guanaco dataset, which contains\\n10,000 high-quality conversations between humans and the\\nOpenAssistant model.\\nPEFT conﬁguration: We’ll specify a LoraConfig with\\ngood initial defaults: a rank (r) of 8 and alpha being\\ndouble its value.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Training arguments: Just as before, we can conﬁgure\\ntraining parameters (such as how often to evaluate and\\nhow many epochs) as well as model hyperparameters\\n(learning rate, weight decay, or number of epochs).\\nIn the previous examples we used the TrainingArguments\\nand Trainer, two general-purpose tools from transformers.\\nWhen ﬁne-tuning an LLM for autoregressive techniques, the trl\\nlibrary SFTConfig and SFTTrainer classes are useful tools.\\nThey are wrappers around the TrainingArguments and\\nTrainer optimized for text generation. For example, they can\\nhandle:\\nEasy dataset loading and processing tools. Rather than\\nhaving to process the dataset ourselves, we can use\\ndataset_text_field to specify the ﬁeld containing the\\ntraining data. Additionally, we can use packing to\\nconcatenate multiple sequences, which is useful for\\neﬃcient batch processing.\\nSupports common prompt templates for conversations and\\ninstructions out-of-the-box.\\nYou can directly pass any PeftConfig to the SFTTrainer\\nto use PEFT techniques.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='As before, we can pass the now quantized model and the\\ndataset (we’ll just pass 300 samples for fast training).\\nSFTTrainer already comes with useful default collators and\\ndataset utilities, so tokenizing and pre-processing the data is\\nunnecessary.\\nfrom trl import SFTConfig, SFTTrainer\\ndataset = \\nload_dataset(\"timdettmers/openassistant-\\nguanaco\", split=\"train\")\\npeft_config = LoraConfig(\\n    r=8,\\n    lora_alpha=16,\\n    lora_dropout=0.05,\\n    task_type=\"CAUSAL_LM\",\\n)\\nsft_config = SFTConfig(\\n    \"fine_tune_e2e\",\\n    push_to_hub=True,\\n    per_device_train_batch_size=8,\\n    weight_decay=0.1,\\n    lr_scheduler_type=\"cosine\",\\n    learning_rate=5e-4,\\n    num_train_epochs=2,\\n2 7'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='eval_strategy=\"steps\",\\n    eval_steps=200,\\n    logging_steps=200,\\n    gradient_checkpointing=True,\\n    max_seq_length=512,\\n    # New parameters\\n    dataset_text_field=\"text\",\\n    packing=True,\\n)\\ntrainer = SFTTrainer(\\n    model,\\n    args=sft_config,\\n    train_dataset=dataset.select(range(300)),\\n    peft_config=peft_config,\\n)\\ntrainer.train()\\ntrainer.push_to_hub()\\nThe previous code might take about an hour or more to run.\\nRemember, QLoRA leads to slower training as well.\\nWhile the model trains, it’s a good opportunity to read more\\nabout the dataset. If you visit the dataset page, you will notice it'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='has the following format:\\n### Human: Can you write a short introduction \\n....### Assistant: \"Monopsony\"\\nrefers to a market ..### Human: Now explain \\nit to a dog\\nEach turn begins with # Human:, followed by a space, and\\nthen the human input.\\nThe model’s response begins with # Assistant:,\\nfollowed by a space, and then the model’s output.\\nThere can be many turns.\\nWhen you ﬁne-tune a model for conversational tasks, it’s\\ncommon to have a chat template. All the details are essential.\\nAdding a new line in the chat, removing a space, or having an\\nadditional # can degrade the model’s generation. These\\nexpectations come from the training format that was used\\nduring training. Similarly, if a model is trained only with single-\\nturn conversations, it will struggle to generate high-quality\\nmulti-turn generations. Knowing the prompt format is\\nimportant as we’ll need to use it to generate high-quality\\nconversations.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Once the training is done, let’s proceed to using the model.\\nWhen we pushed the model, we just pushed the adapter. Let’s\\nrun inference with the model and the adapter.\\n# We load the base model just as before\\ntokenizer = \\nAutoTokenizer.from_pretrained(\"mistralai/Mistral-\\n7B-v0.3\")\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"mistralai/Mistral-7B-v0.3\",\\n    torch_dtype=torch.float16,\\n    device_map=\"auto\",\\n)\\n# You can load the adapter with \\n`load_adapter`\\n# Then load the model with `from_pretrained`.\\nmodel.load_adapter(\"genaibook/fine_tune_e2e\")  \\n# change with your adapter name\\n# Alternatively, you could just use \\n`from_pretrained` with the adapter name and \\nit\\n# will automatically take care of loading the \\nbase and adapter models.\\n# model = \\nAutoModelForCausalLM.from_pretrained(\"genaibook/f'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='pipe = pipeline(\"text-generation\", \\nmodel=model, tokenizer=tokenizer)\\npipe(\"### Human: Hello!### Assistant:\", \\nmax_new_tokens=100)\\nThe code above would output something like this:\\n### Human: Hello\\n### Assistant: Hello! How can I help you?\\n### Human: I want to know how to make a \\nwebsite.\\n### Assistant: Sure! Here are some steps to \\nhelp you get started...\\nImpressive, we just ﬁne-tuned a 7B model to make it\\nconversational without needing a huge GPU.\\nAs conversational models became more common, Hugging Face\\ntransformers added a way for model creators to specify the\\nchat_template. Thanks to this, end users don’t need to worry\\nso much about the prompt template and can instead focus on\\nthe content of the conversation. For example, you can simply\\npass the messages to the model and the tokenizer will take care\\nof formatting them automatically.\\n2 8'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='pipe = pipeline(\\n    \"text-generation\", \"HuggingFaceTB/SmolLM-\\n135M-Instruct\", device=device\\n)\\nmessages = [\\n    {\\n        \"role\": \"system\",\\n        \"content\": \"\"\"You are a friendly \\nchatbot who always responds\\n        in the style of a pirate\"\"\",\\n    },\\n    {\\n        \"role\": \"user\",\\n        \"content\": \"How many helicopters can \\na human eat in one sitting?\",\\n    },\\n]\\nprint(pipe(messages, max_new_tokens=128)[0]\\n[\"generated_text\"][-1])\\n{\\'content\\': \\'The number of helicopters that \\ncan be eaten in one \\'\\n            \\'sitting depends on the number of \\npeople in the room. If \\'\\n            \\'there are 10 people, then there \\nare 10 helicopters that \\'\\n            \\'can be eaten in one sitting. If'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='there are 15 people, \\'\\n            \\'then there are 15 helicopters \\nthat can be eaten in one \\'\\n            \\'sitting. If there are 20 people, \\nthen there are 20 \\'\\n            \\'helicopters that can be eaten in \\none sitting.\\\\n\\'\\n            \\'\\\\n\\'\\n            \\'The number of helicopters that \\ncan be eaten in one \\'\\n            \\'sitting depends on the number of \\npeople in the room. If \\'\\n            \\'there are 10 people, then there \\nare 10 helicopters\\',\\n \\'role\\': \\'assistant\\'}\\nIf you just want to apply the chat template but not pass it to a\\nmodel, you can use the tokenizer.apply_chat_template()\\nmethod directly.\\ntokenizer = \\nAutoTokenizer.from_pretrained(\"HuggingFaceTB/Smol\\n135M-Instruct\")\\nchat = [\\n    {\"role\": \"user\", \"content\": \"Hello, how \\nare you?\"},'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='{\\n        \"role\": \"assistant\",\\n        \"content\": \"I\\'m doing great. How can \\nI help you today?\",\\n    },\\n    {\\n        \"role\": \"user\",\\n        \"content\": \"I\\'d like to show off how \\nchat templating works!\",\\n    },\\n]\\ntokenizer.apply_chat_template(chat, \\ntokenize=False)\\nWe can use print() to get the full prompt, but take into\\naccount that the original string passed to the model contains\\ncharacters such as ++ to mark new lines.\\nprint(tokenizer.apply_chat_template(chat, \\ntokenize=False))\\n<|im_start|>user\\nHello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI\\'m doing great. How can I help you today?'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"<|im_end|>\\n<|im_start|>user\\nI'd like to show off how chat templating \\nworks!<|im_end|>\\nThis will work for models where the chat_template is\\nspeciﬁed in the tokenizer_config.json. To learn more\\nabout chat templates, we suggest reading the oﬃcial\\ndocumentation.\\nA Deeper Dive into Evaluation\\nHow can we evaluate the quality of the generated text? So far,\\nwe discussed popular benchmarks that evaluate general\\nknowledge and reasoning, but you might wonder how to\\nevaluate the quality of the generated text in a more general\\ncontext. The ﬁrst thing to diﬀerentiate is the evaluation of base\\nmodels vs.\\xa0ﬁne-tuned or chat models. The expectations are\\ndiﬀerent for each, so we shouldn’t evaluate them in exactly the\\nsame way. For example, we should not expect a base model to\\nhave instruct or chat capabilities out of the box, so evaluating it\\non these tasks would be unfair.\\nLet’s begin discussing some ways to evaluate base models:\\n2 9\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Perplexity: Perplexity measures how well a language\\nmodel predicts a given dataset. A lower perplexity value\\nindicates better performance and less uncertainty in\\ngenerating text, suggesting that the model can predict the\\nnext word more accurately. Perplexity is particularly\\nrelevant during the training phase of base models, as it\\nreﬂects the model’s ability to learn eﬀective probability\\ndistributions over word sequences.\\nBLEU: BLEU measures the similarity between the\\ngenerated text and the reference text. It does so by\\ncalculating the proportions of n-grams in the generated text\\nthat are also present in the reference text. Given that BLEU\\nheavily relies on exact n-gram matches, it can fail to\\ncapture the diversity of natural language and also lacks\\nsemantic understanding.\\nROUGE: Similar to BLEU, ROUGE measures the overlap\\nbetween two texts. However, ROUGE focuses on recall\\nrather than precision, making it very useful for tasks such\\nas summarization. However, it still lacks semantic\\nunderstanding and tends to be biased toward longer\\noutputs.\\nAs you can see, evaluating base models is not straightforward.\\nDuring training, loss and perplexity are usually tracked with'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='the expectation that both decrease over time. ROUGE and BLEU\\nare often used with datasets that have reference text.\\nAs you can see, evaluating base models is not straightforward.\\nDuring training, loss and perplexity are usually tracked with\\nthe expectation that both decrease over time, reﬂecting better\\nmodel learning. For tasks involving structured outputs like\\ntranslation or summarization, metrics like ROUGE and BLEU\\nare commonly used when reference text is available. However,\\nboth metrics rely on exact matches and overlook semantic\\nsimilarities, making them limited when evaluating more\\ncreative or diverse text generation.\\nNOTE\\nA recent work, Urial,  states that most of the gains we see in instruct-tuning actually\\ncome from the base model. The authors analyze the token distribution shift between\\nbase and instruct-tuned models and ﬁnd that there is very little shift in the majority\\nof tokens. The shifts that do occur are mostly stylistic, involving tokens like greetings\\nand disclaimers expected in conversational models. This ﬁnding suggests that base\\nmodels already possess much of the knowledge needed to follow instructions,\\nhighlighting the importance of pre-training. There’s also potential for tuning-free\\nmethods (which don’t require ﬁne-tuning) to achieve similar performance to\\ninstruct-tuning, though this area is still under active research.\\nWhile these metrics provide a quantitative measure, qualitative\\nevaluation, including human judgment, is also crucial to gauge\\n3 0'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='the overall coherence and relevance of the generated text to the\\nintended task. Quantitative metrics are useful for large-scale\\ncomparisons, but they might miss subtleties like ﬂuency,\\ncreativity, or context appropriateness. This is where human\\nevaluation shines. Balancing both quantitative and qualitative\\nassessments ensures a more comprehensive evaluation of text\\ngeneration models.\\nFor end-user generative models (such as chat models), one of\\nthe best things to do is play with the model. There are also\\npopular arenas, such as LMSYS, where users interact with\\ndiﬀerent anonymized models and pick the best results. The\\nresults are then aggregated in a leaderboard that ranks the\\nmodels, shown in Figure 6-10. These arenas are better than\\nautomated leaderboards as they reﬂect performance closer to\\nreal-world usage. Unfortunately, getting arena scores is\\nexpensive and, many times, not possible for a brand-new\\nmodel. In such cases, benchmarks such as MT Bench, IFEval, EQ\\nBench, and AGIEval have some correlation to arena scores and\\nhence can be useful to get a rough idea of how well a model will\\nperform in the real world. New benchmarks are coming out\\nfrequently, so keeping current with the latest research is\\nimportant.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 6-9. Figure 6-10. A screenshot of the LMSYS Leaderboard\\nhttps://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\\nBenchmarks, while valuable tools, also have their limitations.\\nFor instance, knowledge-based benchmarks often exhibit a US-\\ncentric bias, with questions about US history and law, as can be\\nfound in MMLU. Additionally, most of these benchmarks are\\nEnglish-based, and very few benchmarks are available for other\\nlanguages. Some community eﬀorts are underway to translate\\nthese benchmarks, but progress is slow and the translations\\nmay not be perfect. Furthermore, chat benchmarks (and often\\narenas as well) tend to focus more on single-turn conversations\\nrather than multi-turn conversations. Evaluating very long\\ncontext models remains a challenge, particularly in a chat\\nsetting, and is an area that is yet to be resolved.\\nThe most important takeaway is to test the model on the task\\nyou want to address. Benchmarks are useful for picking an'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='initial base model for ﬁne-tuning or general-purpose chat\\nmodels, but they are not a substitute for real-world testing.\\nProject Time: Retrieval Augmented\\nGeneration (RAG)\\nLLMs can only use information based on their context and the\\ndata used to train them. If you want to ask an LLM for\\ninformation about a speciﬁc topic, it will only know the answer\\nif it is part of its data. For example, if you try to ask Llama about\\nnew movies, it will struggle to provide accurate information.\\nRAG is a technique in which the model can access information\\n(e.g.\\xa0paragraphs or documents) stored somewhere. With RAG,\\nthe LLM uses both the user input and the stored information to\\ngenerate a response. This approach is very powerful as it allows\\nthe model to access a large amount of information, which is\\neasier to update than to retrain the model.\\nUnfortunately, there might be millions of documents, so you\\ncan’t just pass all of them to the model. To solve this, we use an\\nembedding model (such as the sentence transformers from\\nChapter 1) to encode each document into a vector, and we store\\nthese vectors (usually into something called a vector database).\\nWe then use a nearest neighbor search to ﬁnd the documents'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='most similar to the user input. Finally, we pass the user input\\nand the retrieved documents to the LLM. This approach is\\ncompelling as it allows the model to access a large amount of\\ninformation as needed.\\nYour goal is to build a pipeline in which:\\n1. The user inputs a question\\n2. The pipeline retrieves the most similar documents to the\\nquestion\\n3. The pipeline passes the question and the retrieved\\ndocuments to the LLM\\n4. The pipeline generates a response\\nFigure 6-10. Placeholder. A RAG pipeline.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='You won’t need to train any model for this task. For retrieval,\\nwe suggest using a sentence_transformers pre-trained model.\\nFeel free to use your favorite model, e.g., Mistral or LLama, for\\ngenerations. You will notice that 1 and 2 were solved in the\\nchallenge of Chapter 1, and 3 and 4 were solved in this chapter.\\nThe challenge is to put all these pieces together. You can use the\\nfollowing functions to guide you\\ndef embed_documents(documents: List[str]):\\n    # Use a sentence transformer model to \\nencode the documents\\n    # Store the documents somewhere\\ndef retrieve_documents(query: str):\\n    # Use the stored documents to retrieve \\nthe most similar documents to the query\\ndef generate_response(query: str, documents: \\nList[str]):\\n    # Use the LLM to generate a response\\ndef pipeline(query: str):\\n    documents = retrieve_documents(query)\\n    response = generate_response(query,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='documents)\\n    return response\\nWhat documents to use? That’s up to you, but we recommend\\nbeginning with a very minimal setup (i.e., pick 5-10 sentences\\nor paragraphs, potentially crafted by yourself), and then you\\ncan scale up to more documents.\\nNOTE\\nAppendix C shows an end-to-end example of how to build a minimal RAG pipeline.\\nWe suggest to try to build the pipeline yourself ﬁrst, and then check the appendix to\\nsee a complete example.\\nSummary\\nThis chapter explored diﬀerent techniques to ﬁne-tune large\\nlanguage models. We began by discussing traditional ﬁne-\\ntuning of encoder models for text classiﬁcation. However, this\\napproach can be used for other tasks, such as answering\\nquestions from a given text and identifying entities in a text. We\\nthen explored how to ﬁne-tune a decoder model for text\\ngeneration. We discussed the beneﬁts and limitations of ﬁne-\\ntuning versus zero-shot or few-shot generation. We also'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='examined how Supervised ﬁne-tuning can enable a generative\\nmodel to solve multiple tasks out of the box.\\nDespite the power of these techniques, scaling them to the\\nlatest, increasingly larger models presents challenges. To\\naddress this, we explored using quantization to run inference\\nwith large models on smaller GPUs, and discussed parameter-\\neﬃcient ﬁne-tuning (PEFT) techniques to ﬁne-tune models with\\nless computational and disk space requirements. By combining\\nthese techniques, we successfully ﬁne-tuned a 7B model to\\nmake it conversational. With these foundations, you are\\nequipped to ﬁne-tune large models for your speciﬁc tasks.\\nWhile this chapter focused heavily on model architecture and\\nﬁne-tuning techniques, it’s important to remember that the\\nsuccess of these models also depends on the quality and\\ndiversity of the training data. It’s not obvious to know how\\nmuch data is needed: it depends on the model size, task\\ncomplexity, and data quality. A few hundred high-quality\\ntraining samples can often be more eﬀective than thousands of\\nlow-quality ones.\\nFor further readings, we suggest the following resources:'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='To learn more about data, we recommend the FineWeb\\ndataset blog post, a comprehensive introduction to a 15\\ntrillion tokens dataset, including an in-depth investigation\\non pre-processing, explanations on how to create high-\\nquality web datasets, and how to create automatic\\nannotations.\\nRegarding evaluation, we suggest reading Let’s talk about\\nLLM evaluation, which presents a high-level overview of\\nmodel evaluation and its challenges. We also recommend\\nreading the Open LLM Leaderboard blog post, which gives\\na very comprehensive overview on community-centric\\nmodel evaluation. Finally, Eugene Yan wrote an excellent\\nblog post on the topic.\\nTo learn more about LoRA, we recommend reading\\nPractical Tips for Finetuning LLMs Using LoRA as well as\\nthe QLoRA launch blog post.\\nTo learn more about quantization, we recommend this\\nvisual guide to quantization as well as the incoming Hands-\\nOn Large Language Models.\\nTo learn more about all the components used in a\\nproduction LLM setup, we suggest the Building a\\nGenerative AI Platform blog post.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Exercises\\n1. What’s the diﬀerence between base and ﬁne-tuned models?\\nWhat kind of model is a conversational one?\\n2. In which cases would you pick a base encoder model for\\nﬁne-tuning?\\n3. Explain the diﬀerences between ﬁne-tuning, instruct-\\ntuning, and QLoRA.\\n4. Does using adapters lead to a larger model size?\\n5. How much GPU memory is needed to load a 70B model in\\nhalf-precision, 8-bit quantization, and 4-bit quantization?\\n6. Why does QLoRA lead to slower training?\\n7. In which cases do we freeze the model weights during ﬁne-\\ntuning?\\nYou can ﬁnd the solutions to these exercises in the GitHub\\nrepository of the book.\\nChallenges\\n8. Image Classiﬁcation. Although this chapter has focused on\\nﬁne-tuning transformer models for NLP tasks, transformers\\ncan also be used for other modalities such as audio and\\nComputer Vision. The goal of this challenge is to ﬁne-tune a\\ntransformer model for image classiﬁcation. We suggest to:'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='1. Use a pre-trained vision transformer model such as\\ngoogle/vit-base-patch16-224-in21k.\\n2. Use a dataset of images and labels such as food101.\\nThe logic will be almost the same, with some key diﬀerences\\nsuch as using an +AutoImageProcessor+ rather than an\\n+AutoTokenizer+. We suggest looking into the\\ndocumentation to guide you through the process.\\nReferences\\n1. Abdin, Marah, et al.\\xa0Phi-3 Technical Report: A Highly\\nCapable Language Model Locally on Your Phone. arXiv, 23\\nMay 2024. arXiv.org, https://arxiv.org/abs/2404.14219\\n2. Belkada, Younes, and Dettmers, Tim. A Gentle Introduction\\nto 8-bit Matrix Multiplication for transformers at scale using\\nHugging Face Transformers, Accelerate and bitsandbytes.\\nAugust, 2022. https://huggingface.co/blog/hf-bitsandbytes-\\nintegration\\n3. Belkada, Younes, et al.\\xa0Making LLMs even more accessible\\nwith bitsandbytes, 4-bit quantization and QLoRA. May 2023.\\nhttps://huggingface.co/blog/4bit-transformers-bitsandbytes\\n4. Chung, Hyung Won, et al.\\xa0Scaling Instruction-Finetuned\\nLanguage Models. arXiv, 20 Oct.\\xa02022. arXiv.org,\\nhttps://arxiv.org/abs/2210.11416'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='5. Dettmers, Tim, et al.\\xa0LLM.int8(): 8-bit Matrix Multiplication\\nfor Transformers at Scale. arXiv, August 2022. arXiv.org,\\nhttps://arxiv.org/abs/2208.07339\\n6. Dubey, Abhimanyu, et al.\\xa0The Llama 3 Herd of Models.\\narXiv, 31 Jul.\\xa02024. arXiv.org, https://arxiv.org/abs/2407.2178\\n7. Honovich, Or, et al.\\xa0Unnatural Instructions: Tuning\\nLanguage Models with (Almost) No Human Labor. arXiv, 19\\nDec.\\xa02022. arXiv.org, https://arxiv.org/abs/2212.09689\\n8. Kocetkov, Denis, et al.\\xa0The Stack: 3 TB of permissively\\nlicensed source code. arXiv, November 2022. arXiv.org\\nhttps://arxiv.org/abs/2211.15533\\n9. Lester, Brian, et al.\\xa0The power of scale for parameter-eﬃcient\\nprompt tuning. arXiv, April 2021. arXiv.org,\\nhttps://arxiv.org/abs/2104.08691\\n10. Lisa Li, Xiang, and Liang, Percy. Preﬁx-tuning: Optimizing\\ncontinuous prompts for generation. arXiv, January 2021.\\narXiv.org, https://arxiv.org/abs/2101.00190\\n11. Liu, Xiao, et al.\\xa0GPT Understands, Too. arXiv, March 2021.\\narXiv.org, https://arxiv.org/abs/2103.10385\\n12. Mishra, Swaroop, et al.\\xa0Cross-Tasks Generalization via\\nNatural Language Crowdsourcing Instructions. arXiv, April\\n2021. arXiv.org, https://arxiv.org/abs/2104.08773\\n13. Sanh. Victor, et al.\\xa0DistilBERT, a distilled version of BERT:\\nsmaller, faster, cheaper and lighter. arXiv, October 2019.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='arXiv.org, https://arxiv.org/abs/1910.01108\\n14. Sanh, Victor, et al.\\xa0Multitask Prompted Training Enables\\nZero-Shot Task Generalization. arXiv, 17 Mar.\\xa02021.\\narXiv.org, https://arxiv.org/abs/2110.08207\\n15. Wei, Jason, et al.\\xa0Finetuned Language Models Are Zero-Shot\\nLearners. arXiv, September 2021. arXiv.org,\\nhttps://arxiv.org/abs/2109.01652\\n16. Wang, Yizhong, et al.\\xa0Self-Instruct: Aligning Language\\nModels with Self-Generated Instructions. arXiv, 25 May 2023.\\narXiv.org, https://arxiv.org/abs/2212.10560\\n17. Wang, Yizhong, et al.\\xa0Super-NaturalInstructions:\\nGeneralization via Declarative Instructions on 1600+ NLP\\nTasks. arXiv, 24 Oct.\\xa02022. arXiv.org,\\nhttps://arxiv.org/abs/2204.07705\\n18. Ye, Qinyuan and Ren, Xiang. Learning to Generate Task-\\nSpeciﬁc Adapters from Task Description. arXiv, 15, Jun.\\xa02021.\\narXiv.org, https://arxiv.org/abs/2101.00420\\n19. Yuchen Lin, Bill, et al.\\xa0The Unlocking Spell on Base LLMs:\\nRethinking Alignment via In-Context Learning. arXiv, 4\\nDec.\\xa02023. arXiv.org, https://arxiv.org/abs/2312.01552\\n20. Zhou, Chunting, et al.\\xa0LIMA: Less Is More for Alignment.\\narXiv, 18 May 2023. arXiv.org,\\nhttps://arxiv.org/abs/2305.11206'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='You can read more details about the dataset in its dataset card.\\n Sahn, Victor, et al.\\xa0DistilBERT, a distilled version of BERT: smaller, faster, cheaper and\\nlighter. arXiv, 2 Oct.\\xa02019. arXiv.org, https://arxiv.org/abs/1910.01108\\n The harmonic mean is a type of average useful when dealing with ratios, as it gives\\nmore weight to lower values.\\n There are dozens of arguments you can modify. We suggest to explore its\\ndocumentation to understand all the options available.\\n A handy tool to estimate how much vRAM is needed to perform inference and\\ntraining is https://huggingface.co/spaces/hf-accelerate/model-memory-usage\\n Dubey, Abhimanyu, et al.\\xa0The Llama 3 Herd of Models. arXiv, 31 Jul.\\xa02024. arXiv.org,\\nhttps://arxiv.org/abs/2407.21783\\n Kocetkov, Denis, et al.\\xa0The Stack: 3 TB of permissively licensed source code. arXiv, 20\\nNov.\\xa02022. arXiv.org, https://arxiv.org/abs/2211.15533\\n For a detailed explanation, refer to the blog post\\n Non-experts (having or pursuing a PhD in a ﬁeld diﬀerent than the question)\\nanswered the questions with unrestricted time and full access to the internet (except\\nusing LLMs). They were paid $10 for attempting to answer each question and a $30\\nbonus for answering correctly. On average, they spent 37 minutes per question and\\neven then had a 34% accuracy!\\n Although originally called instruct-tuning, the community has settled on Supervised\\nﬁne-tuning, specially in the context of chat models after the InstructGPT paper.\\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n 0'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Chung, Hyung Won, et al.\\xa0Scaling Instruction-Finetuned Language Models. arXiv, 20\\nOct.\\xa02022. arXiv.org, https://arxiv.org/abs/2210.11416\\n Wei, Jason, et al.\\xa0Fine-tuned Language Models Are Zero-Shot Learners. arXiv, 8\\nFeb.\\xa02022. arXiv.org, https://arxiv.org/abs/2109.01652\\n Mishra, Swaroop, et al.\\xa0Cross-Task Generalization via Natural Language\\nCrowdsourcing Instructions. arXiv, 14 Mar.\\xa02022. arXiv.org,\\nhttps://arxiv.org/abs/2104.08773\\n Sanh, Victor, et al.\\xa0Multitask Prompted Training Enables Zero-Shot Task\\nGeneralization. arXiv, 17 Mar.\\xa02021. arXiv.org, https://arxiv.org/abs/2110.08207\\n Wang, Yizhong, et al.\\xa0Super-NaturalInstructions: Generalization via Declarative\\nInstructions on 1600+ NLP Tasks. arXiv, 24 Oct.\\xa02022. arXiv.org,\\nhttps://arxiv.org/abs/2204.07705\\n Honovich, Or, et al.\\xa0Unnatural Instructions: Tuning Language Models with (Almost)\\nNo Human Labor. arXiv, 19 Dec.\\xa02022. arXiv.org, https://arxiv.org/abs/2212.09689\\n Wang, Yizhong, et al.\\xa0Self-Instruct: Aligning Language Models with Self-Generated\\nInstructions. arXiv, 25 May 2023. arXiv.org, https://arxiv.org/abs/2212.10560\\n In practice, this is more nuanced. The authors provided eight randomly sampled\\ninstructions and asked the model to generate more task instructions. The authors\\nalso removed duplicate and similar instructions.\\n Zhou, Chunting, et al.\\xa0LIMA: Less Is More for Alignment. arXiv, 18 May 2023.\\narXiv.org, https://arxiv.org/abs/2305.11206\\n Ye, Qinyuan and Ren, Xiang. Learning to Generate Task-Speciﬁc Adapters from Task\\nDescription. arXiv, 15, Jun.\\xa02021. arXiv.org, https://arxiv.org/abs/2101.00420\\n 1 \\n 2 \\n 3 \\n 4 \\n 5 \\n 6 \\n 7 \\n 8 \\n 9 \\n 0'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='This example is inspired from Sebastian’s excellent article\\nhttps://magazine.sebastianraschka.com/p/practical-tips-for-ﬁnetuning-llms\\n Learn more about LoRAX at https://github.com/predibase/lorax\\n The default target modules depend on the model architecture.\\n This would be 0.017GB, which is extremely light and can even run locally in a web\\nbrowser.\\n This would be 405B*16 = 810GB. This would require at least two nodes with 8 A100\\nGPUs each.\\n To learn more about model quantization techniques, we recommend reading the\\nquantization guide in transformers\\nhttps://huggingface.co/docs/transformers/main/quantization/overview\\n We recommend reviewing the trl documentation for more information\\nhttps://huggingface.co/docs/trl/en/sft_trainer\\n We added a new line between human and assistant for readability, as well as an\\nextra line between each turn.\\n This is changing recently. Some base models are adding instruction to their training\\ndata mixture, so they can perform some basic instruction following out of the box.\\n Yuchen Lin, Bill, et al.\\xa0The Unlocking Spell on Base LLMs: Rethinking Alignment via\\nIn-Context Learning. arXiv, 4 Dec.\\xa02023. arXiv.org, https://arxiv.org/abs/2312.01552\\n 1 \\n 2 \\n 3 \\n 4 \\n 5 \\n 6 \\n 7 \\n 8 \\n 9 \\n 0'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Chapter 7. Fine-Tuning Stable\\nDiﬀusion\\nA NOTE FOR EARLY RELEASE READERS\\nWith Early Release ebooks, you get books in their earliest form\\n—the authors’ raw and unedited content as they write—so you\\ncan take advantage of these technologies long before the oﬃcial\\nrelease of these titles.\\nThis will be the seventh chapter of the ﬁnal book. Please note\\nthat the GitHub repo will be made active later on.\\nIf you have comments about how we might improve the content\\nand/or examples in this book, or if you notice missing material\\nwithin this chapter, please reach out to the editor at\\njleonard@oreilly.com.\\nIn the previous chapter, we introduced how ﬁne-tuning can\\nteach language models to write in a particular style or to learn\\nconcepts for a speciﬁc domain. We can apply the same\\nprinciples to text-to-image models, allowing us to customize the\\nmodels even with access to a single GPU (versus the multi-GPU\\nnodes required to pre-train a model like Stable Diﬀusion).'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='In this chapter, we will use the base pre-trained Stable Diﬀusion\\nmodel we learned in Chapter 5 and extend it to learn styles and\\nconcepts it might not know about, such as the concept of \"your\\npet\" or a particular painting style. We will also learn how to\\ngive it new capabilities, such as inpainting and giving new\\nconditions as inputs.\\nRather than writing code from scratch here, we will look into\\nunderstanding and running existing scripts created for ﬁne-\\ntuning the models in this section. For that, we recommend you\\nclone the diﬀusers library, as most examples will be in the\\nexamples folder of the library:\\ngit clone \\nhttps://github.com/huggingface/diffusers.git\\nFull Stable Diﬀusion Fine-Tuning\\nFull model is a qualiﬁer to ﬁne-tuning that emerged after the\\ndevelopment of speciﬁc model customization techniques such\\nas Low-Rank Adaptation (LoRA), Textual Inversion, and\\nDreambooth. Those techniques do not fully ﬁne-tune the entire\\nmodel, but rather either provide an eﬃcient way for ﬁne-tuning\\n(as we learned with LoRAs for LLMs in Chapter 6), or provide'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='novel ways to \"teach\" the model new concepts. We will\\ndiscuss these techniques further in the chapter.\\nBefore the emergence of such techniques, qualiﬁers such as full\\nmodel didn’t exist, as it was simply called ﬁne-tuning. Fine-\\ntuning in this context means further training the diﬀusion\\nmodel - like we learned in Chapters 3 and 4, but with the goal to\\nsteer it towards speciﬁc knowledge you want to add. You could\\nmake Stable Diﬀusion learn a style or subject that you can’t get\\nvia prompting or that was only invented after the model was\\nreleased. As the qualiﬁer full model may imply, once the model\\ngets ﬁne-tuned, it will become good at the style or subject you\\nintroduced into it, and it may become specialized in producing\\nprimarily that type of content. This section will use a pre-made\\nscript to perform full ﬁne-tuning. We are going to use the script\\ndiﬀusers/examples/text_to_image/train_text_to_image.py from\\nthe diﬀusers library.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 7-1. Figure 7-1. Stable Diﬀusion Fine-Tuning architecture diagram\\nPreparing the Dataset\\nThe most important part of the dataset is quality. Filtering the\\ndataset to keep only high-quality samples and removing low-\\nquality examples can signiﬁcantly aﬀect the quality of your\\nﬁne-tuning.\\nA relatively large dataset of 500+ images may be required for\\nhigh-quality full model ﬁne-tuning. Although this might sound\\nlike a lot, compared to the billions of images needed to train the\\nentire Stable Diﬀusion model, having a dataset of hundreds of\\nimages is still a tiny fraction. In the speciﬁc model\\ncustomization techniques we will discuss further in the chapter,\\nwe will learn how to customize the model with as little as four\\nimages.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Back to the full model ﬁne-tuning. As we steer a text-to-image\\nmodel, we must show it a dataset containing images and the\\nrespective captions that describe those images, just like the\\nmodel was shown during its pre-training training. If you want\\ninspiration, some examples can be:\\nRenaissance paintings to make a Renaissance ﬁne-tune.\\nPictures of buildings in your favorite architectural style to\\nmake an architectural model.\\nA set of landscape photographs to ﬁne-tune the model for\\ngenerating realistic landscape scenes encompassing serene\\nforests, majestic mountains, and tranquil lakeshores.\\nWARNING\\nWhile we encourage exploration, for any deployment of ﬁne tunes that go beyond\\nlearning and educational content, in case the images you are using are the style of an\\nartist, the face of a person, or any material that holds intellectual property rights,\\nasking the author or the IP holders whether you can do that is not only nice but could\\nbe a legal requirement in your jurisdiction.\\nIn our example, we will use Hubble Telescope imagery, which is\\nput in the public domain by NASA 6 months after they are\\ntaken. Creating a Hubble Telescope dataset and ﬁne-tuning\\nStable Diﬀusion on Hubble imagery was pioneered by\\nresearcher Maxwell Weinzierl of The University of Texas at'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Dallas, who made the esa-hubble dataset and the Hubble\\nDiﬀusion 1 and 2 models. For this example, we aim to re-create\\nthe Hubble Diﬀusion 1 model by ﬁne-tuning Stable Diﬀusion\\nv1.5 with the esa-hubble dataset.\\nThe esa-hubble dataset was created by crawling images\\ncaptured by the Hubble Telescope from the European Space\\nAgency website. Thankfully, the captions describing the\\nastronomical phenomena depicted in those images are also\\navailable, so both can be saved and put into a format\\ncompatible with the datasets library that can then be used for\\nﬁne-tuning. How to collect the data (via web scraping or\\notherwise) is beyond the scope of this book, but you can look\\ninto tools such as Scrapy or BeautifulSoup for web scraping in\\nPython. Be mindful of each website’s policy regarding crawling\\nor scraping.\\nOnce you have a dataset with image-text pairs, you can load\\nthem into the datasets library. The following section shows you\\nhow to do so. If you don’t have your image dataset available,\\nyou can proceed with the provided esa-hubble dataset.\\nfrom datasets import load_dataset'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='dataset = load_dataset(\"imagefolder\", \\ndata_dir=\"/path/to/folder\")\\nimagefolder is a special mode for load_dataset that\\nallows the loading of a directory of images and a metadata ﬁle\\ncontaining the captions for each image. This mode requires you\\nto have all your images in /path/to/folder and a metadata.csv ﬁle\\ncontaining the caption corresponding to each image. So your\\nfolder can have this structure:\\nfolder/metadata.csv\\nfolder/0001.png\\nfolder/0002.png\\nfolder/0003.png\\nand the metadata.csv ﬁle should look like this\\nfile_name,text\\n0001.png,This is a golden retriever playing \\nwith a ball\\n0002.png,A german shepherd\\n0003.png,One chihuahua\\nOnce your dataset is loaded, you can push it to the Hugging Face\\nHub to share it with the community with a simple command.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='dataset.push_to_hub(\"my-hf-username/my-\\nincredible-dataset\")\\nAnd you have your dataset saved and ready to ﬁne-tune a\\nmodel! . In some cases, you have a perfect image dataset, but no\\nhuman captions have been made for it. In these cases, you can\\nuse image-to-text captioning models to create captions that you\\ncan then use to ﬁne-tune the model. Models such as BLIP-2 by\\nSalesforce or Florence 2 by Microsoft are widely used for this\\ntask. To learn more, you can check the Image-to-Text task page.\\nIf you don’t have a dataset, don’t worry; you can use the\\nprovided esa-hubble dataset.\\nFine-Tuning the Model\\nTo ﬁne-tune the model, we will need a dataset, as discussed in\\nthe previous section, as well as a training script and the weights\\nof a pre-trained model that we will be ﬁne-tuning. Thankfully,\\nwe can easily set this up using the diﬀusers library, which\\nprovides example training scripts, and the accelerate library\\n(for eﬃcient training procedures, allowing training loops of\\nPyTorch to work on multi-GPU, TPUs or diﬀerent precisions like\\nBF16). This script is useful as it contains all the necessary code\\nto eﬃciently train the UNet of Stable Diﬀusion while still giving\\n1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='control to the user with the exposed hyperparameters we will\\nexplore.\\nYou will need a GPU with at least 16GB of VRAM to perform this\\nﬁne-tuning or use services such as Google Colab Pro. The\\ncustomization techniques we will learn about this chapter\\nallow for training on more modest GPUs or the free version of\\nGoogle Colab.\\nYou can follow along either by using your own dataset or by\\nutilizing the esa-hubble dataset by Maxwell Weinzierl to\\nfollow along with replicating Hubble Diﬀusion.\\nTo begin ﬁne-tuning the model, you’ll need to ﬁrst clone the\\nnecessary training scripts from the diﬀusers repository:\\ngit clone \\nhttps://github.com/huggingface/diffusers.git\\ncd diffusers/examples/text_to_image/\\nThen, use the train_text_to_image.py script. For VRAM-\\nconstrained setups, we will use use_8bit_adam, which will\\nrequire bitsandbytes and gradient_accumulation_steps to\\nuse larger batch sizes than would typically ﬁt in GPU memory.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='accelerate launch train_text_to_image.py \\\\\\n--\\npretrained_model_name_or_path=\"Lykon/dreamshaper-\\n8\" \\\\\\n--dataset_name=\"Supermaxman/esa-hubble\" \\\\\\n--use_ema \\\\\\n--mixed_precision=\"fp16\" \\\\\\n--resolution=512 \\\\\\n--center_crop \\\\\\n--random_flip \\\\\\n--train_batch_size=1 \\\\\\n--gradient_checkpointing \\\\\\n--gradient_accumulation_steps=4 \\\\\\n--use_8bit_adam \\\\\\n--checkpointing_steps=1000 \\\\\\n--num_train_epochs=50 \\\\\\n--validation_prompts \\\\\\n    \"Hubble image of a colorful ringed \\nnebula: \\\\\\nA new vibrant ring-shaped nebula was imaged \\nby the \\\\\\nNASA/ESA Hubble Space Telescope.\" \\\\\\n    \"Pink-tinted plumes in the Large \\nMagellanic Cloud: \\\\\\nThe aggressively pink plumes seen in this \\nimage are extremely uncommon, \\\\\\nwith purple-tinted currents and nebulous \\nstrands reaching out into \\\\'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='the surrounding space.\" \\\\\\n--validation_epochs 5 \\\\\\n--learning_rate=1e-05 \\\\\\n--output_dir=\"sd-hubble-model\" \\\\\\n--push_to_hub\\nBefore we dig into the parameters, we suggest you to run the\\nscript while you keep reading the chapter, as the training will\\ntake some time (between 1-3 hours depending on your GPU) -\\nand then iterate the hyper-parameters further as you’ve read\\nthrough the entire chapter.\\nGoing in detail on how the train_text_to_image.py script works\\nunder the hood is outside of the scope of this book, however the\\nbasics are the same as we learned on the \"Training the\\nModel\" section of Chapter 4, where we covered the core\\nconcepts of training a diﬀusion model. The key steps involve\\npreparing a dataset of images, deﬁning a noise schedule, and\\ncreating a UNet model to predict noise. The training loop\\niteratively adds noise to clean images, has the model predict\\nthis noise, calculates loss between predicted and actual noise,\\nand updates model weights via backpropagation. Here we are\\ngoing to understand the key hyper-parameters (which are the\\nsettings you set before starting to ﬁne-tune your model) is still'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='very important; we are going to go over every setting in the\\ntraining script above.\\nThe most important concepts from the setup above to learn are\\nlearning_rate and num_train_epochs.\\nlearning_rate denotes the amount by which the weights\\nof your model are updated for each training step. If you\\naim for a higher learning rate, the optimization process for\\nﬁne-tuning the model may not stabilize, while a too-low\\nvalue may under-ﬁt, and your model may never learn. We\\nrecommend experimenting between 1e-04 (0.0001) and 1e-\\n06 (0.000001).\\nnum_train_epochs denotes how many times your model\\nwill go through the entire dataset. It is normal for the\\nmodel to need multiple passes over the dataset to learn a\\nconcept. Another way to set how many times the model will\\nrun the training loop is by setting up the\\nmax_train_steps variable, where you can set the exact\\namount of training steps the model will go through (even if\\nit wraps up in the middle of an epoch).\\nWe will also go over the other hyperparameters in less detail:'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='use_ema: denotes using the exponential moving average\\nto train the model, which helps stabilize the model’s\\ntraining over epochs by averaging the weights.\\nmixed_precision: will train the model in mix-precision.\\nIf set to FP16 as above, all non-trainable weights, such as\\nthe VAE, will be cast to half-precision. These weights are\\nonly used for inference, so we don’t need them in full\\nprecision. This will use less VRAM and speed up training.\\nresolution: speciﬁes the image resolution for training.\\nThey will then get resized based on parameters such as\\ncenter_crop (if the images are larger than the target\\nresolution, they will get center-cropped) and random_flip\\n(some images will get ﬂipped during training for more\\nrobustness).\\ntrain_batch_size: speciﬁes how many examples are\\nshown to the model simultaneously. The larger the batch\\nsize, the larger the VRAM requirement, but the lower the\\ntraining time.\\ngradient_checkpointing and\\ngradient_accumulation_steps: this enables users to ﬁt\\ntraining on less VRAM as the gradients accumulate before\\nstepping the optimizer.\\nuse_8bit_adam: Whether or not to use 8-bit Adam\\nOptimizer from bitsandbytes to reduce the required GPU'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='memory. It makes it faster and uses less memory, with\\nlower precision than FP16 or FP32 precision for the\\ngradient accumulation (summing gradients over multiple\\nmini-batches before updating the model weights).\\ncheckpointing_steps: After how many training steps\\n(batches the model saw) is a snapshot of the model saved?\\nSaving intermediary models is useful if you set up a high\\nnumber of num_train_epochs or max_train_steps.\\nYou may realize that the best-performing model is trained\\non 30 epochs, and the full 50 got overﬁt.\\nvalidation_prompts: Prompts that help you check how\\nyour model is doing during training. For every\\nvalidation_epochs epochs, your model will generate the\\nimages with the validation_prompts prompts, and you\\ncan perceptually analyze how the model is learning.\\noutput_dir: local directory where the model will get\\nsaved to.\\npush_to_hub: whether to push your model after trained\\nto the Hugging Face Hub.\\nThe train_text_to_image.py training script has more parameters\\nand settings. Stable Diﬀusion XL can also be ﬁne-tuned\\nsimilarly. Once your model is trained and pushed to the Hub,\\nyou can now run inference on it.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Inference\\nOnce ﬁne-tuned, the model can be used for inference just like a\\nregular Stable Diﬀusion model, as we learned in Chapter 4, but\\nthis time, the model to be loaded is a newly trained model. In\\ncase you don’t have the compute to train the model, here’s a\\nmodel you to try out inference trained on the same Hubble\\ndataset: Supermaxman/hubble-diﬀusion-1. You can also share\\nyour model with others using the Hugging Face platform.\\nimport torch\\nfrom diffusers import StableDiffusionPipeline\\nfrom genaibook.core import get_device\\nmodel_id = \"Supermaxman/hubble-diffusion-1\"\\ndevice = get_device()\\npipe = \\nStableDiffusionPipeline.from_pretrained(\\n    model_id,  # your-hf-username/your-\\nfinetuned-model\\n    torch_dtype=torch.float16,\\n).to(device)\\nprompt = (\\n    \"Hubble reveals a cosmic dance of binary'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='stars: In this stunning new image \"\\n    \"from the Hubble Space Telescope, a pair \\nof binary stars orbit each other in a \"\\n    \"mesmerizing ballet of gravity and light. \\nThe interaction between these two \"\\n    \"stellar partners causes them to shine \\nbrighter, offering astronomers crucial \"\\n    \"insights into the mechanics of dual-star \\nsystems.\"\\n)\\npipe(prompt).images[0]\\n  0%|          | 0/50 [00:00<?, ?it/s]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 7-2. Image of a galaxy. This is the output from the Hubble\\nDiﬀusion ﬁne-tuned model\\nAs you play with the model, you may realize the model has\\nbecome very good at generating images that could have come\\nfrom the Hubble telescope (or whatever you have ﬁne-tuned it\\nfor), as shown in Figure 7-2. However, it became a specialist in\\nthat. If you prompt it to produce anything else, it will make\\neither some galaxy-looking output or just gibberish. This is'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='because with ﬁne-tuning the full model, it experiences\\ncatastrophic forgetting, by which the entire model gets tuned\\ntowards the direction you steered it towards. Also, as\\nmentioned before, you needed quite a few images to train it.\\nTechniques such as Dreambooth and Low-Rank Adaptation\\n(LoRA) can overcome these limitations.\\nDreambooth\\nDreambooth is a customization technique for ﬁne-tuning Stable\\nDiﬀusion that ﬁrst appeared in the DreamBooth paper  by\\nNataniel Ruiz et al.\\xa0from Google Research. The Dreambooth\\ntechnique works by fully ﬁne-tuning the Stable Diﬀusion UNet,\\nhowever providing grounding for the model, such as class\\nimages for a prior preservation loss - that is utilized in\\nconjunction with the reconstruction loss of training and by\\nproviding a unique trigger word to activate the knowledge of\\nthe concept into the model.\\n2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 7-2. Figure 7-3. Technical diagram for Dreambooth architecture ﬂow\\nDreambooth brings in three exciting advancements when\\ncompared with full model ﬁne-tuning text-to-image diﬀusion\\nmodels:\\nCustomizing a diﬀusion model by teaching it a new concept\\nwhile retaining all the previous knowledge (avoiding the\\ncatastrophic forgetting property mentioned in the previous\\nsection). As shown in Figure 7-3, Dreambooth tunes a\\nparticular unique token or set of tokens towards a new\\nconcept being added. So, if you want to include your dog in\\nthe model, you can train it with the sentence a [T] dog,\\nand every time you reference a [T] dog in your model, it\\nwill be able to do generations with the speciﬁc\\n\"dreamboothed\" dog while keeping its characteristics.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='This can be achieved by utilizing the semantic knowledge\\nthe model already possesses (e.g., it \"knows\" what a dog is)\\nwith a novel class-speciﬁc prior preservation loss, allowing\\nthe model to keep that knowledge when generating the new\\nconcept. This combination allows for the creation of the\\nsubject in various scenes, poses, viewpoints, and lighting\\nconditions that are not present in the reference images.\\nWithout a unique token, the model can mix in the new\\nknowledge you are trying to train with an already existing\\nconcept associated with that token.\\nCustomizing a diﬀusion model with only 3-5 examples:\\ninstead of 500+ examples from full ﬁne-tuning, the model\\ncan still learn and generalize well by giving the model a\\nsmall number of examples. This happens because the\\nmodel can leverage its internal knowledge for the same\\nclass of content as the one you are aiming to customize (for\\nexample, a [T] dog contains the word dog, which will\\nthen use the internal representations of dog from the\\nmodel to then customize it to the speciﬁc dog in your\\nimage). A consequence of fewer examples is also fast\\ntraining, as the model needs to process less.\\nWriting a caption for each instance image that will be\\nuploaded for the model to learn from, as we done for full'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='ﬁne-tuning is compatible, but optional, as using the a token,\\nwithout captioning the images is suﬃcient for Dreambooth.\\nThe Dreambooth technique in the original paper was applied to\\nGoogle’s proprietary diﬀusion model Imagen. However, the\\nopen-source community member Xavier Xiao adapted the\\ntechnique to Stable Diﬀusion, and since then, many community\\nimplementations, such as the ones from TheLastBen and khoya-\\nss, emerged. The diﬀusers library also has Dreambooth training\\nscripts.\\nOverall, through trial, error, and decentralized experience,\\ncommunity ﬁndings suggest that:\\nThree to ﬁve images typically suﬃce to train a common\\nsubject or style on Stable Diﬀusion.\\nUsing between 8 and 20 images is more eﬀective for\\ntraining unique styles or rare objects.\\nPrior preservation loss is beneﬁcial for faces but may not\\nbe needed for other subjects or styles.\\nFine-tuning both the model’s text encoder and UNet can\\nproduce good outcomes.\\nMost of these insights have been incorporated into the\\ncommunity’s training scripts.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='We will use the script\\ndiﬀusers/examples/dreambooth/train_dreambooth.py from the\\ndiﬀusers library.\\nNOTE\\nDreambooth is not the ﬁrst customization technique with the same goals. Textual\\nInversion, based on the seminal paper _An Image is Worth One Word_ , showcases\\nhow to train a new embedding for the text encoder of the Stable Diﬀusion model to\\ncontain a new subject. The technique is still relevant, as the embeddings trained can\\nbe small (just a few kilobytes). However, there’s a size versus quality trade-oﬀ, and\\nDreambooth’s quality has made this technique dominant in the text-to-image\\ncommunity. . Experiments with techniques combining Textual Inversion and\\nDreambooth (named Pivotal Tuning) have also shown good results. With\\nDreambooth, you need to ﬁnd a unique trigger word. However, by utilizing Textual\\nInversion, we can create trigger words as new tokens, leading to better injection of\\nnew concepts.\\nPreparing the Dataset\\nFive to 20 examples tend to be enough to train a new object or\\nface. For styles, if the model is struggling to learn with that\\nrange, adding more examples can help. As captioning the\\nimages is not required for Dreambooth, all you have to do is\\nhave your training images in a folder of your preference that\\nyou can reference using the training code. For example, you can\\ndownload the pictures of your pet.\\n3 \\n4'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='For this example, we will train a model on the face of one of the\\nauthors. If you would like to follow along, feel free to train a\\nmodel on your own face (or maybe a pet).\\nPrior Preservation\\nWith Dreambooth, you can optionally take advantage of a prior\\npreservation class. This works by providing a prior\\npreservation loss during training so that the model understands\\nthat it is generating elements of that same class. For example,\\nwhen teaching the model your own face, having a collection of\\nface images will help the model \"understand\" that the class\\nyou are trying to train are faces, so even though you provide\\nfew examples, it will be grounded on images of faces. If the\\nmodel already has knowledge about the class you are going to\\ncreate, you can even generate the prior images yourself (and\\nthe training code has a ﬂag allowing you to do that) or upload\\nthem to a speciﬁc folder.\\nTo enable prior preservation, you can conﬁgure a couple of\\nparameters in the training script:\\nwith_prior_preservation: whether to use prior\\npreservation. If set to True, the model will generate\\nimages based on the class_prompt and'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='num_class_images parameters. If you provide a\\nclass_data_dir folder, images inside that folder will be\\nused as class images instead.\\nclass_prompt: the prompt from which the model will\\nattempt to learn the concept—for example, the face of\\na Brazilian man.\\nnum_class_images: the number of class images to be\\ngenerated. If you provide a class_data_dir folder,\\nimages inside that folder will be used as class images\\ninstead. If the folder has fewer images than\\nnum_class_images, the remaining ones will be generated\\nby the class_prompt.\\nDreamboothing the Model\\nJust like with ﬁne-tuning the entire model, the most important\\nvariables are learning_rate and num_train_epochs. A low\\nlearning rate followed by slowly incrementing the number of\\ntrain epochs or steps can provide a good starting point to land\\nin good quality. Another exploration route is ﬁxing the number\\nof train epochs or steps and increasing the learning rate. Both\\nstrategies can be combined to ﬁnd the optimal\\nhyperparameters.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Let’s go over some parameters that are exclusive to\\nDreambooth:\\ninstance_prompt: the prompt from which the model will\\nattempt to learn the concept. Try to ﬁnd a rare token or\\ncombination for your subject name and surround it with\\ncontext, for example, in the style of mybtfuart, a\\nphoto of plstps, or an sckpto toy.\\ntrain_text_encoder: whether to train the text-encoder\\nas well. It can yield good results but consumes more VRAM\\nmemory. The reason why training the text encoder together\\nwith the UNet may be useful is that you are also injecting\\nknowledge about this new concept into the textual\\ninterpretation of the prompt.\\nwith_prior_preservation: whether to utilize prior\\npreservation loss.\\nclass_prompt or class_data_dir: a prompt to\\ngenerate the class images for the prior preservation loss.\\nprior_loss_weight: controls the inﬂuence of the prior\\npreservation loss on the model.\\nLet’s use the train_dreambooth.py script to train a model on\\nthe face of one of the authors, as shown in Figure 7-4. If you\\nwant to follow along, feel free to train a model on your own.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 7-3. Figure 7-4. Training set for faces of \"Apolinário Passos\" for\\nDreambooth training\\naccelerate launch train_dreambooth.py \\\\\\n--\\npretrained_model_name_or_path=\"Lykon/dreamshaper-\\n8\"  \\\\\\n--instance_data_dir=\"my-pictures\" \\\\\\n--instance_prompt=\"a photo of plstps\" \\\\  \\n--resolution=512 \\\\\\n--train_batch_size=1 \\\\\\n--with_prior_preservation \\\\\\n--class_prompt=\"an ultra realistic portrait \\nof a man\" \\\\  \\n--gradient_accumulation_steps=1 \\\\\\n--train_text_encoder \\\\\\n--learning_rate=5e-6 \\\\\\n--num_train_epochs=100 \\\\\\n--output_dir=\"myself-model\" \\\\\\n--push_to_hub'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='plstps is a unique token for the model to learn the concept\\non.\\nHere you can add a generic description of what you are\\ntraining\\nOnce the model is trained, you have a model you can use to\\ngenerate new images. Let’s explore how to do that.\\nInference\\nEven though the overall structure of the model is preserved and\\nonly the new token based on the instance_prompt is altered, the\\nnew Dreambooth model is still a full Stable Diﬀusion weight.\\nTherefore, it can be loaded as such for inference.\\nmodel_id = \"your-hf-profile/your-custom-\\ndreambooth-model\"\\npipe = \\nStableDiffusionPipeline.from_pretrained(\\n    model_id,\\n    torch_dtype=torch.float16,\\n).to(device)\\nprompt = \"a photo of plstps speaking on a \\nmicrophone\"  # Insert here your instance'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='prompt and some custom customization\\npipe(prompt).images[0]\\nFigure 7-5 shows some results of the model trained on one of\\nthe author’s face.\\nFigure 7-4. Figure 7-5. Output images generated from the model \"Dreamboothed\" on\\nApolinário’s face\\nTraining LoRAs\\nWe have an issue with full ﬁne-tuning and Dreambooth: once\\nwe ﬁnish tuning our model, we end up with new weights as\\nlarge as the original Stable Diﬀusion model. This scenario is not\\nideal for sharing, hosting locally, stacking models, serving it in'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='the cloud, and other downstream applications. For this, Low-\\nRank Adaptations (LoRA) can be used, just as we did in the\\nprevious chapter for LLMs.\\nFigure 7-5. Figure 7-6. Technical diagram for LoRA architecture ﬂow\\nAs discussed in Chapter 6, LoRAs allow freezing of the pre-\\ntrained model weights and inject rank decomposition matrices,\\nsigniﬁcantly reducing the number of parameters to be trained.\\nThe LoRA-trained ranks can also be shared as artifacts that can\\nbe merged into the model without additional inference latency.\\nSounds great! But we are in the ﬁne-tuning diﬀusion chapter,\\nand the original LoRAs were focused on transformers. This is\\nwhere Simo Ryu’s Stable Diﬀusion LoRA GitHub repository\\ncomes into the picture. Realizing that LoRA rankings can be\\nattached to the Stable Diﬀusion UNet and text encoder in the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='same way they can be added to transformers LLMs has now\\nunlocked the power of LoRAs for diﬀusion models.\\nOnce again, the diﬀusers library comes to the rescue,\\nincorporating a script for LoRA training for both full model\\nﬁne-tuning and dreambooth ﬁne-tuning. Training a LoRA\\nweight with diﬀusers is virtually the same as full model ﬁne-\\ntuning or Dreambooth ﬁne-tuning with diﬀusers, with some key\\ndiﬀerences:\\nThe dataset format is the same as the one used for full\\nmodel ﬁne-tuning and Dreambooth ﬁne-tuning.\\nThe training scripts are diﬀerent, although the\\nhyperparameters are the same. We’ll use\\nexamples/text_to_image/train_text_to_image_lora.py and\\nexamples/dreambooth/train_dreambooth_lora.py from the\\ndiﬀusers library.\\nFor inference, the process involves loading the base model\\ninto the pipeline and then adding the LoRA adapter. This\\napproach is convenient because it allows you to quickly\\nload and switch between diﬀerent LoRA adapters while\\nkeeping the same base model. For example, you can use a\\npre-trained LoRA ﬁne-tune shared by another user (you can\\nﬁnd pre-trained options on the Hugging Face Hub). The\\nsteps are: select the base model (the one the LoRA will be\\n5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='attached to), load the diﬀusers pipeline, load the LoRA\\nweights into the model, and optionally fuse the LoRA\\nweights for better eﬃciency and speed.\\nLet’s see how to load a LoRA ﬁne-tuned model and perform\\ninference with it. The main diﬀerences are determining the\\nbase model and loading the LoRA weights into the model.\\nfrom diffusers import DiffusionPipeline\\nfrom huggingface_hub.repocard import RepoCard\\n# We\\'ll use a classic hand drawn cartoon \\nstyle\\nlora_model_id = \"alvdansen/littletinies\"\\n# Determine which is the base model\\n# This information is frequently in the model \\ncard\\n# It\\'s CompVis/stable-diffusion-v1-4 in this \\ncase\\ncard = RepoCard.load(lora_model_id)\\nbase_model_id = card.data[\"base_model\"]\\n# Load the base model\\npipe = DiffusionPipeline.from_pretrained(\\n    base_model_id, torch_dtype=torch.float16\\n)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='pipe = pipe.to(device)\\n# Add the LoRA to the model\\npipe.load_lora_weights(lora_model_id)\\n# Merge the LoRA with the base model\\npipe.fuse_lora()\\nimage = pipe(\\n    \"A llama drinking boba tea\", \\nnum_inference_steps=25, guidance_scale=7.5\\n).images[0]\\nimage\\n  0%|          | 0/25 [00:00<?, ?it/s]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 7-7. A llama drinking boba tea in classical illustration\\nstyle\\nGiving Stable Diﬀusion New\\nCapabilities\\nFine-tuning to teach the model new styles or subjects is\\nincredible. But what if we could use ﬁne-tuning to give Stable\\nDiﬀusion more capabilities than usual? By ﬁne-tuning with\\nsome special techniques, we can provide the model the\\ncapability of inpainting or include additional conditionings.\\nInpainting\\nInpainting involves masking a speciﬁc area of an image that\\nyou would like to replace with something else. It is similar to\\nimage-to-image, with the diﬀerence that noise is added only to\\nthe masked area: the model denoises only that area, aiming to\\nchange or remove that item from the image while keeping the\\nrest of the image intact.\\nIt is possible to give inpainting capability to a pre-trained text-\\nto-image diﬀusion model by including additional input\\nchannels for the UNet. In the case of the inpainting specialist'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Stable Diﬀusion v1 model, they added ~400K steps by having\\nﬁve zeroed-out input channels for the UNet, with four for the\\nencoded masked image and one for the mask itself. During\\ntraining, synthetic masks are generated, with 25% of everything\\nmasked. As you have the ground truth of the image behind the\\nmask, the model learns how to ﬁll in the masked areas based on\\nthe prompt, becoming a powerful image editing tool.\\nWith more advanced models, such as Stable Diﬀusion XL, some\\ninpainting capabilities come out of the box without further\\ntuning, which made some people question whether a\\nspecialized ﬁne-tuned model could improve this capability.\\nHowever, SDXL specialist inpainting models were released with\\nsome extra capabilities, showing the potential for this technique\\neven in bigger and more advanced models. While this\\ntechnique is not accessible for training on domestic hardware,\\nrequiring full ﬁne-tuning for hundreds of thousands of steps,\\nthe ﬁne-tuned models are accessible and available to everyone.\\nIn the next chapter, we are going to explore inpainting in more\\ndepth (with code).\\nAdditional Inputs for Special'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Conditionings\\nJust like new input channels can be added to the UNet for the\\nmodel to learn how to perform inpainting, other conditionings\\ncan be added. One example of this application was the Stable\\nDiﬀusion 2 Depth, a model resumed from stable-diffusion-\\n2-base and ﬁne-tuned for 200K steps with an extra input\\nchannel that processes both the user prompt and an image that\\ncontains a monocular depth (distance relative to the camera)\\nprediction produced by MiDaS. An example can be seen in\\nFigure 7-8.\\nFigure 7-6. Figure 7-8. Example inference of Stable Diﬀusion 2 ﬁne-tuned with MiDaS\\ndepth conditioning, as seen in https://huggingface.co/stabilityai/stable-diﬀusion-2-\\ndepth/resolve/main/depth2image.png\\nWhile this technique works well, ﬁne-tuning the base model for\\nhundreds or thousands of steps to get new conditioning limited\\nthis process to only a few companies and labs. However,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='techniques that append adaptors on top of the model, such as\\nControlNets, ControLoras, and T2I adaptors, emerged to make\\nthis process more eﬃcient for training and inference; we are\\ngoing to explore more of those creative applications of text-to-\\nimage in the next chapter.\\nProject Time: Train an SDXL\\nDreambooth LoRA by Yourself\\nFine-tuning is a great way to bring more knowledge to text-to-\\nimage diﬀusion models, and as we learned in this chapter,\\nDreambooth allows for ﬁne-tuning to happen with just a few\\nexample images, and LoRA training allows for small models\\nand lower GPU usage when compared to ﬁne-tuning the entire\\nmodel. For this project, you will ﬁne-tune a Dreambooth LoRA.\\nAfter learning the basics above, you can use diﬀusers more\\nadvanced scripts. If you do not own a GPU with at least 16GB of\\nVRAM, we recommend using Google Colab or Hugging Face\\nSpaces for this project.\\nYour goal is to be able to prompt a new, not-yet-existing object\\nor style into Stable Diﬀusion and have the model successfully\\ngenerate a new image with it.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Step 1: Dataset creation:\\nFind an object or style you want to include in the model. It\\ncould be a unique item you own (e.g., a wooden cat toy) or a\\nstyle of furniture/paintings/rugs that you have in your\\nhouse.\\nTake a couple of pictures of these objects from diﬀerent\\nangles, in diﬀerent backgrounds; around 3-8 pictures\\nshould suﬃce.\\nWrite a descriptive caption for each of the images, and use\\na unique token to describe your object (e.g., cttoy), for\\nexample:\\nA photo of the front of a cttoy, white background\\nA photo of the side of a cttoy, ﬂower pot in the\\nbackground\\netc.\\nEither upload the dataset to the Hugging Face Datasets Hub\\nor keep it in a local folder.\\nStep 2: Model training:\\nOpen any training script that suits you (based on the\\nrecommended ones or others you may ﬁnd).\\nPoint the image folder to either the local folder or the\\nHugging Face Dataset you’ve created.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Run the training. As explained earlier, you can experiment\\nwith learning_rate, batch_size, and other\\nhyperparameters parameters until you are satisﬁed with\\nyour LoRA. Refer to the LoRA section of this chapter to\\nlearn how to load your trained LoRA into Stable Diﬀusion\\nto test it out.\\nWith your validation_prompts you can check how the\\nsamples are being generated during training. Once the\\nmodel is trained, you can load it with\\nload_lora_weights to understand how your model was\\ntrained.\\nSummary\\nAs training a big text-to-image model from scratch requires a\\nsigniﬁcant amount of computing resources, ﬁne-tuning steps in\\nto enable single-GPU operations to customize pre-existing\\nmodels to produce what you need. In this chapter, we learned\\nhow ﬁne-tuning diﬀusion models can lead to expanding\\nknowledge and customization of the model for particular needs\\nwhile retaining its overall knowledge. We learned how to do a\\nfull ﬁne-tune, Dreambooth for speciﬁc characters or styles, and\\nLoRA for eﬃciency. We also learned that ﬁne-tuning diﬀusion'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='models can give them new capabilities. Overall, ﬁne-tuning is a\\npowerful tool.\\nThis chapter explored techniques for ﬁne-tuning the Stable\\nDiﬀusion model to teach it styles, subjects, or capabilities.\\nStarting with full model ﬁne-tuning, we learned how to alter the\\nmodel’s behavior to generate images in a desired style or\\nsubject. We then moved to techniques like Dreambooth and\\nLow-Rank Adaptation (LoRA), which allow for customization\\nwith fewer examples and less risk of catastrophic forgetting.\\nWe also discussed the potential of ﬁne-tuning to add new\\ncapabilities to the model, such as inpainting and special\\nconditionings, expanding the utility of Stable Diﬀusion beyond\\nits original conﬁguration.\\nFor additional readings, we suggest to review: * How to ﬁne\\ntune stable diﬀusion: how we made the text-to-pokemon model\\nat Lambda: https://lambdalabs.com/blog/how-to-ﬁne-tune-stable-\\ndiﬀusion-how-we-made-the-text-to-pokemon-model-at-lambda *\\nAn artist’s guide to LoRA training: how Araminta K trains her\\nLoRAs: https://huggingface.co/blog/alvdansen/training-lora-m3lt *\\nCreate an inﬁnite icon library by ﬁne-tuning Stable Diﬀusion\\nhttps://modal.com/blog/ﬁne-tuning-stable-diﬀusion * Advanced\\ndiﬀusion training guide:'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='(https://github.com/huggingface/diﬀusers/tree/main/examples/adv\\nanced_diﬀusion_training) * The DreamBooth paper itself:\\nhttps://arxiv.org/abs/2208.12242 * Lenghty introduction to LoRAs\\non diﬀusion models: https://github.com/cloneofsimo/lora?\\ntab=readme-ov-ﬁle#lengthy-introduction\\nExercises\\n1. Explain the main diﬀerences between full model ﬁne-\\ntuning and Dreambooth.\\n2. What are the advantages of using LoRA over full model\\nﬁne-tuning regarding computational resources and model\\nadaptability?\\n3. Why is it important to utilize a unique token when doing\\nDreambooth training?\\n4. Besides teaching new concepts, ﬁne-tuning can also add\\nnew capabilities to the model. Cite two capabilities that the\\nmodel can learn by applying ﬁne-tuning techniques.\\n5. Discuss how the choice of hyperparameters aﬀects the\\noutcome of ﬁne-tuning a diﬀusion model.\\n6. Describe the potential risks of ﬁne-tuning text-to-image\\nmodels on biased datasets.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='You can ﬁnd the solutions to these exercises in the GitHub\\nrepository of the book.\\nChallenges\\n7. LoRA versus full-ﬁne tuning comparison: Train a\\nDreambooth model with LoRA and full-ﬁne tuning and\\ncompare the results. Try to modify the \"rank\"\\nhyperparameter for the LoRA to see how much it aﬀects the\\nresults.\\nReferences\\n1. Gal, Rinon, et al.\\xa0An Image is Worth One Word:\\nPersonalizing Text-to-Image Generation using Textual\\nInversion. arXiv, 2022. arXiv.org,\\nhttps://arxiv.org/abs/2208.01618\\n2. Hu, Edward J., et al.\\xa0LoRA: Low-Rank Adaptation of Large\\nLanguage Models. International Conference on Learning\\nRepresentations, 2022. OpenReview.net,\\nhttps://openreview.net/forum?id=nZeVKeeFYf9\\n3. Podell, Dustin, et al.\\xa0SDXL: Improving Latent Diﬀusion\\nModels for High-Resolution Image Synthesis. arXiv, 4 July'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='2023. arXiv.org, http://arxiv.org/abs/2307.01952\\n4. Ruiz, Nataniel, et al.\\xa0DreamBooth: Fine Tuning Text-to-image\\nDiﬀusion Models for Subject-Driven Generation. arXiv, 2022.\\narXiv.org, https://arxiv.org/abs/2208.12242\\n5. Ryu, Simo. LoRA Repository. GitHub, 2022. GitHub.com,\\nhttps://github.com/cloneofsimo/lora\\n For more details on creating an image dataset with the datasets library, check out\\nCreate an image dataset docs (https://huggingface.co/docs/datasets/image_dataset)\\n Ruiz, Nataniel, et al.\\xa0DreamBooth: Fine Tuning Text-to-image Diﬀusion Models for\\nSubject-Driven Generation. arXiv, 2022. arXiv.org, https://arxiv.org/abs/2208.12242\\n Gal, Rinon, et al.\\xa0An Image is Worth One Word: Personalizing Text-to-Image\\nGeneration using Textual Inversion. arXiv, 2022. arXiv.org,\\nhttps://arxiv.org/abs/2208.01618\\n Learn more about Textual Inversion at\\nhttps://huggingface.co/docs/diﬀusers/training/text_inversion\\n LoRA trainer scripts such as Kohya, TheLastBen and Advanced LoRA Trainer built\\nupon the diﬀusers scripts oﬀer a lot more experimental functionality. Those are\\nadvanced but very well regarded by the community.\\n1 \\n2 \\n3 \\n4 \\n5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Chapter 8. Creative Applications of\\nText-To-Image Models\\nA NOTE FOR EARLY RELEASE READERS\\nWith Early Release ebooks, you get books in their earliest form\\n—the authors’ raw and unedited content as they write—so you\\ncan take advantage of these technologies long before the oﬃcial\\nrelease of these titles.\\nThis will be the eighth chapter of the ﬁnal book. Please note\\nthat the GitHub repo will be made active later on.\\nIf you have comments about how we might improve the content\\nand/or examples in this book, or if you notice missing material\\nwithin this chapter, please reach out to the editor at\\njleonard@oreilly.com.\\nThis chapter will present creative applications that leverage\\ntext-to-image models and increase their capabilities beyond just\\nusing text as control. We will start with the most basic ones and\\nthen move on to more advanced ones.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Image-to-Image\\nEven though generative text-to-image diﬀusion models like\\nStable Diﬀusion can produce images from text from a fully\\nnoised image, as we learned in Chapters 4 and 5, it is possible to\\nstart from an already existing image instead of a fully noised\\nimage. That is, add some noise to an initial image and have the\\nmodel modify it partially by denoising it. This process is called\\nimage-to-image, as an image is transformed into another image\\nbased on how much it is noised and based on the text prompt.\\nWith the diﬀusers library, we can load a image-to-image\\npipeline to load the class. For example, let’s explore how to use\\nSDXL for this task. The main diﬀerences are:\\nWe use the StableDiffusionXLImg2ImgPipeline rather\\nthan the usual StableDiffusionXLPipeline.\\nWe pass both a prompt and an initial image to the pipeline.\\nWe can use either the stabilityai/stable-diffusion-xl-\\nbase-1.0 or the stabilityai/stable-diffusion-xl-\\nrefiner-1.0 model for applying our image-to-image\\nreﬁnements. The base model is recommended when you want\\nto stylize your image or create new context from what is there.\\nThe reﬁner model, which specializes in working out ﬁne details'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='for the images, can be good if you want to reﬁne or add details\\nwithout many creative transformations to the image.\\nimport torch\\nfrom diffusers import \\nStableDiffusionXLImg2ImgPipeline\\nfrom diffusers.utils import load_image, \\nmake_image_grid\\nfrom genaibook.core import SampleURL, \\nget_device\\ndevice = get_device()\\n# Load the pipeline\\nimg2img_pipeline = \\nStableDiffusionXLImg2ImgPipeline.from_pretrained\\n    \"stabilityai/stable-diffusion-xl-refiner-\\n1.0\",\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\",\\n)\\nThen, we can move the pipeline to our device (usually cuda for\\nGPU). As some examples might require too much GPU, an\\nalternative is to use'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='img2img_pipeline.enable_model_cpu_offload() which\\nmoves submodules to the GPU as needed. This will make\\ninference slower but will allow you to run the model on a\\nsmaller GPU.\\n# Move the pipeline to the device\\n# Alternatively, img2img_pipeline.set \\nenable_model_cpu_offload()\\nimg2img_pipeline.to(device)\\nNow that we have our pipeline, let’s try an example.\\n# Load the image\\nurl = SampleURL.ToyAstronauts\\ninit_image = load_image(url)\\nprompt = \"Astronaut in a jungle, cold color \\npalette, muted colors, detailed, 8k\"\\n# Pass the prompt and the image through the \\npipeline\\nimage = img2img_pipeline(prompt, \\nimage=init_image, strength=0.5).images[0]\\nmake_image_grid([init_image, image], rows=1, \\ncols=2)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Our StableDiffusionXLImg2ImgPipeline takes in the same\\ninputs as the normal Stable Diﬀusion pipeline we’ve used so far,\\nplus two extra parameters:\\ninit_image: the original image that we are going to\\nmodify.\\nstrength: how much noise we will add to the image. A\\nstrength of 0 will return the exact same image as no noise\\nhas been added. A strength of 1 will fully noise the image,\\nignoring it entirely and behaving like the regular text-to-\\nimage pipeline.\\nCheck out the experiment in Figure 8-1, in which we took the\\nsame image and applied strengths between 0 and 1.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Inpainting\\nInpainting, exempliﬁed in Figure 8-2, is the process of ﬁlling in\\nmissing parts of an image based on the surrounding context. As\\nwe discussed in the previous chapter, it is possible to either use\\na model as it is for inpainting or ﬁne-tune a text-to-image\\ndiﬀusion model to improve its inpainting capabilities.\\nBefore we dive into the speciﬁcs of text-to-image diﬀusion\\nmodels for inpainting, it’s worth noting the distinction between\\nthis text-to-image generative approach and classical image\\nprocessing techniques for inpainting. Traditional methods\\ntypically rely on analyzing the surrounding pixels and using\\nvarious algorithms to ﬁll in the masked area based on local\\nimage statistics or patch-based sampling. While these classical\\napproaches can be eﬀective for simple backgrounds or small\\nareas, they often struggle with complex textures or semantic\\nunderstanding of the image content. In contrast, inpainting\\nusing text-to-image diﬀusion models oﬀers several advantages.\\nThese models can understand and generate content based on\\nboth visual and semantic context, allowing for more coherent\\nand creative results. However, ML methods generally require\\nmore computational resources.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Let’s showcase how to perform inpainting and what creative\\napplications it can leverage. As before, we can use a pipeline,\\nStableDiffusionXLInpaintPipeline, to handle this.\\nfrom diffusers import \\nStableDiffusionXLInpaintPipeline\\n# Load the pipeline\\ninpaint_pipeline = \\nStableDiffusionXLInpaintPipeline.from_pretrained\\n    \"stabilityai/stable-diffusion-xl-base-\\n1.0\",\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\",\\n).to(device)\\nimg_url = SampleURL.DogBenchImage\\nmask_url = SampleURL.DogBenchMask\\ninit_image = \\nload_image(img_url).convert(\"RGB\").resize((1024, \\n1024))\\nmask_image = \\nload_image(mask_url).convert(\"RGB\").resize((1024,\\n1024))\\n# Pass images and prompt through the pipeline\\nprompt = \"A majestic tiger sitting on a'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='bench\"\\nimage = inpaint_pipeline(\\n    prompt=prompt,\\n    image=init_image,\\n    mask_image=mask_image,\\n    num_inference_steps=50,\\n    strength=0.80,\\n    width=init_image.size[0],\\n    heigth=init_image.size[1],\\n).images[0]\\nmake_image_grid([init_image, mask_image, \\nimage], rows=1, cols=3)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 8-2. Inpainting with a source image (left), mask image\\n(middle) and output (right) of \"A majestic tiger sitting\\non a bench“.\\nSome of the most important parameters the\\nStableDiffusionXLInpaintPipeline pipeline takes in are:\\ninit_image: the image that will be inpainted.\\nmask_image: a binary color mask image. Black should be\\nwhere the image remains the same, and white should be\\nwhere you would like to replace it.\\nstrength: how much noise will we add to the mask. Just\\nlike the strength for image-to-image, but applying only to\\nthe masked area. A strength of 0 will return the same\\nimage as if no noise has been added. A strength of 1 will\\nfully noise the masked area, which is not the best scenario\\nfor a smooth blending. Experiment between 0.6 and 0.8.\\nApart from the base diﬀusion models, some models, such as\\ndiffusers/stable-diffusion-xl-inpainting, are ﬁne-\\ntuned to be specialized in inpainting. These models were ﬁne-\\ntuned explicitly for this task, which allows us to use a higher\\nstrength during inference.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Prompt Weighting and Image Editing\\nAs we learned in Chapter 4, diﬀusion models use transformer-\\nlike attention mechanisms that allow the model to focus ﬂexibly\\non the most relevant parts of the input. Speciﬁcally, cross-\\nattention is used to condition transformers inside the UNet\\nlayers with a text prompt to condition image generation.\\nHowever, you want more control over the generated image in\\nsome cases. For example, we may want to:\\nModify how much weight is given to each word of a prompt\\nby modifying the scale of the text embeddings.\\nCombine multiple prompts to generate an image.\\nChange the generations while keeping the structure for\\nimage editing.\\nFor that purpose, the Prompt-to-Prompt Image Editing with\\nCross Attention Control paper  introduced the idea of\\nmodifying the diﬀusion with the goal of obtaining steerability\\nby modifying and controlling the cross-attention. That paper\\nalso has a non-oﬃcial implementation for diﬀusers.\\nBesides prompt-to-prompt, other techniques for editing\\ngenerated images, such as Attend-and-Excite  and Semantic\\n1 \\n2 \\n3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Guidance  emerged, both with oﬃcial diﬀusers\\nimplementations. In this chapter, we will dive deeper into\\nSemantic Guidance editing, as it balances steerability and edit\\nquality.\\nPrompt Weighting and Merging\\nThe compel prompt enhancement library implements key\\naspects of prompt weighting and merging and is easy to use\\nwith the diﬀusers library. It works by pre-processing the strings\\nand enhancing the corresponding embeddings in the CLIP\\nembedding space. As Stable Diﬀusion XL utilizes two text\\nencoders, it adds complexity into the prompt weighting process.\\nThis complexity arises because the weighting must be\\nharmonized between both encoders, and the output from the\\nsecond text encoder needs to be pooled . The compel library\\nabstracts this complexity away.\\nTwo simple ways to control the prompt are:\\nIncreasing the weight with + signs after the word to give it\\nmore prominence in the image, modifying the scale of the\\ntext embedding. You can also decrease the weight with -. By\\nadding multiple + and - signs, you can increase or\\ndecrease the weight of a word even more. Although one can\\n3 \\n4'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='reduce the prominence of a word, it may not always\\ncompletely remove the concept from the image.\\nMerge two prompts (by having them within brackets) and\\nthen specify the weight for each prompt.\\nLet’s write some code. As usual, we begin by loading a pipeline.\\nfrom diffusers import DiffusionPipeline\\npipeline = DiffusionPipeline.from_pretrained(\\n    \"stabilityai/stable-diffusion-xl-base-\\n1.0\",\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\",\\n).to(device)\\nWe’ll now initialize a Compel class which requires providing\\nthe tokenizer and text encoders from the diﬀusion model. The\\nclass also requires specifying which text embedding will be\\npooled.\\nfrom compel import Compel, \\nReturnedEmbeddingsType\\n# Use the penultimate CLIP layer as it is \\nmore expressive'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='embeddings_type = (\\n    \\nReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_\\n)\\ncompel = Compel(\\n    tokenizer=[pipeline.tokenizer, \\npipeline.tokenizer_2],\\n    text_encoder=[pipeline.text_encoder, \\npipeline.text_encoder_2],\\n    returned_embeddings_type=embeddings_type,\\n    requires_pooled=[False, True],\\n)\\nFinally, we can generate images, displayed in Figure 8-3, with\\ndiﬀerent compel-enhanced prompts.\\nfrom diffusers.utils import make_image_grid\\n# Prepare the prompts\\nprompts = []\\nprompts.append(\"a humanoid robot eating \\npasta\")\\nprompts.append(\\n    \"a humanoid+++ robot eating pasta\"\\n)  # make its humanoid characteristics a bit \\nmore pronounced\\nprompts.append('),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='\\'[\"a humanoid robot eating pasta\", \"a van \\ngogh painting\"].and(0.8, 0.2)\\'\\n)  # make it van gogh!\\nimages = []\\nfor prompt in prompts:\\n    # Use the same seed across generations\\n    generator = \\ntorch.Generator(device=device).manual_seed(1)\\n    # The compel library returns both the \\nconditioning vectors and the pooled prompt \\nembeds\\n    conditioning, pooled = compel(prompt)\\n    # We pass the conditioning and pooled \\nprompt embeds to the pipeline\\n    image = pipeline(\\n        prompt_embeds=conditioning,\\n        pooled_prompt_embeds=pooled,\\n        num_inference_steps=30,\\n        generator=generator,\\n    ).images[0]\\n    images.append(image)\\nmake_image_grid(images, rows=1, cols=3)\\n  0%|          | 0/30 [00:00<?, ?it/s]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='0%|          | 0/30 [00:00<?, ?it/s]\\n  0%|          | 0/30 [00:00<?, ?it/s]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 8-3. Images showcasing prompt \"a humanoid robot\\neating pasta“, \"a humanoid robot eating pasta+++\"\\nand \"a humanoid robot eating pasta\" with Van Gogh style\\nprompt merging\\nThe is equivalent to multiplying the prompt weight by 1.1, and\\nthe - is equivalent to a 0.9 multiplication. Besides using the and\\n-, one can also weigh the tokens as follows: a robot eating\\n(pasta)1.2. For more references at the compel library, check\\nout its oﬃcial reference guide.\\nEditing Diﬀusion Images with Semantic\\nGuidance\\nAs mentioned, a few techniques exist for image editing for\\ndiﬀusion-generated images; while cross-attention control with\\nprompt-to-prompt is a popular way to provide edits, Semantic\\nGuidance (SEGA) allows for more ﬁne-grained controls and\\nprecise edits.\\nSEGA operates by manipulating the model’s noise estimates at\\neach step of the reverse diﬀusion process. This dynamic noise\\nadjustment allows SEGA to perform semantic edits in the latent\\nspace based on textual descriptions. By dynamically adjusting\\nthe predicted noise, SEGA ensures that the modiﬁcation is'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='steered toward the semantic direction derived from the text\\nembeddings. The method calculates gradients of the text\\nembeddings relative to the latent space, eﬀectively guiding the\\nimage generation or modiﬁcation toward the desired semantic\\noutcomes. This process is achieved without needing to retrain\\nor modify the original architecture of the model, allowing for\\ndynamic and directed changes based solely on textual input.\\nLet’s begin by showcasing the\\nSemanticStableDiffusionPipeline to generate an image\\nof a photo of the face of a man, as shown in Figure 8-4.\\nfrom diffusers import \\nSemanticStableDiffusionPipeline\\nsemantic_pipeline = \\nSemanticStableDiffusionPipeline.from_pretrained(\\n    \"CompVis/stable-diffusion-v1-4\", \\ntorch_dtype=torch.float16, variant=\"fp16\"\\n).to(device)\\ngenerator = \\ntorch.Generator(device=device).manual_seed(100)\\nout = semantic_pipeline(\\n    prompt=\"a photo of the face of a man\",'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='negative_prompt=\"low quality, deformed\",\\n    generator=generator,\\n)\\nout.images[0]\\n  0%|          | 0/51 [00:00<?, ?it/s]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 8-4. First image generated with the prompt \"a photo\\nof the face of a man\"\\nNow, let’s guide the prompt in the direction of an\\nediting_prompt to make the man smile, as shown in Figure\\n8-5. SEGA contains a few important parameters for its editing\\ncapabilities:\\nedit_guidance_scale: How strongly the model should\\nfollow the edits.\\nedit_warmup_steps: How many denoising steps should\\nthe model start with before applying semantic guidance.\\nedit_threshold: What percentage of the pixels of the\\noriginal image should be preserved.\\nreverse_editing_direction: Whether the edit should\\ninclude (False) or remove (True) the concept.\\ngenerator = \\ntorch.Generator(device=device).manual_seed(100)\\nout = semantic_pipeline(\\n    prompt=\"a photo of the face of a man\",\\n    negative_prompt=\"low quality, deformed\",\\n    editing_prompt=\"smiling, smile\",\\n    edit_guidance_scale=4,\\n    edit_warmup_steps=10,\\n    edit_threshold=0.99,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='edit_momentum_scale=0.3,\\n    edit_mom_beta=0.6,\\n    reverse_editing_direction=False,\\n    generator=generator,\\n)\\nout.images[0]\\n  0%|          | 0/51 [00:00<?, ?it/s]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 8-5. Image with \"a photo of the face of a man\"\\nedited with \"smiling, smile\" prompt\\nLet’s do another edit, this time to make the man wear glasses.\\ngenerator = \\ntorch.Generator(device=device).manual_seed(100)\\nout = semantic_pipeline(\\n    prompt=\"a photo of the face of a man\",\\n    negative_prompt=\"low quality, deformed\",\\n    editing_prompt=\"glasses, wearing \\nglasses\",\\n    reverse_editing_direction=False,\\n    edit_warmup_steps=10,\\n    edit_guidance_scale=4,\\n    edit_threshold=0.99,\\n    edit_momentum_scale=0.3,\\n    edit_mom_beta=0.6,\\n    generator=generator,\\n)\\nout.images[0]\\n  0%|          | 0/51 [00:00<?, ?it/s]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 8-6. Image with \"a photo of the face of a man\"\\nedited with \"glasses, wearing glasses\" prompt\\nFinally, let’s do multiple edits simultaneously as shown in\\nFigure 8-7. The only diﬀerence is that editing_prompt is now\\na list of prompts, and the key parameters\\n(edit_warmup_steps and so on) must also be lists.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='generator = \\ntorch.Generator(device=device).manual_seed(100)\\nout = semantic_pipeline(\\n    prompt=\"a photo of the face of a man\",\\n    negative_prompt=\"low quality, deformed\",\\n    editing_prompt=[\\n        \"smiling, smile\",\\n        \"glasses, wearing glasses\",\\n    ],\\n    reverse_editing_direction=[False, False],\\n    edit_warmup_steps=[10, 10],\\n    edit_guidance_scale=[6, 6],\\n    edit_threshold=[0.99, 0.99],\\n    edit_momentum_scale=0.3,\\n    edit_mom_beta=0.6,\\n    generator=generator,\\n)\\nout.images[0]\\n  0%|          | 0/51 [00:00<?, ?it/s]\\n/home/osanseviero/miniconda3/envs/book/lib/python\\npackages/torch/nn/modules/conv.py:456: \\nUserWarning: Plan failed with a \\ncudnnException: \\nCUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR:'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='cudnnFinalize Descriptor Failed cudnn_status: \\nCUDNN_STATUS_NOT_SUPPORTED (Triggered \\ninternally at \\n../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\\n  return F.conv2d(input, weight, bias, \\nself.stride,\\nFigure 8-7. Image with \"a photo of the face of a man\"\\nedited with both \"smiling, smile\" and \"glasses, wearing'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='glasses\" prompts together\\nReal Image Editing via Inversion\\nInversion is a technique for bringing a real image back into the\\nlatent space of a pre-trained generative model. The technique\\nshowed promising results with GANs and was successfully\\nimplemented in guided diﬀusion models.\\nThis allows us to answer a question you may have raised when\\nlearning about Semantic Guidance: \"but what if we want\\nto edit images from the real world instead?“. One\\nalternative is to use the image-to-image approach presented\\nearlier. However, it can only provide very limited edits and does\\nnot always produce the expected results, as the results can\\nchange dramatically without much control. To solve that, we\\ncan combine a diﬀusion image editing technique (such as\\nprompt-to-prompt or Semantic Guidance) with an inversion\\ntechnique to give more ﬁne-grained editing.\\nThe inversion process for guided diﬀusion models involves\\nusing a inversed scheduler of a denoiser. The ﬁrst denoiser with\\ninversion introduced was DDIM Inverse, which can predict\\nsamples from previous timesteps into the latent space by'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='performing DDIM sampling in reverse order (starting with the\\nreal image and gradually adding noise to it). Then, when you\\ndenoise it, you have your original image as expected. By itself,\\nthis is not super interesting; you’ve just used noisy latents to\\nreconstruct an image you already had. However, inversion can\\nbecome powerful if your goal is to provide edits to the image.\\nThe most naive way to edit via inversion is to:\\n1. Obtain the DDIM Inverse of a real prompt and a description\\n(e.g., A photo of a horse in the field).\\n2. Modify the prompt to your target image (e.g., A photo of\\na zebra in the field).\\n3. Reconstruct the image with the modiﬁed prompt.\\nThe results are shown in Figure 8-8.:\\nFigure 8-8. Examples of editing with the DDIM Inversion\\ntechnique from An Edit Friendly DDPM Noise Space: Inversion\\nand Manipulations by Inbar et. al.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='While these results are interesting, they aren’t ideal for real\\nimage editing. More advanced techniques, such as better\\ninversions with DDPM Inversion with an edit-friendly noise\\nspace, can provide better reconstructions but still cannot oﬀer\\nthe broadest range of edits.\\nWe can combine an inversion technique with editing\\ntechniques to provide a wide range of edits for real images. For\\nexample, Null-text Inversion for Editing Real Images using\\nGuided Diﬀusion Models  and LEDITS  utilize prompt-to-\\nprompt and Semantic Guidance, respectively.\\nThese techniques leverage the editing techniques we learned\\nbefore, but instead of providing the edits in the latent space\\nwith an image that would be generated via a prompt, it happens\\nin the latent space of the reconstruction of the image once it is\\ninverted. Here, we will learn how to provide real-world image\\nedits with LEDITS++, leveraging the already learned Semantic\\nGuidance with inversion.\\nEditing with LEDITS++\\nLEDITS++ combines two techniques we just learned: Semantic\\nGuidance and Inversion. It further implements a technique to\\nground your edit with the cross-attention and noise masks\\n5 6'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='produced by the model. This combination allows us to edit real\\nimages with the same parameters we learned for SEGA.\\nThe process in which LEDITS++ works is:\\n1. Apply inversion to bring the image we are interested in into\\na format the model can manipulate\\n2. We decide our editing editing_prompt list and the\\nediting directions (whether to add or remove such concept)\\n3. We apply the same edit_guidance_scale and\\nedit_threshold we learned in Semantic Guidance for\\nour edits\\nfrom diffusers import \\nLEditsPPPipelineStableDiffusion\\n# Load the model as usual\\npipe = \\nLEditsPPPipelineStableDiffusion.from_pretrained(\\n    \"Lykon/dreamshaper-8\", \\ntorch_dtype=torch.float16, variant=\"fp16\"\\n)\\npipe.to(device)\\nimage = \\nload_image(SampleURL.ManInGlasses).convert(\"RGB\")'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='# Invert the image, gradually adding noise to \\nit so\\n# it can be denoised with modified \\ndirections,\\n# effectively providing an edit\\npipe.invert(image=image, \\nnum_inversion_steps=50, skip=0.2)\\n# Edit the image with an editing prompt\\nedited_image = pipe(\\n    editing_prompt=[\"glasses\"],\\n    # tell the model to remove the glasses by \\nediting the direction\\n    reverse_editing_direction=[True],\\n    edit_guidance_scale=[1.5],\\n    edit_threshold=[0.95],\\n).images[0]\\nmake_image_grid([image, edited_image], \\nrows=1, cols=2)\\nFigure 8-9. An original image (left) and the other edited to\\nremove the man’s glasses (right)\\nReal Image Editing via Instruction Fine-'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Tuning\\nAnother way to provide real image editing for diﬀusion models\\nis to ﬁne-tune the model exclusively for this task. This approach\\nwas pioneered by the InstructPix2Pix paper . For training, it\\nrequires a dataset of edit instruction pairs containing the\\noriginal image, edit instructions, and the edited image.\\nThe Stable Diﬀusion model is then appended with additional\\ninput channels to the ﬁrst convolutional layer, allowing it to\\ntake in an image input. The model is then trained with the same\\ntext conditioning mechanism intended for captions in the\\noriginal Stable Diﬀusion model, modiﬁed to take an edit\\ninstruction as its prompt.\\n7'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 8-10. Examples of edits with the InstructPix2Pix\\ntechnique\\nFurther trained and improved InstructPix2Pix models emerged\\nafter the publication of the original paper. The most prominent\\nis CosXL Edit by Stability AI, trained on a variant Stable\\nDiﬀusion XL to perform high quality edits, as shown in Figure\\n8-11.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The CoSXL repository is gated, visit the model page in Hugging\\nFace read the license and click the button to accept it, if you\\nagree to the terms. Run huggingface-cli login in a\\nterminal session to log-in. You’ll be asked for an access token\\nthat you can create in your settings page. If you are\\ndownloading the model from a Google Colab session, you can\\nset up a HF_TOKEN secret or environment variable and give\\npermission to your notebook to use it.\\nfrom diffusers import (\\n    EDMEulerScheduler,\\n    StableDiffusionXLInstructPix2PixPipeline,\\n)\\nfrom huggingface_hub import hf_hub_download\\nedit_file = hf_hub_download(\\n    repo_id=\"stabilityai/cosxl\", \\nfilename=\"cosxl_edit.safetensors\"\\n)\\n# from_single_file loads a diffusion model \\nfrom a single diffusers file\\npipe_edit = \\nStableDiffusionXLInstructPix2PixPipeline.from_sin\\n    edit_file, num_in_channels=8, \\nis_cosxl_edit=True, torch_dtype=torch.float16\\n)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='# The model was trained so that the \\nEDMEulerScheduler\\n# is the correct noise scheduler for \\ndenoising\\npipe_edit.scheduler = EDMEulerScheduler(\\n    sigma_min=0.002,\\n    sigma_max=120.0,\\n    sigma_data=1.0,\\n    prediction_type=\"v_prediction\",\\n    sigma_schedule=\"exponential\",\\n)\\npipe_edit.to(device)\\nprompt = \"make it a cloudy day\"\\nimage = load_image(SampleURL.Mountain)\\nedited_image = pipe_edit(\\n    prompt=prompt, image=image, \\nnum_inference_steps=20\\n).images[0]\\nmake_image_grid([image, edited_image], \\nrows=1, cols=2)\\nFigure 8-11. An example edit done with CosXL'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='ControlNet\\nControlNet is a model for controlling image diﬀusion models by\\nconditioning the model with additional conditions besides the\\ntext-prompt condition. The ControlNet models are trained over\\na trainable copy of the original model. Unlike direct ﬁne-tuning,\\nControlNet preserves the original model completely and injects\\nall the new conditions into this trainable copy of the original\\nmodel. This allows for preserving the model’s capabilities—\\neven if your ControlNet is trained with relatively few samples.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 8-12. ControlNet examples with image input, canny\\nedges and open pose conditionings and the generated output\\nThe ControlNet models are trained to take in various\\nconditions, two of which you see in the Figure 8-11: canny\\nedges and human pose (OpenPose). Besides those, there are\\nalso depth maps, scribble, segmentation, lineart, and\\nmore. Check all the oﬃcial Stable Diﬀusion v1-5 ControlNets\\nhere and check Hugging Face Models for community trained\\nones.\\nControlNet’s versatility and eﬃciency make it a powerful tool\\nfor various image generation and manipulation tasks. By\\nallowing ﬁne-grained control over the output while\\nmaintaining the original model’s capabilities, ControlNet opens\\nup new possibilities for creative and practical applications. For\\ninstance, it can be used in ﬁelds such as fashion design to\\nvisualize clothing on diﬀerent body poses, in architecture to\\ngenerate building designs based on rough sketches, or in ﬁlm\\npre-production to quickly create storyboards from simple line\\ndrawings. The ability to use diﬀerent types of control inputs,\\nsuch as edges, pose estimations, or depth maps, provides\\ncreatives and developers with a ﬂexible framework to guide the\\nimage generation process according to their speciﬁc needs.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 8-13. Duck ControlNet Canny example\\nWe’ll use an auxiliary library, controlnet_aux, to pre-process the\\ninput images into the desired condition format for the oﬃcial\\npre-trained ControlNet models. The diﬀusers library authors\\nhave also trained ControlNets for Stable Diﬀusion XL. You can\\nﬁnd them in its Hugging Face collection.\\nFirst, we load the main model with\\nStableDiffusionXLControlNetPipeline. This pipeline also\\nexpects a ControlNelModel parameter with the model that\\nprovides additional conditioning to the UNet during denoising.\\nfrom diffusers import ControlNetModel, \\nStableDiffusionXLControlNetPipeline\\ncontrolnet = ControlNetModel.from_pretrained(\\n    \"diffusers/controlnet-depth-sdxl-1.0\",'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='torch_dtype=torch.float16,\\n    variant=\"fp16\",\\n)\\ncontrolnet_pipeline = \\nStableDiffusionXLControlNetPipeline.from_pretrain\\n    \"stabilityai/stable-diffusion-xl-base-\\n1.0\",\\n    controlnet=controlnet,\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\",\\n)\\ncontrolnet_pipeline.enable_model_cpu_offload()  \\n# Optional, saves VRAM\\ncontrolnet_pipeline.to(device)\\nThen, we can use controlnet_aux to pre-process the image into\\nthe desired condition format. Here, we are using the\\nMidasDetector pre-processing as it is a depth estimation\\nmodel. This model takes in an image input and outputs an\\nestimated depth map, which is exactly what we need to feed in\\nfor our diffusers/controlnet-depth-sdxl-1.0 model.\\nfrom controlnet_aux import MidasDetector\\nfrom PIL import Image'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='original_image = \\nload_image(SampleURL.WomanSpeaking)\\noriginal_image = original_image.resize((1024, \\n1024))\\n# loads the MiDAS depth detector model\\nmidas = \\nMidasDetector.from_pretrained(\"lllyasviel/Annotat\\n# Apply MiDAS depth detection\\nprocessed_image_midas = \\nmidas(original_image).resize(\\n    (1024, 1024), Image.BICUBIC\\n)\\nFinally, we can pass the prompt and the processed image to the\\npipeline to generate the new image. The\\ncontrolnet_conditioning_scale will dictate how strongly\\nthe condition will inﬂuence the ﬁnal result, sown in Figure 8-14.\\nimage = controlnet_pipeline(\\n    \"A colorful, ultra-realistic masked super \\nhero singing a song\",\\n    image=processed_image_midas,\\n    controlnet_conditioning_scale=0.4,\\n    num_inference_steps=30,\\n).images[0]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='make_image_grid([original_image, \\nprocessed_image_midas, image], rows=1, \\ncols=3)\\nFigure 8-14. A ControlNet output with original image, depth\\nmap and edited image with a masked singer\\nWhile training your ControlNets is outside the scope of this\\nbook, if you are interested in this subject, we recommend you\\ncheck the Hugging Face blog’s Train your ControlNet with\\ndiﬀusers.\\nImage Prompting and Image\\nVariations\\nText prompts are great, but sometimes, more is needed to\\nexpress our intent to the model. Prompting diﬀusion models\\nwith images allows us to amplify our input range to the visual\\nrealm.\\nImage Variations\\nTo ﬂourish our creativity, we sometimes need to look at\\nsomething similar but diﬀerent. That is the purpose of image'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='variations: to take a given image and reinterpret it, providing a\\nfamiliar yet diﬀerent image generation. Let’s explore two\\napproaches: using CLIP image embeddings and IP-Adapter.\\nUsing CLIP image embeddings: As we learned in Chapter 5,\\nStable Diﬀusion uses CLIP as its text-encoder. Apart from the\\ntext encoder, CLIP can also be used to produce image\\nembeddings. Some diﬀusion models were trained in such a way\\nthat they could use image embeddings as input to generate new\\nimage variations. That’s the case of Karlo and Kandinsky. For\\nStable Diﬀusion, this does not work out of the box. However, it\\ncan be achieved with ﬁne-tuning. Stable Diﬀusion Image\\nVariations is a ﬁne-tuned Stable Diﬀusion v1-5 model that\\naccepts CLIP Image Embeddings as its inputs. You can try its\\ndemo here.\\nUsing IP-Adapter: Another approach that does not require ﬁne-\\ntuning the model is to utilize pre-trained IP-Adapters (Image\\nPrompt Adapters). These adapters allow prompting with\\nimages, allowing for image variations and a wide range of other\\nuse cases of image prompting, such as style transfer, subject\\nidentity preservation, and structure control'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 8-1. Figure 8-15. A diagram of the IP-Adapter architecture\\nAs shown in Figure 8-15, IP-Adapter comprises two\\ncomponents: an encoder that extracts features from the image\\nand decoupled cross-attention modules that get attached to the\\npre-trained Stable Diﬀusion UNet.\\nUsing IP Adapter just requires some minimal changes over the\\nbase SDXL pipeline:\\nUse load_ip_adapter() to load the IP Adapter model.\\nSpecify the IP Adapter scale with\\nset_ip_adapter_scale().\\nAnd that’s it! To do image variations with IP Adapter, we can\\nprovide the reference image and an empty prompt.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from diffusers import \\nStableDiffusionXLPipeline\\nsdxl_base_pipeline = \\nStableDiffusionXLPipeline.from_pretrained(\\n    \"stabilityai/stable-diffusion-xl-base-\\n1.0\",\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\",\\n)\\nsdxl_base_pipeline.to(device)\\n# We load the IP Adapter too\\nsdxl_base_pipeline.load_ip_adapter(\\n    \"h94/IP-Adapter\", \\nsubfolder=\"sdxl_models\", weight_name=\"ip-\\nadapter_sdxl.bin\"\\n)\\n# We can set the scale of how strong we\\n# want our IP Adapter to impact our overall \\nresult\\nsdxl_base_pipeline.set_ip_adapter_scale(0.8)\\nimage = load_image(SampleURL.ItemsVariation)\\noriginal_image = image.resize((1024, 1024))\\n# Create the image variation'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='generator = \\ntorch.Generator(device=device).manual_seed(1)\\nvariation_image = sdxl_base_pipeline(\\n    prompt=\"\",\\n    ip_adapter_image=original_image,\\n    num_inference_steps=25,\\n    generator=generator,\\n).images\\nmake_image_grid([original_image, \\nvariation_image[0]], rows=1, cols=2)\\nFigure 8-16. Images side by side, left is the original image, right\\nis the image variation\\nWith IP Adapter, we generated a reinterpretation of our image\\n(you can see on Figure 8-16 that it removed the whipped cream\\n— we can live with that!), which is really cool and enables fun\\nuse cases. However, IP Adapters are way more powerful than\\nwhat we’ve done so far. Image prompting can allow the\\ncombination of IP Adapter with text prompting and the other\\ncontrols we learned in this chapter.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Image Prompting\\nIP adapters allow for more than just generating image\\nvariations. They allow you to utilize an image as one of the\\nprompts, which enables you to apply techniques like style\\ntransfer and all the other techniques we learned in this chapter,\\nadding an image prompt and a text prompt.\\nStyle Transfer\\nWhile IP Adapter works great out of the box with style transfer,\\nthe researchers behind the InstantStyle  paper realized that if\\nthe IP Adapter is applied to only certain blocks of the UNet of\\nthe Stable Diﬀusion model, it can aﬀect exclusively the image\\nstyle. The idea is that we can pass a prompt and a style image to\\nthe model, and the model will generate an image that follows\\nthe prompt but with the style of the style image. In this\\nexample, we are going to apply the style of the work by the\\nBrazilian painter Tarsila do Amaral, \"O Mamoeiro“. The main\\ndiﬀerences are the scale of the IP Adapter and the input\\nprompt, which is no longer empty.\\n# We load the model and the IP Adapter, just \\nas before\\npipeline = \\n8'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='StableDiffusionXLPipeline.from_pretrained(\\n    \"stabilityai/stable-diffusion-xl-base-\\n1.0\", torch_dtype=torch.float16\\n).to(device)\\n# Load the IP Adapter into the model\\npipeline.load_ip_adapter(\\n    \"h94/IP-Adapter\", \\nsubfolder=\"sdxl_models\", weight_name=\"ip-\\nadapter_sdxl.bin\"\\n)\\n# We are applying the IP Adapter only to the \\nmid block,\\n# which is where it should be mapped to the \\nstyle in SDXL\\nscale = {\"up\": {\"block_0\": [0.0, 1.0, 0.0]}}\\npipeline.set_ip_adapter_scale(scale)\\nimage = load_image(SampleURL.Mamoeiro)\\noriginal_image = image.resize((1024, 1024))\\n# Run inference to generate the stylized \\nimage\\ngenerator = \\ntorch.Generator(device=device).manual_seed(0)\\nvariation_image = pipeline(\\n    prompt=\"a cat inside of a box\",'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='ip_adapter_image=original_image,\\n    num_inference_steps=25,\\n    generator=generator,\\n).images\\nmake_image_grid([original_image, \\nvariation_image[0]], rows=1, cols=2)\\nLoading pipeline components...:   0%|          \\n| 0/7 [00:00<?, ?it/s]\\n  0%|          | 0/25 [00:00<?, ?it/s]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 8-17. Images side by side, left is the original image \"O\\nmamoeiro“, right a generated image on the same style\\nAdditional Controls\\nNow, to wrap up this chapter, we can show how the multiple\\ntechniques we learn all composite together. We will add our IP\\nAdapter \"O Mamoeiro\" style to our previous example of the\\nControlNet masked singer, which you can see in Figure 8-18.\\ncontrolnet = ControlNetModel.from_pretrained(\\n    \"diffusers/controlnet-depth-sdxl-1.0\", \\ntorch_dtype=torch.float16\\n)\\n# Load the ControlNet pipeline\\ncontrolnet_pipeline = \\nStableDiffusionXLControlNetPipeline.from_pretrain\\n    \"stabilityai/stable-diffusion-xl-base-\\n1.0\",\\n    controlnet=controlnet,\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\",\\n)\\ncontrolnet_pipeline.to(device)\\n# Load the IP Adapter'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='controlnet_pipeline.load_ip_adapter(\\n    \"h94/IP-Adapter\", \\nsubfolder=\"sdxl_models\", weight_name=\"ip-\\nadapter_sdxl.bin\"\\n)\\n# We are applying the IP Adapter only to the \\nmid block,\\n# which is where it should be mapped to the \\nstyle in SDXL\\nscale = {\\n    \"up\": {\"block_0\": [0.0, 1.0, 0.0]},\\n}\\ncontrolnet_pipeline.set_ip_adapter_scale(scale)\\n# Load the original image\\noriginal_image = \\nload_image(SampleURL.WomanSpeaking)\\noriginal_image = original_image.resize((1024, \\n1024))\\n# Load the style image\\nstyle_image = load_image(SampleURL.Mamoeiro)\\nstyle_image = style_image.resize((1024, \\n1024))\\n# Apply the MiDAS depth estimation\\nprocessed_image_midas = \\nmidas(original_image).resize('),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='(1024, 1024), Image.BICUBIC\\n)\\nimage = controlnet_pipeline(\\n    \"A masked super hero singing a song\",\\n    image=processed_image_midas,\\n    ip_adapter_image=style_image,\\n    controlnet_conditioning_scale=0.5,\\n).images[0]\\nmake_image_grid(\\n    [original_image, style_image, \\nprocessed_image_midas, image], rows=1, cols=4\\n)\\nLoading pipeline components...:   0%|          \\n| 0/7 [00:00<?, ?it/s]\\n  0%|          | 0/50 [00:00<?, ?it/s]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 8-18. Grid with 4 images, 1. original image, 2. style\\nreference, 3. depth map, 4. generated image with prompt \"A\\nmasked super hero singing a song\"\\nProject Time: Your Creative Canvas\\nNow it’s your creative canvas 🎨 . In this chapter, we’ve provided\\nmultiple mechanisms for creative applications and expression.\\nIt’s your time to exercise that now; your challenge in this\\nchapter is to try and combine at least two techniques we\\nlearned here into your own creative ways - similarly to how we\\nhave done the masked superhero in the \"memoirs\" style. Some\\nideas to guide your creative journey:\\nUse ControlNet Canny edges to re-imagine your living space\\nwith an IP-Adapter style applied from a reference you hold\\ndear.\\nDraw a rough sketch of a city on a piece of paper, take a\\npicture and apply image-to-image to turn that city into a\\nsolarpunk utopia. Now use the style of this solapunk utopia\\ngenerated by your sketch as a reference and create\\nbuildings, transportation systems and inhabitants in your\\nnew city.\\nOr ﬁnd other ways to explore this latent space!'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Summary\\nIn this chapter, we explored various creative applications that\\nprovide more control and extend the capabilities of text-to-\\nimage models with ﬁne-grained controllability, increasing the\\nrange of inputs these models can take. As these techniques are\\ncomposable and compatible with one another, we allow for\\ncomplex creative pipelines and artistic processes to go beyond\\njust inputting text and getting an image as an output, be it with\\na more tuned prompt control, image transformations,\\nvariations, style or structural reference in images creatives can\\nbe super-charged with Machine Learning and utilize these\\nmechanisms as a tool.\\nHowever, going beyond generating images, especially when\\nbringing real images into the latent space of the model, brings\\nin new challenges. Key ethical concerns include\\nmisinformation, deception, and ownership. Making image\\nediting accessible to everyone (and not only Photoshop\\nspecialists, for example) creates new opportunities and\\nchallenges, such as image manipulations that can be used to\\nspread misinformation and deceive people with deepfakes or\\nnon-existent content. The question of ownership is also key:\\nartistic styles can be remixed, but the legal and ethical'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='boundaries for when such remix is done fairly is still an open\\nquestion. Mitigation strategies such as watermarking already\\nexist in libraries like diﬀusers; however, there are more\\ndiscussions to be held at a societal level about how to handle\\nthese new abilities Machine Learning gives us.\\nNonetheless, the creative potential of these new models is an\\nexciting development. When used ethically and responsibly,\\nthey will certainly empower creatives with tooling that goes\\nbeyond imagination.\\nFor further readings, we suggest the following resources: * IP\\nAdapter: all you need to know https://stable-diﬀusion-art.com/ip-\\nadapter/ * Train your ControlNet with diﬀusers\\nhttps://huggingface.co/blog/train-your-controlnet * Diﬀusers\\ninpainting guide https://huggingface.co/docs/diﬀusers/en/using-\\ndiﬀusers/inpaint * Instruction tuning Stable Diﬀusion with\\nInstructPix2Pix https://huggingface.co/blog/instruction-tuning-sd\\n* Read the papers in the references section.\\nExercises\\n1. Explain how inpainting diﬀers from image-to-image\\ntransformation and provide an example of a practical'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='application.\\n2. How can prompt weighting help overcome the limitations\\nof the diﬀusion models?\\n3. What are the key diﬀerences between prompt-to-prompt\\nediting and semantic guidance?\\n4. How does ControlNet enhance the capabilities of diﬀusion\\nmodels? Give examples of conditions that can be used with\\nControlNet.\\n5. What is \"Inversion\" in the context of text-to-image\\nmodels, and what does it allow us to do?\\n6. Describe the potential risks of ﬁne-tuning text-to-image\\nmodels on biased datasets.\\nYou can ﬁnd the solutions to these exercises in the GitHub\\nrepository of the book.\\nReferences\\n1. Brack, Manuel, et al.\\xa0LEDITS++: Limitless Image Editing\\nusing Text-to-Image Models. arXiv, 30 November 2023.\\narXiv.org, http://arxiv.org/abs/2311.16711.\\n2. Brack, Manuel, et al.\\xa0SEGA: Instructing Text-to-Image\\nModels using Semantic Guidance. arXiv, 29 January 2023.\\narXiv.org, http://arxiv.org/abs/2301.12247.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='3. Chefer, Hila, et al.\\xa0Attend-and-Excite: Attention-Based\\nSemantic Guidance for Text-to-Image Diﬀusion Models.\\narXiv, 31 January 2023. arXiv.org,\\nhttp://arxiv.org/abs/2301.13826.\\n4. Hertz, Amir, et al.\\xa0Prompt-to-prompt Image Editing with\\nCross Attention Control. arXiv, 3 August 2022. arXiv.org,\\nhttp://arxiv.org/abs/2208.01626.\\n5. Mokady, Ron, et al.\\xa0Null-text Inversion for Editing Real\\nImages using Guided Diﬀusion Models. arXiv, 17 November\\n2022. arXiv.org, http://arxiv.org/abs/2211.09794.\\n6. Podell, Dustin, et al.\\xa0SDXL: Improving Latent Diﬀusion\\nModels for High-Resolution Image Synthesis. arXiv, 4 July\\n2023. arXiv.org, http://arxiv.org/abs/2307.01952\\n7. Wang, Haofan, et al.\\xa0InstantStyle: Free Lunch towards Style-\\nPreserving in Text-to-Image Generation. arXiv, 4 April 2024.\\narXiv.org, http://arxiv.org/abs/2404.02733.\\n8. Ye, Hu, et al.\\xa0IP-Adapter: Text Compatible Image Prompt\\nAdapter for Text-to-Image Diﬀusion Models. arXiv, 14 August\\n2023. arXiv.org, http://arxiv.org/abs/2308.06721.\\n9. Zhang, Lvmin, et al.\\xa0Adding Conditional Control to Text-to-\\nImage Diﬀusion Models. arXiv, 11 February 2023. arXiv.org,\\nhttps://arxiv.org/abs/2302.05543.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Hertz, Amir, et al.\\xa0Prompt-to-prompt Image Editing with Cross Attention Control.\\narXiv, 3 August 2022. arXiv.org, https://arxiv.org/abs/2208.01626\\n Chefer, Hila, et al.\\xa0Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-\\nImage Diﬀusion Models. arXiv, 31 January 2023. arXiv.org,\\nhttps://arxiv.org/abs/2301.13826\\n Brack, Manuel, et al.\\xa0SEGA: Instructing Text-to-Image Models using Semantic\\nGuidance. arXiv, 29 January 2023. arXiv.org, https://arxiv.org/abs/2301.12247\\n Pooling involves converting the token embeddings into a single ﬁxed-length\\nembedding that reﬂects the entire sequence, just as we did with sentence\\nembeddings\\n Mokady, Ron, et al.\\xa0Null-text Inversion for Editing Real Images using Guided Diﬀusion\\nModels. arXiv, 17 Nov.\\xa02022. arXiv.org, https://arxiv.org/abs/2211.09794.\\n Brack, Manuel et al.\\xa0LEDITS++: Limitless Image Editing using Text-to-Image Models .\\narXiv, 29 Nov.\\xa02023. arXiv.org, https://arxiv.org/abs/2311.16711.\\n Brooks, Tim, et al.\\xa0InstructPix2Pix: Learning to Follow Image Editing Instructions.\\narXiv, 17 Nov.\\xa02022. arXiv.org, https://arxiv.org/abs/2211.09800\\n Wang, Haofan, et al.\\xa0InstantStyle: Free Lunch towards Style-Preserving in Text-to-\\nImage Generation. arXiv, 3 Apr.\\xa02024. arXiv.org, https://arxiv.org/abs/2404.02733.\\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Chapter 9. Generating Audio\\nA NOTE FOR EARLY RELEASE READERS\\nWith Early Release ebooks, you get books in their earliest form\\n—the authors’ raw and unedited content as they write—so you\\ncan take advantage of these technologies long before the oﬃcial\\nrelease of these titles.\\nThis will be the ninth chapter of the ﬁnal book. Please note that\\nthe GitHub repo will be made active later on.\\nIf you have comments about how we might improve the content\\nand/or examples in this book, or if you notice missing material\\nwithin this chapter, please reach out to the editor at\\njleonard@oreilly.com.\\nIn Chapter 1, we glimpsed at the potential of audio generation\\nwith a transformers pipeline based on the MusicGen model by\\nMeta. This chapter will dive into generative audio using both\\ndiﬀusion and transformer-based techniques, which will\\nintroduce a new set of exciting challenges and applications.\\nImagine if you could remove all background noise in real-time\\nduring a call, get high-quality transcriptions and summaries of'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='conferences, or if a singer could re-generate their songs in other\\nlanguages. You could even generate a theme of Mozart and\\nBillie Eilish’s compositions that gets a mariachi-infused twist.\\nWell, that’s the ﬁeld’s trajectory, exciting times ahead.\\nWhat kind of things can one do with ML and audio? The two\\nmost common tasks are transcribing Speech to Text (Automatic\\nSpeech Recognition) and generating speech from text (Text to\\nSpeech). In Automatic Speech Recognition (ASR), a model\\nreceives as input audio of someone (or multiple people)\\nspeaking, and outputs the corresponding text. For some models,\\nthe output captures additional information, such as which\\nperson is speaking or the times when somebody said\\nsomething. ASR systems are widely used, from virtual speech\\nassistants to caption generators. Thanks to many open-access\\nmodels made available to the public in recent years, there has\\nbeen exciting research on multilingualism and running the\\nmodels directly on edge.\\nIn Text to Speech (TTS), a model generates synthetic and,\\nhopefully, realistic speech. As with ASR, in TTS there has been\\nconsiderable interest in running models on-device as well as\\nmultilingualism. TTS also presents its own set of challenges,\\nsuch as generating audios with multiple speakers, making the\\nvoices sound more natural, and bringing intonation, pauses,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='emotion markers, pitch control, accent, and other\\ncharacteristics in the generations.\\nAlthough TTS and ASR are the most popular tasks, there is a\\nplethora of other things one can do with ML and audio, some of\\nwhich are shown in Figure 9-1:\\nText to Audio: Text to Speech can be generalized to Text to\\naudio, where, based on a prompt, a model can generate\\nmelodies, sound eﬀects, and songs.\\nVoice cloning: A person’s voice, including tone, pitch, and\\nprosody, is preserved to generate new sounds.\\nAudio classiﬁcation: The model classiﬁes a provided\\naudio. Typical examples are command recognition and\\nspeaker identiﬁcation.\\nVoice enhancement: The model removes noise from the\\naudio and cleans the voice to be clearer.\\nAudio translation: The model receives audio with a source\\nlanguage X and outputs audio with a target language Y.\\nSpeaker Diarization: The model identiﬁes the speaker at a\\nspeciﬁc time.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 9-1. Figure 9-1. Some examples of audio tasks\\nAudio-related tasks are challenging for multiple reasons. First,\\nworking with the raw audio signal is more complex and less\\nintuitive than working with text. For many applications, the\\nexpectation is that audio models can perform in real-time or\\non-device, which brings many constraints regarding model size\\nand inference speed. For example, current diﬀusion models\\nwould be too slow if you wanted to use them for interactive\\ntranslation. Finally, evaluating generative audio models can be\\nchallenging. How do you measure if the quality of a song\\ngenerated by a model is good?\\nWe can use multiple tools and hundreds of open-access models\\nand datasets for these tasks. Common Voice, a popular crowd-'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='sourced dataset by the Mozilla Foundation, contains over two\\nthousand hours of audio ﬁles and their corresponding text in\\nover a hundred languages. Apart from Common Voice, many\\nother popular audio datasets, such as LibriSpeech, VoxPopuli,\\nand GigaSpeech, are also available, each with its own domain\\nand use cases. Just as there are many open-source datasets, we\\ncan also use many open-access models. This chapter will\\nexplore transformers-based models such as Meta’s Wav2Vec2,\\nOpenAI Whisper, Microsoft SpeechT5, and Suno Bark. We’ll also\\nexplore exciting diﬀusion models that can generate songs, such\\nas Stable Diﬀusion (but for songs), Dance Diﬀusion, and\\nAudioLDM. Although jumping into another modality might be\\ndaunting, many of the tools we’ve collected in our generative\\njourney are about to be used.\\nAudio Data\\nTo get started, we’ll learn about how audio data is structured\\nand how to use it. We’ll explore the LibriSpeech dataset, which\\ncontains over 1000 hours of books read out loud and is useful\\nfor training and evaluating speech recognition systems. One of\\nthe ﬁrst challenges with audio datasets is that they tend to be\\nlarge, so loading simultaneously all the data might not be'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='feasible. Audio datasets can quickly spawn terabytes of data\\nand not ﬁt in a hard drive.\\nWe can use load_dataset_builder() to get a better\\noverview of the dataset structure without loading all the data.\\nfrom datasets import load_dataset_builder\\nds_builder = load_dataset_builder(\\n    \"openslr/librispeech_asr\", \\ntrust_remote_code=True\\n)\\nds_builder.info.splits\\n{\\'test.clean\\': SplitInfo(name=\\'test.clean\\',\\n                         num_bytes=368449831,\\n                         num_examples=2620,\\n                         shard_lengths=None,\\n                         dataset_name=None),\\n \\'test.other\\': SplitInfo(name=\\'test.other\\',\\n                         num_bytes=353231518,\\n                         num_examples=2939,\\n                         shard_lengths=None,\\n                         dataset_name=None),\\n \\'train.clean.100\\': \\nSplitInfo(name=\\'train.clean.100\\','),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"num_bytes=6627791685,\\n                              \\nnum_examples=28539,\\n                              \\nshard_lengths=None,\\n                              \\ndataset_name=None),\\n 'train.clean.360': \\nSplitInfo(name='train.clean.360',\\n                              \\nnum_bytes=23927767570,\\n                              \\nnum_examples=104014,\\n                              \\nshard_lengths=None,\\n                              \\ndataset_name=None),\\n 'train.other.500': \\nSplitInfo(name='train.other.500',\\n                              \\nnum_bytes=31852502880,\\n                              \\nnum_examples=148688,\\n                              \\nshard_lengths=None,\\n                              \\ndataset_name=None),\\n 'validation.clean':\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"SplitInfo(name='validation.clean',\\n                               \\nnum_bytes=359505691,\\n                               \\nnum_examples=2703,\\n                               \\nshard_lengths=None,\\n                               \\ndataset_name=None),\\n 'validation.other': \\nSplitInfo(name='validation.other',\\n                               \\nnum_bytes=337213112,\\n                               \\nnum_examples=2864,\\n                               \\nshard_lengths=None,\\n                               \\ndataset_name=None)}\\nThe dataset authors found that the size of the corpus made it\\nimpractical to work with it, so they decided to split it into\\nsubsets of 100, 360, and 500 hours. It makes sense as, after all,\\njust the training data is over 60 Gigabytes. Without loading all\\nthis data, we can start looking into the features using\\n.info.features.\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"ds_builder.info.features\\n{'file': Value(dtype='string', id=None),\\n 'audio': Audio(sampling_rate=16000, \\nmono=True, decode=True, id=None),\\n 'text': Value(dtype='string', id=None),\\n 'speaker_id': Value(dtype='int64', id=None),\\n 'chapter_id': Value(dtype='int64', id=None),\\n 'id': Value(dtype='string', id=None)}\\nThe most valuable features are text and audio. We have all\\nthe data needed to build an initial speech recognition pipeline\\nwith these two features: the audio and its corresponding\\ntranscription. Under the hood, each feature has a type. text,\\nfor example, is a Value feature, which contains the data type\\n(string). audio is a Audio feature which contains the audio\\ninformation. Just as images, audio can be represented with\\nmultiple channels. The mono attribute indicates whether the\\naudio is mono (single channel, which provides a uniform sound\\nexperience) or stereo (two channels, which provide a sense of\\ndirectionality.). We’ll discuss what sampling_rate and\\ndecode mean in the next section.\\nGiven that the dataset is so large, we need to ﬁnd ways to work\\nwith it eﬃciently. Rather than downloading the whole dataset\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='and then using it, you can use streaming mode to load one\\nexample at a time, hence not consuming disk space and being\\nable to use samples from the dataset as they are downloaded.\\nWhen using streaming mode with the datasets library, we get\\nan IterableDataset, which can be used as any Python\\niterator. Let’s look at the ﬁrst example of the 100-hour split.\\nfrom datasets import load_dataset\\nds = load_dataset(\\n    \"openslr/librispeech_asr\",\\n    split=\"train.clean.360\",\\n    streaming=True,\\n)\\nsample = next(iter(ds))\\nsample\\n{\\'audio\\': {\\'array\\': array([ 9.15527344e-05,  \\n4.57763672e-04,  5.18798828e-04, ...,\\n       -4.57763672e-04, -5.49316406e-04, \\n-4.88281250e-04]),\\n           \\'path\\': \\'1487-133273-0000.flac\\',\\n           \\'sampling_rate\\': 16000},\\n \\'chapter_id\\': 133273,\\n \\'file\\': \\'1487-133273-0000.flac\\',\\n \\'id\\': \\'1487-133273-0000\\','),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"'speaker_id': 1487,\\n 'text': 'THE SECOND IN IMPORTANCE IS AS \\nFOLLOWS SOVEREIGNTY MAY BE '\\n         'DEFINED TO BE THE RIGHT OF MAKING \\nLAWS IN FRANCE THE KING '\\n         'REALLY EXERCISES A PORTION OF THE \\nSOVEREIGN POWER SINCE '\\n         'THE LAWS HAVE NO WEIGHT'}\\nThe sample provides us with audio and the corresponding text.\\nThe audio entry is the feature type that contains:\\nan array with the decoded audio data. Recall that the\\naudio feature had decode set to True, which means that\\nthe audio is already decoded for you. Otherwise, the audio\\ncontains the bytes and you need to decode it yourself.\\nthe path to the downloaded audio ﬁle.\\nthe data sampling_rate which is essential for loading the\\naudio properly.\\nIf these concepts sound foreign to you, no worries. This is the\\nperfect opportunity to learn what exactly audio is.\\nAudio is an inﬁnite set of values over time. Computers can’t\\nwork with continuous data, so we need to process the audio\\nsignal and have a digital discrete (ﬁnite) representation. To\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='achieve this, we take many snapshots at a given second. That’s\\ncalled the sampling rate. For example, for this audio sample, we\\ncan ﬁnd in the audio feature that the sampling rate is 16,000\\n(with the unit being Hertz). This means that 16,000 samples are\\ntaken in a given second. If we have an audio ﬁle of a minute,\\nthat’s almost a million values – no wonder audio datasets are\\nhuge. Figure 9-2 shows an audio waveform sampled using a\\nsampling rate of 6.\\nFigure 9-2. Figure 9-2. A waveform sampled using a sampling rate of 6. Discretization\\nhappens at intervals 1/6th of a second apart.\\nThe sampling rate is an essential parameter: when working\\nwith audio ML, one needs to ensure that all the audio samples\\nhave the same sampling rate. The models are pre-trained with'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='data sampled at a speciﬁc rate, so when ﬁne-tuning or running\\ninference you need to ensure to use the same sampling rate.\\nAlthough some of the most popular audio datasets have a\\nsampling rate of 16,000, this is only sometimes the case, so you\\nmust resample the data in the pre-processing stage.\\narray = sample[\"audio\"][\"array\"]\\nsampling_rate = sample[\"audio\"]\\n[\"sampling_rate\"]\\n# Let\\'s get the first 5 seconds\\narray = array[: sampling_rate * 5]\\nprint(f\"Number of samples: {len(array)}. \\nValues: {array}\")\\n(\\'Number of samples: 80000. Values: \\n[9.15527344e-05 4.57763672e-04 \\'\\n \\'5.18798828e-04 ... 7.05261230e-02\\\\n\\'\\n \\' 5.92041016e-02 6.50329590e-02]\\')\\nOf course, we cannot print an audio ﬁle in the book, but you\\ncan run some code to listen to the audio. We can use the\\nIPython.display.Audio() function for this. Let’s listen to\\nthe ﬁrst audio sample in the 100-hour split. Alternatively, you\\n1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='can visit the oﬃcial interactive demo to listen to all the audio\\nsamples of the chapter.\\nimport IPython.display as ipd\\nipd.Audio(data=array, rate=sampling_rate)\\nWaveforms\\nWe just saw that we use a digital discrete representation of\\naudio to be able to work with it. Under the hood, an audio is\\njust an array of values. These arrays contain the information\\nneeded to train models for many tasks, so it’s worth investing\\ntime in understanding the array before going into the\\napplications.\\nWhat does the array represent? Each value in the array\\nrepresents the amplitude, which describes the strength of the\\nsound wave and is measured in decibels . The amplitude tells\\nus how loud a sound is relative to a reference value. 0 dB, the\\nreference value, represents the lowest sound perceived by the\\nhuman ear. Your breathing is around 10 dB, an intense concert\\ncould be 120 dB (starts getting painful), and the Krakatoa\\nEruption, a colossal volcano eruption in 1883, could be heard\\n2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='even 3,000 miles (4,800 kilometers) away with an estimated 310\\ndB.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Amplitude is usually measured in decibels, but the array is\\nfrequently normalized, so the numbers are between -1 and 1. To\\nvisualize the audio, we can use a waveform, a plot of the\\namplitudes over time. Let’s plot the waveform (Figure 9-4) with\\nlibrosa, a popular library for working with audio data.\\nimport librosa.display\\nlibrosa.display.waveshow(array, \\nsr=sampling_rate);'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 9-4. Plot of a waveform, displayed with the librosa audio\\nlibrary.\\nThe waveform aids in doing the initial exploration of audio\\ndata. If you listen to the audio, you can identify that the ﬁrst\\nwaves correspond to when the reader says \"chapter 16\"\\nfollowed by silence. More generally, waveforms are an intuitive\\nway to identify irregularities in the audio and get an overall\\nsense of the signal and its patterns.\\nSpectrogram and Mel Spectrogram'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 9-5. Spectrogram (left) and mel spectrogram (right)\\nrepresenting the same audio clip.\\nSpectrograms and mel spectrograms (Figure 9-5) are diﬀerent\\nways to represent audio signals. This section explains what they\\nare and when to use them.\\nLet’s say we want to train a generative model to create a new\\nsound or synthetic speech conditioned on an input audio. A\\nstraightforward approach might be to use the raw audio\\nwaveform as the model’s input. Waveforms are a direct and\\nintuitive representation of sound, tcontaining all the\\ninformation necessary to reproduce the original audio. You can\\neasily verify this by listening to a few samples from a dataset,\\nwhether they include voices, music, or other sounds. So, if\\nwaveforms can accurately represent sound, why don’t we use\\nthem to train our models?\\nThe ﬁrst challenge lies in the waveform’s dimensionality.\\nAlthough waveforms are one-dimensional, they consist of a\\nvery large number of data points. Each second of audio has tens\\nof thousands of samples for the model to process, making it\\ndiﬃcult to learn patterns and structures in the data. This high\\ndimensionality makes it diﬃcult for models to eﬀectively learn\\npatterns and structures in the data.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Additionally, our hearing system is very sensitive to small\\ndiﬀerences in attributes like pitch or timbre, which are related\\nto the frequency characteristics of the sound. While these\\nattributes are encoded in the waveform’s shape, they are\\nchallenging to identify -even for humans- just by looking at the\\nwaveform. The most obvious information we can extract from a\\nwaveform is how amplitude changes over time, often called the\\ntime domain. However, attributes related to frequency, such as\\nwhat makes a piano sound diﬀerent from a violin, are much\\nharder to discern from the waveform alone (Figure 9-6).\\nWhile it is possible to train a model using raw waveform data\\n(as we’ll explore later in this chapter), the sheer number of\\nsamples involved presents a signiﬁcant challenge. For instance,\\none minute of audio at a sampling rate of 16,000 contains\\nnearly a million samples. If we attempt to reduce this\\ndimensionality by averaging or combining samples, we risk\\nlosing the nuanced diﬀerences that are crucial in the frequency\\ndomain. Even with a sampling rate of 16,000, which is adequate\\nfor voice, it’s insuﬃcient for high-quality music. There’s a\\nreason why consumer audio standards, from CDs to streaming\\nservices, typically use a sampling rate of 44.1kHz or higher.\\nGiven these challenges, a more eﬀective approach often\\ninvolves converting the audio waveform into a diﬀerent'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='representation called spectrogram. Spectrograms are compact\\nvisual representations that depict how the frequency and\\namplitude of sound change over time. By explicitly capturing\\nfrequency details, spectrograms provide a more structured and\\ninformative representation of the audio signal, which makes it\\neasier for models to learn from. Spectrograms transform the\\nproblem from the time domain to the time-frequency domain,\\nwhich is often more suitable for Machine Learning tasks\\ninvolving audio. However, it is important to note that\\nspectrograms are a lossy representation, meaning they do not\\nretain all the information from the original waveform. Despite\\nthis, the loss is usually acceptable, as spectrograms preserve the\\nmost perceptually relevant features.\\nimage::images/placeholder.png[\"\"]Before we dive into\\nspectrograms, let’s look at frequencies. We’ll plot four waves\\nwith the same amplitude ranges but varying frequencies,\\ndepicted in Figure 9-7.\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\ndef plot_sine(freq):\\n    sr = 1000  # samples per second'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='ts = 1.0 / sr  # sampling interval\\n    t = np.arange(0, 1, ts)  # time vector\\n    amplitude = np.sin(2 * np.pi * freq * t)\\n    plt.plot(t, amplitude)\\n    plt.title(\"Sine wave with frequency \\n{}\".format(freq))\\n    plt.xlabel(\"Time\")\\nfig = plt.figure()\\nplt.subplot(2, 2, 1)\\nplot_sine(1)\\nplt.subplot(2, 2, 2)\\nplot_sine(2)\\nplt.subplot(2, 2, 3)\\nplot_sine(5)\\nplt.subplot(2, 2, 4)\\nplot_sine(30)\\nfig.tight_layout()\\nplt.show()'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 9-7. Waves with the same amplitude but varying\\nfrequencies.\\nAs you can observe, despite the waves sharing the same\\namplitude ranges, they exhibit diﬀerent frequencies. While\\nthese are simple sine waves, real-world sounds are more\\ncomplex. Typically, a sound will be a composition of multiple\\nwaves combined, each with its frequency and amplitude.\\nTherefore, our ﬁrst task is to break down the sound wave into\\nits multiple, simpler components. Why is this useful? This'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='process allows us to extract valuable information that a model\\ncan leverage: How does the amplitude change over time at\\ndiﬀerent frequencies? How can we decompose the sound? We\\nemploy Fourier Transforms, a mathematical tool that enables\\ndecomposing a single function into multiple functions.\\nLet’s begin with some simple sinusoidal functions, as shown in\\nFigure 9-8. In the ﬁrst column, we have the sine functions. In\\nthe second column, we have a plot of the Fourier Transform\\n(FT), which is the function in the frequency domain.\\n3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 9-8. Some sinusoidal functions and their frequency\\nspectrums. Pure notes (sine waves) have a single peak at the\\nsine period, whereas more complex sounds show several peaks\\nin the frequency domain.\\nLet’s analyze the top row. On the left, we have the original\\nwaveform - a sine wave with a one-cycle-per-second frequency.\\nOn the right, the FT plot depicts frequency on the x-axis and\\namplitude in the frequency domain on the y-axis. Observe the\\npeak at 1, which aligns with the original waveform’s frequency.\\nThe following rows show examples with diﬀerent frequencies\\nand share the same behavior: the FT plot peaks at the original\\nwaveform’s frequency. The y-value is half the number of\\nsamples (in this example, the sampling rate is 2000 and is just\\none second, so we have 2000 samples) multiplied by the\\namplitude in the original waveform amplitude (1 in the ﬁrst\\nﬁve rows).\\nNow, let’s explore the last row - a more intriguing case. The\\nwaveform combines three distinct sinusoidal functions with\\nvarying amplitudes (1, 3, and 1.5) and frequencies (2, 5, and 14).\\nIt’s hard to discern the composition just by looking at the\\nwaveform, so here’s where the frequency domain is useful. In\\nthe frequency domain, we can observe three peaks\\ncorresponding to the original function’s frequencies: 2, 5, and\\n4'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='14. Consequently, we can reverse-engineer and describe the\\ninitial waveform’s three functions.\\nAt a frequency of 2 Hz, the frequency domain amplitude (y-axis\\nat the right) is 1000. By performing 1000 * 2 / 2000, we end up\\nwith 1, the amplitude of the ﬁrst sinusoidal function composing\\nthe waveform. Similarly, a frequency of 3 Hz yields a frequency\\ndomain amplitude of 3000, and by doing 3000 * 2 / 2000, we end\\nwith 3. The decomposed sine waves are depicted in Figure 9-9.\\nFourier Transforms give us a mechanism that allows us to\\nanalyze complex waveforms and understand the diﬀerent\\nfrequencies that compose them. We’ve revealed hidden\\ncomplexities within a sound, information that we thought\\nwasn’t there but is. This provides much more information and\\nwill be key to models that can transcribe speech or generate\\nmusic.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 9-9. A complex sound wave can be decomposed into\\nsinusoidal frequencies by analyzing the spectrum\\nrepresentation.\\nWhat about the sound at the beginning of the chapter? As with\\nthe waveforms in Figure 9-9, the function can be broken into\\nmultiple sine functions with their amplitudes and frequencies.\\nLet’s look at its frequency domain plot (Figure 9-10).\\n# Compute the Fast Fourier Transform (FFT) of \\nthe input signal\\nX = np.fft.fft(array)\\n# Length of the FFT result (which is the same \\nas the length of the input signal)\\nN = len(X)\\n# Calculate the frequency bins corresponding \\nto the FFT result\\nn = np.arange(N)\\nT = N / sampling_rate\\nfreq = n / T\\n# Plot the amplitude spectrum for the first \\n8000 frequency bins\\n# We could plot all the bins, but we would \\nget a mirror image of the spectrum'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='plt.stem(freq[:8000], np.abs(X[:8000]), \"b\", \\nmarkerfmt=\" \", basefmt=\"-b\")\\nplt.xlabel(\"Frequency (Hz)\")\\nplt.ylabel(\"Amplitude in Frequency Domain\")\\nplt.show()\\nFigure 9-10. Frequency representation of an arbitrary audio\\nclip.\\nThis audio is more challenging to interpret than the previous\\nexamples. We can see that most of the sound is in the 0-800 Hz\\nrange. We can also see that around 170 Hz, there are some loud'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='noises. Although the plot is interesting, we lose information in\\nthe time domain. We don’t know at what time we had sounds\\nwith speciﬁc frequencies. Waveforms have amplitude and time\\ninformation, and FT plots have amplitude and frequency\\ninformation. Can we combine the three at the same time?\\nSpectrograms plot how the frequency and amplitude of the\\nsignal change through time. They are informative tools that\\nvisualize time, frequency, and amplitude in a single plot. To\\ncreate a spectrogram, we’ll slide a window through the original\\nwaveform and compute the Fourier Transform for that segment\\nto capture how the frequencies change through time. The\\nwindows can then be stacked together to form the spectrogram.\\nThis approach of a sliding window through the audio is called\\nShort-Time Fourier Transform.\\nWith librosa we can use stft() to obtain the Short-Time\\nFourier Transform. In addition to computing the spectrogram,\\nwe convert the amplitude to a decibel scale, which is\\nlogarithmic and much better for visualizing. Remember that the\\namplitude is the diﬀerence of sound pressure. Hence, the\\nnumerical range of sound pressure is very wide. By using a\\nlogarithmic scale, we limit the scale, make the plots more\\ninformative, and have information closer to how humans'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='perceive sound. Let’s look at the spectrogram of our example,\\nshown in Figure 9-11.\\nThe decibel scale is logarithmic, meaning that each 10 dB\\nincrease corresponds to a tenfold increase in relative sound\\nintensity. However, our perception of loudness does not align\\ndirectly with these physical changes. While the dB scale and our\\nperception of loudness both follow logarithmic patterns, they\\ndo so diﬀerently. Speciﬁcally, a 10 dB increase in sound\\nintensity is perceived by the human ear as roughly doubling the\\nloudness, not tenfold. This diﬀerence arises because the human\\nauditory system compresses changes in intensity into a more\\nmanageable range of perceived loudness.\\nIn summary, a 10 dB increase results in a tenfold increase in\\nsound intensity, but our ears perceive this as roughly a\\ndoubling of loudness.\\n# Compute Short-Time Fourier Transform (STFT)\\n# We take the absolute value of the STFT to \\nget the amplitude\\n# of each frequency bin.\\nD = np.abs(librosa.stft(array))\\n# Convert the ampltiude into decibels\\n# which is logarithmic.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='S_db = librosa.amplitude_to_db(D, ref=np.max)\\n# Generate the spectrogram display\\nlibrosa.display.specshow(S_db, \\nsr=sampling_rate, x_axis=\"time\", y_axis=\"hz\")\\nplt.colorbar(format=\"%+2.0f dB\");\\nFigure 9-11. Spectrogram of the same audio clip. This\\nvisualization is much easier to interpret then the raw frequency\\nrepresentation.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The x-axis is time, just like in the waveform. The y-axis shows\\nthe frequency (using Hertz, a linear unit), and the color\\nrepresents the intensity (decibels) of the frequency at a given\\npoint. Areas in black represent areas with no energy (silence).\\nAs before, we can observe some noise in the ﬁrst 2.4 seconds\\nand the last 1.6 seconds. The loudest points happen at a low\\nfrequency (bright color and low value in the y-axis). This\\nmatches the waveform and the frequency domain plot, where\\nwe got a high amplitude at low frequencies.\\nNOTE\\nYou might be wondering why we have negative decibel values. As we used\\namplitude_to_db() with a ref=np.max, the maximum value of the spectrogram\\nis 0 dB. The rest of the values are relative to this maximum value. For example, a\\nvalue of -20 dB means that the amplitude is 20 dB lower than the maximum value.\\nA popular spectrogram variation is called the mel spectrogram.\\nWhile in a normal spectrogram, the unit for frequency is linear,\\nthe mel spectrogram uses a scale similar to how we (humans)\\nperceive sound. Humans perceive audio logarithmically,\\nmeaning we’re more sensitive to changes at low frequencies\\nbut less so at high frequencies. The diﬀerence between 500 and\\n1000 Hz is much more noticeable than between 5000 and 5500.\\nlibrosa once again oﬀers a convenient method that computes'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='the mel spectrogram. In the mel spectrogram, equal distances in\\nthe frequency (y-axis) have the same perceptual distance.\\n# Generate a Mel-scaled spectrogram from the \\naudio signal.\\n# The result is a matrix where each element \\ncorresponds to the power\\n# of a frequency band (in the Mel scale) at a \\nspecific time.\\nS = librosa.feature.melspectrogram(y=array, \\nsr=sampling_rate)\\n# We convert the power spectrogram to a \\ndecibel scale\\nS_dB = librosa.power_to_db(S, ref=np.max)\\n# Display the Mel-scaled spectrogram\\nlibrosa.display.specshow(S_dB, \\nsr=sampling_rate, x_axis=\"time\", \\ny_axis=\"mel\")\\nplt.colorbar(format=\"%+2.0f dB\");'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 9-12. Mel spectrogram of the same clip. The logarithmic y\\nscale better conveys how humans perceive sound.\\nThe mel spectrogram, depicted in Figure 9-12, has similar\\npatterns to the original, but we can notice some diﬀerences.\\nFirst, the y-scale is not linear: the distance between 512 and\\n1024 is the same as between 2048 and 4096. Second, the areas\\nwith more energy (more decibels) in low frequency are much\\nmore noticeable in the mel spectrogram. This corresponds to\\nhow humans perceive sound.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Apart from being great visual representations to understand\\naudio signals, spectrograms are commonly used directly by\\nMachine Leanring models. For example, a spectrogram of a\\nsong can be used as input to a model to classify its genre.\\nSimilarly, a model can receive some words as input and output\\na spectrogram representing the audio of a person speaking\\nthose words.\\nSpeech to Text with Transformers-\\nBased Architectures\\nLet’s now dive into ASR, the task of transcribing an audio ﬁle to\\ntext. As with many other tasks, we can use the transformers\\npipeline(), which conveniently takes care of all pre- and\\npost-processing and is a useful inference wrapper. Let’s use the\\nsmallest variant of Whisper, a popular open-source model\\nreleased by OpenAI, to get an initial baseline.\\nfrom transformers import pipeline\\npipe = pipeline(\\n    \"automatic-speech-recognition\",\\n    model=\"openai/whisper-small\",\\n    max_new_tokens=200,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=')\\npipe(array)\\n{\\'text\\': \\' The second in importance is as \\nfollows. Sovereignty may be defined to be\\'}\\nThe results are solid for using a small version (244M\\nparameters, which can eﬃciently run on-device). More\\nsurprisingly, if you hear the original audio, it was cut in the\\nmiddle of \"be“. Whisper can predict the entire word even if it\\nwas not completed. Additionally, Whisper predicted\\npunctuation (e.g.\\xa0the period). The following sections will shed\\nsome light on how models that perform ASR work. Before we\\nexplore Whisper, let’s discuss how encoder-only models can be\\napplied to ASR.\\nEncoder-Based Techniques\\nOne way to look at the ASR task is to think of it like we do about\\ntext token classiﬁcation. The idea is analogous to how you\\nwould do it with BERT in the NLP world. First, you pre-train a\\nmodel with masked language modeling using an encoder-only\\ntransformer; that is, a model is pre-trained with a large amount\\nof unlabeled data, and part of the input is masked. The model’s\\nobjective is to predict which word should ﬁll the mask. We can'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='apply the same principle in audio. Rather than masking text, we\\nmask audio (to be precise, the latent speech representation),\\nand the model learns contextualized representations.\\nAfterward, this model can be ﬁne-tuned with labeled data on\\ndiﬀerent audio tasks, such as speaker identiﬁcation or\\nautomatic speech recognition. Let’s dive into achieving ASR\\nwith this idea.\\nThe ﬁrst challenge, which we already mentioned before, is that\\nan audio sample can be very long. A 30-second audio sample\\nwith a sample rate of 16kHz will yield an array of 480,000\\nvalues. Using 480,000 values for a single input sample to the\\ntransformer encoder would require massive amounts of\\nmemory and computation. To mitigate this, one approach is to\\nhave a Convolutional Neural Network (CNN) as a feature\\nencoder. The CNN will slide through the input data (the\\nwaveform) and output a latent speech representation. For\\nexample, assuming a window of 20 milliseconds sliding through\\none second of audio, we would end up with 50 latent\\nrepresentations (this assumes there are no overlapping\\nwindows; in CNN terms, we are using a stride of 20). These\\nlatent representations are then passed to a classical\\ntransformer encoder, which outputs embeddings for each of the\\n50 representations. During pre-training, spans of the latent'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='representations are masked, and hence, the model learns to\\npredict how to ﬁll the missing parts.\\nA simple linear classiﬁcation head is added to the encoder for\\nASR ﬁne-tuning: the goal is that the classiﬁer estimates the text\\nthat corresponds to each of the audio windows processed by the\\nencoder. The number of classes that the model will classify is a\\ndesign decision. One could decide to classify entire words,\\nsyllables, or just characters, but given we’re using a window of\\n20 milliseconds, a single word would not ﬁt in a window.\\nClassifying characters appears to make a lot of sense in this\\ncontext, and it has the additional beneﬁt that we can keep a\\nvery small vocabulary. For example, for English, we can use a\\nvocabulary of the 26 English characters plus some special\\ntokens (e.g., a token for silence, a token for unknown, etc.). To\\nkeep a minimal vocabulary, we usually pre-process the text,\\nupper-casing all of it and converting numbers to words (e.g.,\\n\"14\" to \"fourteen“).'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='NOTE\\nIn the ﬁrst part of this chapter, we used spectrograms to capture the amplitude and\\nfrequency characteristics of the input data in a concise 2D visual representation.\\nNow, we are exploring architectures that process the data diﬀerently, using CNNs to\\nextract features directly from the waveform without converting it to a spectrogram.\\nThe choice between these approaches depends on factors like the speciﬁc task and\\nthe architecture’s design.\\nAs we’ll discuss soon, some models take spectrograms as input, while others work\\ndirectly with the raw waveform. Transformers, with their attention mechanisms, are\\nparticularly eﬀective for handling sequential data, making it important to consider\\nthe temporal structure of the input.\\nLet’s recap the whole ﬂow to perform ASR with encoder-based\\nmodels:\\n1. Raw audio data (1D array) representing the amplitudes is\\nreceived.\\n2. Data is normalized to zero mean and univariance to\\nstandardize across diﬀerent amplitudes.\\n3. A small Convolutional Neural Network turns the audio into\\na latent representation. This reduces the length of the input\\nsequence.\\n4. The representations are then passed to an encoder model,\\nwhich outputs embeddings for each representation.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='5. Each embedding is ﬁnally passed through a classiﬁer, which\\npredicts the corresponding character for each one.\\nThe output of such a model would be something as follows:\\nCHAAAAAPTTERRRSSIXTEEEEENIMMMIIGHT...\\nHmmm. That resembles a text, but obviously, it’s not right.\\nWhat’s going on? If the sound of a character spreads over a\\nperiod longer than a single window, it might appear multiple\\ntimes in the output. Remember that the model does not know\\nwhen each character happened during training, so it’s\\nimpossible to align the audio with the text directly.\\nAn approach to solve this, used initially for Recurrent Neural\\nNetworks, is called Connectionist Temporal Classiﬁcation (CTC).\\nThe idea behind using CTC in audio is to add the padding token\\n(for visualization purposes, we’ll use the character *), which\\nhelps as a boundary between groups of characters, and a\\nseparator token (/) that separates words. The model will learn\\nto predict such tokens as well. During inference, the output\\nmight look as follows:\\nCHAAAAA*PTT*ERRR/SS*IX*T*EE*EEN/I/MMM*II*GHT'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='With this output, we can deduplicate by merging equal\\nconsecutive characters in the same group, resulting in our\\ndesired outcome.\\nCHAPTER SIXTEEN I MIGHT\\nAll of these ideas (using an encoder-only model, processing the\\nwaveform with a CNN, and using CTC to perform classiﬁcation)\\nform the foundation of encoder-based architectures such as\\nMeta’s Wav2Vec2 (2020)  and HuBERT (2021)  . Wav2Vec2 is\\npre-trained with Librispeech and LibriVox (both unlabeled\\ndatasets). It can be ﬁne-tuned with just 10 minutes of labeled\\ndata to outperform models trained with signiﬁcantly more data.\\nThis is very interesting as a base model can easily be tuned for\\nspeciﬁc domains or accents without much data. The\\ndownstream task being solved here is ASR, but the same pre-\\ntrained model can be ﬁne-tuned for other tasks, such as speaker\\nrecognition and language detection. The following code shows\\neach step of running inference with Wav2Vec2 (note that you\\ncan also use pipeline() as a high-level API).\\nimport torch\\nfrom transformers import Wav2Vec2ForCTC, \\nWav2Vec2Processor\\n5 6'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from genaibook.core import get_device\\ndevice = get_device()\\n# The AutoProcessor has the pre and post-\\nprocessing incorporated\\nwav2vec2_processor = \\nWav2Vec2Processor.from_pretrained(\\n    \"facebook/wav2vec2-base-960h\"\\n)\\nwav2vec2_model = \\nWav2Vec2ForCTC.from_pretrained(\\n    \"facebook/wav2vec2-base-960h\"\\n).to(device)\\n# Run forward pass, making sure to resample \\nto 16kHz\\ninputs = wav2vec2_processor(\\n    array, sampling_rate=sampling_rate, \\nreturn_tensors=\"pt\"\\n)\\nwith torch.inference_mode():\\n    outputs = \\nwav2vec2_model(**inputs.to(device))\\n# Transcribe\\npredicted_ids = torch.argmax(outputs.logits, \\ndim=-1)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"transcription = \\nwav2vec2_processor.batch_decode(predicted_ids)\\nprint(transcription)\\n['THE SECOND IN IMPORTANCE IS AS FOLLOWS \\nSOVEREIGNTY MAY BE DEFINED TO']\\nNOTE\\nAs the models are pre-trained with data with a speciﬁc sampling rate, using audio\\nwith the same sampling rate is required during inference. You can achieve this by\\nresampling the data (e.g., doing dataset.cast_column(“audio”,\\nAudio(sampling_rate=16_000))) or specifying the sampling_rate in the\\nprocessor, as done in the snippet above.\\nHuBERT follows the same concept of pre-training to learn\\nuseful latent representations, but changes the training process\\nto use the original masked language modeling objective of\\nBERT.  While Wav2Vec2 predicts characters, HuBERT processes\\nthe waveform with clustering techniques to learn discrete\\nhidden speech units, which can be seen as equivalent to tokens\\nin NLP. The model predicts these hidden units in the second\\nstage at randomly masked locations.\\n7\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Note that Wav2Vec2 and HuBERT only work for English. A few\\nweeks after the Wav2Vec2 release, Meta released XLSR-53,\\nwhich has the same architecture as Wav2Vec2 but was pre-\\ntrained on 56 thousand hours of speech in 53 languages. XLSR\\nlearns speech units common to several languages. Hence,\\nlanguages with little digital data can still get decent results. In\\n2021, Meta released XLS-R  (yes, it’s diﬀerent from XLSR), a 2\\nbillion parameters model trained with a similar setup, but using\\nnearly 10 times as much unlabeled data (436k hours) from 128\\nlanguages.\\nIt is important to note that these models are acoustic: their\\noutputs are based purely on the sound of the input and lack\\ninherent language information. For example, the model might\\nfrequently misspell words, output things that are not words, or\\nconfuse homophones (e.g., \"bear\" vs.\\xa0\"bare“). An approach to\\nmitigate this is to incorporate language information during the\\ngeneration phase.\\nTypically, in the classiﬁcation stage, we compute the\\nargmax(logits) to predict the most likely character.\\nHowever, another approach is to introduce a language model\\nthat can predict the most likely word given the sequence of\\ncharacters. An n-gram model is a type of language model that\\npredicts the likelihood of a word based on the n-1 previous\\n8'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='words in a given text corpus. For instance, a bigram model\\n(n=2) considers pairs of words, while a trigram model (n=3)\\nconsiders triplets. This probabilistic model helps in\\nunderstanding the context and structure of the language, and\\ncan be very small and eﬃcient. A full-ﬂedged transformer-\\nbased language model, like the ones we saw in chapter 2, can\\nalso be used. However, n-grams achieve most of the quality\\nimprovements these models would, at a fraction of the cost in\\ncompute, memory, and inference time.\\nThe n-gram score can be added to beam search to generate the\\nk most probable text sequences. By integrating a language\\nmodel into beam search, we can enhance the decoding process.\\nThe beam search evaluates both the acoustic and language\\nmodel scores, which helps correct misspellings and ﬁlter out\\nnonsensical words. This combined approach signiﬁcantly\\nimproves the accuracy and coherence of the generated text.\\nHOT-WORD BOOSTING\\nYou might want to increase the probabilities of certain words (e.g., if some words are\\nnot in the language model or you need to boost domain-speciﬁc data). To do this, you\\ncan count the number of hot words in the output and increase the probability.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Encoder-Decoder Techniques\\nEncoder models with a CTC head are one of the most popular\\napproaches for ASR. As discussed above, acoustic models might\\ngenerate spelling errors, for which a n-gram model needs to be\\nincorporated. These challenges lead to exploring encoder-\\ndecoder architectures.\\nOne can formulate the ASR problem as a sequence-to-sequence\\nproblem rather than a classiﬁcation problem. This is what\\nWhisper, the open-source model we introduced at the beginning\\nof this section, does. Unlike Wav2Vec2 or HuBERT, Whisper\\n(Figure 9-13) was trained in a supervised setting with a massive\\namount of labeled data: over 680,000 hours of audio with their\\ncorresponding text. For comparison, Wav2Vec2 was only\\ntrained on less than 60,000 hours of unlabeled data. About one-\\nthird of the data is multilingual, which enables Whisper to\\nperform ASR for 96 languages. Given that the model is trained\\nwith labeled data, Whisper learns a speech to text mapping\\ndirectly during pre-training, without requiring ﬁne-tuning.\\nAnother Whisper peculiarity is that it’s trained to operate\\nwithout an attention mask: it can directly infer where to ignore\\nthe inputs.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='How can we do inference? Whisper, unlike Wav2Vec2, operates\\nwith spectrograms. We begin by padding and/or truncating a\\nbatch of audio samples to ensure uniform input length,\\nconverting them into log-mel spectrograms, and processing the\\noutputs by a CNN before passing them to the encoder. The\\nencoder output is then passed to the decoder, which predicts the\\nnext token, one at a time, in an auto-regressive way (just as\\nmodels like Llama) until the end token is generated. While the\\nencoder-decoder architecture may be slower than encoder-only\\napproaches, Whisper can handle long audio samples, predict\\npunctuation, and does not require an additional language\\nmodel during inference.\\nIn encoder-only models, incorporating a language model is\\nnecessary to address spelling errors generated by acoustic\\nmodels, often requiring the use of an external n-gram model. In\\nWhisper’s case, the decoder serves a dual purpose: generating\\ntext transcriptions while also functioning as a language model.\\nWhy does the decoder operate as a language model? By\\nlearning to predict the next token in the transcription sequence\\nbased on contextual information from the encoder, Whisper\\neliminates the need for an external language model during\\ninference.\\n9'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 9-3. Figure 9-13. Whisper training is modeled in a sequence-to-sequence\\nfashion on a wide variety of tasks, including translation, transcription, multilingual\\nspeech recognition, and others. Special tokens are used to identify the task, language\\nan interesting points in the data, conditioning the model to perform the desired\\noperation. Image from the Whisper paper.\\nWhisper uses a very speciﬁc sequence format, so looking at\\nFigure 9-13 is essential to understand its generation. Special\\ntokens are used to indicate the language or task, and thus guide\\nthe model towards the desired output. This is akin to the\\nconditioning methods we covered in previous chapters. Some of\\nthe most important tokens are:\\nThe speech begins with a start of transcript token.\\nIf the language is not English, there is a language tag\\ntoken (e.g., hi for Hindi).'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='With the language tag, one can perform language\\nidentiﬁcation, transcription, or translate to English.\\nIf there’s a no speech token, Whisper is used for voice\\nactivity detection.\\nLet’s look at an example in Spanish with its corresponding\\nformat.\\nfrom transformers import WhisperTokenizer\\ntokenizer = WhisperTokenizer.from_pretrained(\\n    \"openai/whisper-small\", \\nlanguage=\"Spanish\", task=\"transcribe\"\\n)  \\ninput_str = \"Hola, ¿cómo estás?\"\\nlabels = tokenizer(input_str).input_ids  \\ndecoded_with_special = tokenizer.decode(\\n    labels, skip_special_tokens=False\\n)  \\ndecoded_str = tokenizer.decode(labels, \\nskip_special_tokens=True)  \\nprint(f\"Input:                         \\n{input_str}\")\\nprint(f\"Formatted input w/ special:    \\n{decoded_with_special}\")'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='print(f\"Formatted input w/out special: \\n{decoded_str}\")\\nLoad the pre-trained tokenizer. As Whisper requires adding\\nsome tokens, such as the language ID token and the task\\nidentiﬁer, we need to specify the language and task\\nparameters.\\nTokenize the input string\\nDecode the token IDs back to the original string, but\\nincluding the special tokens.\\nDecode the token IDs back to the original string, but\\nexcluding the special tokens.\\n\\'Input:                         Hola, ¿cómo \\nestás?\\'\\n(\\'Formatted input w/ special:    \\'\\n \\'<|startoftranscript|><|es|><|transcribe|>\\n<|notimestamps|>Hola, \\'\\n \\'¿cómo estás?<|endoftext|>\\')\\n\\'Formatted input w/out special: Hola, ¿cómo \\nestás?\\''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Creating transcriptions with Whisper is not too diﬀerent from\\nWav2Vec2:\\n1. We use the processor to prepare the audio for the model’s\\nexpected format. In this case, mel spectrograms are\\nextracted from the raw speech and then processed to be\\nready to be consumed by the model.\\n2. The model generates the token IDs corresponding to the\\ntranscription.\\n3. The processor decodes the IDs and converts them into\\nhuman-readable strings.\\nOpenAI released nine Whisper variants, ranging from 39\\nmillion to 1.5 billion parameters, with model checkpoints for\\nmultilingual and English-only setups. In this example, we’ll use\\nthe intermediate, small, multilingual model, which can run\\nwith 2 GB of GPU memory and is six times faster than the\\nlargest model.\\nNOTE\\nNew models, such as the second and third versions of the large model, keep being\\nreleased. Additionally, the Distil Whisper project has achieved high-quality smaller\\nvariants of the original models, up to 6 times faster and 49% smaller.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from transformers import \\nWhisperForConditionalGeneration, \\nWhisperProcessor\\nwhisper_processor = \\nWhisperProcessor.from_pretrained(\"openai/whisper-\\nsmall\")\\nwhisper_model = \\nWhisperForConditionalGeneration.from_pretrained(\\n    \"openai/whisper-small\"\\n).to(device)\\ninputs = whisper_processor(\\n    array, sampling_rate=sampling_rate, \\nreturn_tensors=\"pt\"\\n)\\nwith torch.inference_mode():\\n    generated_ids = \\nwhisper_model.generate(**inputs.to(device))\\ntranscription = \\nwhisper_processor.batch_decode(\\n    generated_ids, skip_special_tokens=False\\n)[0]\\nprint(transcription)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"('<|startoftranscript|><|en|><|transcribe|>\\n<|notimestamps|> The '\\n 'second in importance is as follows. \\nSovereignty may be defined to '\\n 'be<|endoftext|>')\\nThis is a good opportunity to dive into the WhisperProcessor.\\nTo better understand the code above, we suggest to:\\nReview its documentation.\\nDetermine the two diﬀerent components of the processor.\\nExamine which are the outputs of the processor (inputs\\nin the code above).\\nFrom Model to Pipeline\\nIn the previous sections, we learned about diﬀerent\\narchitectures and approaches to performing Automatic Speech\\nRecognition. However, there are still three challenges to tackle\\nwhen using them in production use cases.\\n1. Long audio transcription. The ﬁrst limitation is that\\ntransformers usually have a ﬁnite input length they can\\nhandle. Wav2vec2, for example, uses attention, which has a\\nquadratic complexity. Whisper does not have an attention\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='mechanism, but it’s designed to work with audios of 30\\nseconds and will truncate longer ones. A simple approach\\nto solve this is chunking: chunk/split audios into shorter\\nsamples, run inference on them, and then reconstruct the\\noutput. Although eﬃcient, this can lead to poor quality\\naround the chunking border. To solve this, one can do\\nchunking with strides, which means we would have\\noverlapping chunks and then chain them. The result will\\nnot be the same as what the model would have predicted\\non the full-length audio, but the results should be very\\nclose. We can batch the chunks and run them through the\\nmodel in parallel, hence being more eﬃcient than\\ntranscribing the whole audio ﬁle sequentially. Both\\nchunking and chunk batching can quickly be done using\\nthe chunk_length_s and batch_size parameters of an\\nASR pipeline.\\n2. Live inference. Performing live ASR is desirable for many\\napplications. Now that we have learned to chunk, we can\\nleverage it with small chunks (e.g., 5 seconds) and a 1-\\nsecond stride. Making live inference with CTC models will\\nbe faster as it’s a single-pass architecture compared to those\\nincorporating decoders. Although Whisper would be\\nslower, you can perform the same chunking logic to\\ntranscribe chunks as they come and obtain strong results.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='3. Timestamps. Having timestamps that indicate the start and\\nend time for short audio passages can be very useful to\\nalign a transcription with the input audio. For example, if\\nyou’re generating subtitles from a video call, you will want\\nto know to which time segment each transcription belongs.\\nThis can easily be enabled using return_timestamps.\\nUnder the hood, we know each outputted token’s context\\nwindow and the sampling_rate for CTC models.\\nLet’s combine all of these with a longer (1 minute) audio:\\nfrom genaibook.core import \\ngenerate_long_audio\\nlong_audio = generate_long_audio()\\ndevice = get_device()\\npipe = pipeline(\\n    \"automatic-speech-recognition\", \\nmodel=\"openai/whisper-small\", device=device\\n)\\npipe(\\n    long_audio,\\n    generate_kwargs={\"task\": \"transcribe\"},\\n    chunk_length_s=5,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"batch_size=8,\\n    return_timestamps=True,\\n)\\n{'chunks': [{'text': ' the second in \\nimportance is as follows.',\\n             'timestamp': (0.0, 3.0)},\\n            {'text': ' Sovereignty may be \\ndefined to be the right of '\\n                     'making laws.',\\n             'timestamp': (3.0, 6.33)},\\n            {'text': ' In France, the king \\nreally exercises a '\\n                     'portion of the \\nsovereign power, since the laws '\\n                     'have no weight till he \\nhas given his assent to '\\n                     'them.',\\n             'timestamp': (6.33, 16.89)},\\n            {'text': ' He is moreover the \\nexecutor of the laws, but '\\n                     'he does not really \\ncooperate in their '\\n                     'formation since the \\nrefusal of his asset does '\\n                     'not annul them. He is \\ntherefore merely to be '\\n                     'considered as the agent\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"of the sovereign '\\n                     'power.',\\n             'timestamp': (16.89, 36.61)},\\n            {'text': ' But not only does the \\nking of France exercise '\\n                     'a portion of the \\nsovereign power, He also '\\n                     'contributes to the \\nnomination of the '\\n                     'legislature, which \\nexercises the other '\\n                     'portion. He has the \\nprivilege of appointing '\\n                     'the members of one \\nchamber and of dissolving '\\n                     'the United States has \\nno share in the '\\n                     'formation of the \\nlegislative body',\\n             'timestamp': (36.61, 59.75)},\\n            {'text': ' and cannot dissolve \\nany part of it. The king '\\n                     'has the same right of \\nbringing forward '\\n                     'measures as the \\nchambers.',\\n             'timestamp': (59.75, 67.09)},\\n            {'text': ' A right which the\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"president does not possess.',\\n             'timestamp': (67.09, 70.43)}],\\n 'text': ' the second in importance is as \\nfollows. Sovereignty may '\\n         'be defined to be the right of \\nmaking laws. In France, the '\\n         'king really exercises a portion of \\nthe sovereign power, '\\n         'since the laws have no weight till \\nhe has given his assent '\\n         'to them. He is moreover the \\nexecutor of the laws, but he '\\n         'does not really cooperate in their \\nformation since the '\\n         'refusal of his asset does not annul \\nthem. He is therefore '\\n         'merely to be considered as the \\nagent of the sovereign '\\n         'power. But not only does the king \\nof France exercise a '\\n         'portion of the sovereign power, He \\nalso contributes to the '\\n         'nomination of the legislature, \\nwhich exercises the other '\\n         'portion. He has the privilege of \\nappointing the members of '\\n         'one chamber and of dissolving the \\nUnited States has no '\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"'share in the formation of the \\nlegislative body and cannot '\\n         'dissolve any part of it. The king \\nhas the same right of '\\n         'bringing forward measures as the \\nchambers. A right which '\\n         'the president does not possess.'}\\nYou may notice that the transcript is mostly accurate, but there\\nare one or two sentences present in the audio that are missing\\nfrom the transcript. This is a common issue with the smaller\\nWhisper models. Since these models are generative (i.e., they\\ngenerate text rather than directly classifying sounds into\\ntokens), they can occasionally miss words or even hallucinate\\ncontent. Let’s discuss some strategies for evaluating these\\nmodels.\\nEvaluation\\nWith so many models that can perform audio tasks, deciding\\nwhich one to pick can be complex. For pre-trained models, one\\nusually looks at multiple tasks’ downstream performance.\\nAlthough the most common evaluation downstream task is\\nAutomatic Speech Recognition, we can also evaluate pre-trained\\nmodels ﬁne-tuned in other tasks such as keyword spotting,\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='intent classiﬁcation, or speaker identiﬁcation. Apart from\\nperformance in downstream tasks, we also need to look into\\nother factors like the model’s size, inference speed, languages\\nfor which it was trained, and proximity of the training data\\nwith the inference data. For example, if you’re working with a\\nspeciﬁc accent, you might want to ﬁne-tune a model with data\\nfrom that accent. If you need real-time inference, you will want\\na smaller variant.\\nIn this section, we’ll do a high-level evaluation of diﬀerent\\nmodels for English speech recognition. Although this won’t\\nprovide an end-to-end evaluation framework for picking the\\nbest model for your use case, it does give some insights into\\npractical ways for analyzing model performance. Let’s evaluate\\ntwo small multilingual models: the 74 million version of\\nmultilingual Whisper and the 94 million parameter version of\\nWav2Vec2. We can compare the inference speed and peak\\namount of GPU used to get started.\\nfrom genaibook.core import \\nmeasure_latency_and_memory_use\\nwav2vec2_pipe = pipeline(\\n    \"automatic-speech-recognition\",\\n    model=\"facebook/wav2vec2-base-960h\",'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='device=device,\\n)\\nwhisper_pipe = pipeline(\\n    \"automatic-speech-recognition\", \\nmodel=\"openai/whisper-base\", device=device\\n)\\nwith torch.inference_mode():\\n    measure_latency_and_memory_use(\\n        wav2vec2_pipe, array, \"Wav2Vec2\", \\ndevice, nb_loops=100\\n    )\\n    measure_latency_and_memory_use(\\n        whisper_pipe, array, \"Whisper\", \\ndevice=device, nb_loops=100\\n    )\\nWav2Vec2 execution time: 0.009195491333007812 \\nseconds\\nWav2Vec2 max memory footprint: \\n1.7330821120000002 GB\\nWhisper execution time: 0.092218232421875 \\nseconds\\nWhisper max memory footprint: 1.6933248 GB\\nUnsurprisingly, the maximum memory footprint (how much\\nVRAM was used) of both models is very similar, which makes'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='sense given both have a similar number of parameters. A 1.7 GB\\nfootprint is relatively small, as it can run on laptops and even\\non some powerful phones. Wav2Vec2 is signiﬁcantly faster,\\nwhich is expected given that Whisper’s decoder generates text\\none token at a time.\\nLet’s now look at how good both models are regarding high-\\nquality predictions. The most common metric for evaluating\\nASR models is the Word Error Rate (WER), which calculates\\nhow many errors there are by looking at the diﬀerences\\nbetween a prediction and the original label. The error is\\ndetermined based on how many substitutions, insertions, and\\ndeletions are needed to get from the prediction to the label. For\\nexample, given the source truth \"how can the llama jump\"\\nand prediction \"can the lama jump up“, we have:\\nOne deletion, as \"how\" is missing\\nOne substitution, a \"llama\" was replaced with \"lama\"\\nOne insertion, as \"up\" is only in the prediction\\nThe word error rate is the sum of the errors divided by the\\nnumber of words in the label, so we have a WER of 0.6 (three\\nerrors divided by ﬁve predictions). Note that although there’s\\njust one character diﬀering between \"llama\" and \"lama“,\\nthat’s counted as a full error. Many alternative metrics, such as'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Character Error Rate (CER), evaluate the diﬀerence based on\\neach character. Still, the industry widely adopts WER as the go-\\nto metric for ASR evaluation. The evaluate library provides a\\nhigh-level API interface to use these metrics. Using the\\nexamples, let’s learn how to load and calculate the WER metric.\\nfrom evaluate import load\\nwer_metric = load(\"wer\")\\nlabel = \"how can the llama jump\"\\npred = \"can the lama jump up\"\\nwer = wer_metric.compute(references=[label], \\npredictions=[pred])\\nprint(wer)\\n0.6\\nA second aspect to consider before doing evaluation is that\\ndiﬀerent ASR models have diﬀerent output formats based on\\ntheir training data. For example, Whisper was trained with\\ncasing and punctuation, so its transcriptions contain them. To\\nbe fair when evaluating models, we can normalize the labels\\nand predictions before computing the WER. This is not perfect,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='as a model that does learn casing and punctuation would not\\nget a lower error than a model that does not, but it’s a solid\\nstarting point.\\ntransformers oﬀers diﬀerent normalizers (BasicNormalizer,\\nEnglishTextNormalizer, etc). The BasicTextNormalizer\\nremoves succesive whitespaces, basic punctuation, and\\nlowercases the text. The EnglishNormalizer is more\\nadvanced, standardizing numbers (million to 1000000),\\nmanaging contractions, and more. Let’s use the\\nBasicNormalizer as it will work well - the important part is\\nthat we normalize all text and transcriptions consistently.\\nfrom \\ntransformers.models.whisper.english_normalizer \\nimport BasicTextNormalizer\\nnormalizer = BasicTextNormalizer()\\nprint(normalizer(\"I\\'m having a great day!\"))\\ni m having a great day\\nWe’ll compare the models using Common Voice, a popular\\ncrowd-sourced multilingual dataset. We’ll use a part of the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='English and French test splits of the dataset for demonstration\\npurposes, but we could use other languages (although we would\\nneed to be careful with the tokenization). We’ll evaluate both\\nWER and CER on 200 samples of each language.\\nNOTE\\nThe Common Voice dataset is public, but you need to accept the terms and conditions\\nand share your name and email with its author, the Mozilla Foundation. You can\\neasily complete this step by visiting the dataset page and clicking the button to signal\\nacceptance. If you don’t agree with the terms, feel free to use another dataset or your\\nown data for evaluation.\\nLet’s begin by implementing the evaluation pipeline.\\n# This code example is optimized for \\nexplainability\\n# The inference could be done in batches for \\nspeedup, for example.\\nfrom datasets import Audio\\ndef normalize(batch):  \\n    batch[\"norm_text\"] = \\nnormalizer(batch[\"sentence\"])\\n    return batch'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='def prepare_dataset(language=\"en\", \\nsample_count=200):\\n    dataset = load_dataset(\\n        \"mozilla-\\nfoundation/common_voice_13_0\",\\n        language,\\n        split=\"test\",\\n        streaming=True,\\n    )  \\n    dataset = dataset.cast_column(\"audio\", \\nAudio(sampling_rate=16000))  \\n    dataset = dataset.take(sample_count)  \\n    buffered_dataset = [sample for sample in \\ndataset.map(normalize)]  \\n    return buffered_dataset\\ndef evaluate_model(pipe, dataset, lang=\"en\", \\nuse_whisper=False):\\n    predictions, references = [], []\\n    for sample in dataset:\\n        if use_whisper:\\n            extra_kwargs = {\\n                \"task\": \"transcribe\",\\n                \"language\": f\"<|{lang}|>\",\\n                \"max_new_tokens\": 100,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='}  \\n            transcription = pipe(\\n                sample[\"audio\"][\"array\"],\\n                return_timestamps=True,\\n                generate_kwargs=extra_kwargs,\\n            )\\n        else:\\n            transcription = \\npipe(sample[\"audio\"][\"array\"])\\n        \\npredictions.append(normalizer(transcription[\"text\\n        \\nreferences.append(sample[\"norm_text\"])\\n    return predictions, references\\nImplement function to normalize a batch using Whisper\\nEnglish normalization.\\nLoad the Common Voice dataset in streaming mode.\\nResample the audio dataset to 16kHz.\\nSample 200 samples from the dataset.\\nCallout text TK'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='For Whisper, we add additional parameters for its\\ngeneration (e.g.\\xa0specifying the language and task).\\nNow that we have the evaluation pipeline, let’s try it with the\\ntwo models and two languages. We’ll ﬁrst specify the suite.\\neval_suite = [\\n    [\"Wav2Vec2\", wav2vec2_pipe, \"en\"],\\n    [\"Wav2Vec2\", wav2vec2_pipe, \"fr\"],\\n    [\"Whisper\", whisper_pipe, \"en\"],\\n    [\"Whisper\", whisper_pipe, \"fr\"],\\n]\\nNow that we have all components in place, let’s run the\\nevaluation.\\ncer_metric = load(\"cer\")\\n# Pre-process the English and French datasets\\nprocessed_datasets = {\\n    \"en\": prepare_dataset(\"en\"),\\n    \"fr\": prepare_dataset(\"fr\"),\\n}\\nfor config in eval_suite:\\n    model_name, pipeline, lang = config[0], \\nconfig[1], config[2]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='dataset = processed_datasets[lang]\\n    predictions, references = evaluate_model(\\n        pipeline, dataset, lang, model_name \\n== \"Whisper\"\\n    )\\n    # Compute evaluation metrics\\n    wer = \\nwer_metric.compute(references=references, \\npredictions=predictions)\\n    cer = \\ncer_metric.compute(references=references, \\npredictions=predictions)\\n    print(f\"{model_name} metrics for lang: \\n{lang}. WER: {wer}, CER: {cer}\")\\nReading metadata...: 16372it [00:00, \\n26197.69it/s]\\nReading metadata...: 16114it [00:00, \\n36235.41it/s]\\nWav2Vec2 metrics for lang: en. WER: \\n0.44012772751463547, CER: 0.22138524750538055'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Wav2Vec2 metrics for lang: fr. WER: \\n1.0099113197704748, CER: 0.5745033112582781\\nWhisper metrics for lang: en. WER: \\n0.2687599787120809, CER: 0.14674232048522795\\nWhisper metrics for lang: fr. WER: \\n0.5477308294209703, CER: 0.27584621044885943\\nLet’s discuss the results\\nWhisper clearly outperforms Wav2Vec2 in both English and\\nFrench (lower error rate is better).\\nAlthough Wav2Vec2 WER for English is high, you can notice\\nthat the CER is much lower. As discussed before, Wav2Vec2\\nis an acoustic model, and it can generate spelling errors.\\nWhisper, on the other hand, has engrained language\\nmodeling, so it’s more likely to generate the correct words.\\nWhisper outperforming Wav2Vec2 in French is not\\nsurprising, after all, Whisper was trained with multilingual\\ndata, while Wav2Vec2 was trained with English data.\\nWe could experiment with a larger Whisper variant and further\\nreduce the French WER (the largest variant is 20 times larger\\nthan the base version). A second interesting aspect is that\\nWhisper performed better than Wav2Vec2 in English. Whisper\\nis multilingual, so one could expect it to perform worse in\\nEnglish than a model tuned entirely with English data. Apart'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from this, OpenAI also released a Whisper model of the same\\nsize but entirely trained on English data. If we wanted to put\\nWhisper in production and knew all our users would speak in\\nEnglish, then it would be better to switch to the English variant.\\nRemember that Whisper is an auto-regressive model. Because\\nof this, it can \"hallucinate“, and sometimes, it can keep\\ngenerating tokens. This is one of the main issues compared to\\nencoder-only approaches. Given that WER has no upper bound,\\na single hallucination can signiﬁcantly drive the WER up. One\\napproach is to limit the maximum number of generated tokens.\\nSince Whisper is designed to transcribe 30 second segments, we\\ncould assume 60 words or approximately 100 tokens. Another\\nmethod is to ground the model to discourage hallucination by\\nforcing it to return the timestamps. In an initial experiment of\\n100 samples, the WER went from 1.72 to 0.84, just by applying\\nthis method.  If hallucination is an issue, how come Whisper\\noutperformed Wav2Vec2 in the previous evaluation? The\\nanswer is a mix of Whisper’s language modeling, the labeled\\ndata, and the massive amount of data used for pre-training.\\nWe suggest to spend some time experimenting:\\nwith diﬀerent model sizes (e.g., Whisper Tiny, of 39 million\\nparameters, vs.\\xa0Large V2, of 1.5 billion parameters, v.s. the\\n1 0'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='distilled variants)\\nwith diﬀerent models (e.g., Wav2Vec2 models ﬁne-tuned in\\nFrench) or the Whisper variant trained only for English\\nwith diﬀerent generation parameters\\nNOTE\\nIf the ASR evaluation topic interests you, we suggest reading the End-to-end Speech\\nBenchmark paper.  The benchmark proposes comparing multiple end-to-end\\nsystems with a uniﬁed evaluation using many datasets. There is also a public open-\\nsource leaderboard for speech recognition models, updated regularly with the latest\\nmodels.\\nThat was a fun dive into ASR technologies, let’s now explore\\nhow to do the inverse: convert text to speech, and then\\ngeneralize to audio generation.\\nFrom Text to Speech to Generative\\nAudio\\nSo far, we’ve learned how to do high-quality transcriptions with\\ntransformers-based models. In this third part of the audio\\nchapter, we’ll dive into audio generation techniques, their\\nevaluations, and challenges. We’ll learn about two popular TTS\\n1 1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='models: SpeechT5 and Bark. Then, we’ll brieﬂy discuss models\\nthat can go beyond speech and generalize to other forms of\\naudio (e.g., music), such as MusicGen, AudioLDM, and\\nAudioGen. Finally, we’ll explore how to use diﬀusion models to\\ngenerate audio with Riﬀusion and Dance Diﬀusion.\\nTraining and evaluating Text to Audio (TTA) models from\\nscratch can be signiﬁcantly expensive and challenging. Unlike\\nASR, TTA models can have multiple correct predictions. Think\\nof a TTS model : the generation can have diﬀerent\\npronunciations, accents, and speaking styles, and all be correct.\\nOn top of that, popular ASR datasets, such as Common Voice,\\ntend to contain noise, as we want to build robust systems for\\ndiﬀerent conditions. Unfortunately, noise in datasets is an\\nundesired trait for TTA as the models would also learn to\\ngenerate it. Imagine your generated speech has a dog barking\\nor a car honking in the background. For this reason, training\\ndatasets for TTA need to be of high quality.\\nGenerating Audio with Sequence-to-\\nSequence Models\\nSpeechT5 is a pre-trained open model by Microsoft that can\\nperform Speech to Text tasks such as ASR and speaker\\nconversion, Speech to Speech tasks such as enhancement and\\n1 2 \\n1 3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='voice conversion, and Text to Speech. SpeechT5, whose\\narchitecture is depicted in Figure 9-14, uses an encoder-decoder\\nsetup where the inputs and outputs can be either speech or\\ntext. To manage inputs from diﬀerent modalities, speech and\\ntext pre-nets convert the input into a hidden representation\\nthat the encoder receives afterward. More speciﬁcally, the\\nencoder pre-nets convert the input waveforms and texts to a\\ncommon hidden representation used by the encoder. Similarly,\\nthe decoder inputs and outputs are pre and post-processed with\\nspeech and text decoder pre and post-nets. This yields six\\nadditional nets, which provide ﬂexibility to accomplish multiple\\ntasks with the same model.\\nText encoder pre-net: A text embedding layer that maps to\\nthe hidden representations the encoder expects.\\nSpeech encoder pre-net: Same idea as the feature\\nextractor from Wav2Vec2: a CNN that pre-processes the\\ninput waveform.\\nText decoder pre-net: During pre-training, this is the same\\nas the text encoder pre-net. In ﬁne-tuning, this is modiﬁed.\\nSpeech decoder pre-net: Takes a log-mel spectrogram and\\ncompresses it into a hidden representation.\\nText decoder post-net: Single linear layer that projects\\ninto probabilities over vocabulary.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Speech decoder post-net: Predicts the output spectrogram\\nand reﬁnes it using additional convolutional layers.\\nFigure 9-4. Figure 9-14. Architecture of the SpeechT5 model. Image inspired by the\\noriginal paper\\nFor example, if we want to perform Automatic Speech\\nRecognition, our input will be audio and the output will be text.\\nIn that case, we’ll want to use the speech encoder pre-net and\\nthe text decoder nets (both pre and post-nets). transformers\\ncomes with a processor class (SpeechT5Processor) that oﬀers\\nthe functionalities to process audio and text inputs and outputs.\\nThe structure is very similar to the ASR section.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from transformers import \\nSpeechT5ForSpeechToText, SpeechT5Processor\\nprocessor = \\nSpeechT5Processor.from_pretrained(\"microsoft/spee\\nmodel = \\nSpeechT5ForSpeechToText.from_pretrained(\"microsof\\ninputs = processor(\\n    audio=array, sampling_rate=sampling_rate, \\nreturn_tensors=\"pt\"\\n)\\nwith torch.inference_mode():\\n    predicted_ids = model.generate(**inputs, \\nmax_new_tokens=70)\\ntranscription = \\nprocessor.batch_decode(predicted_ids, \\nskip_special_tokens=True)\\nprint(transcription)\\n[\\'chapter sixteen i might have told you of \\nthe beginning i might \\'\\n \\'have told you of the beginning of the \\nbeginning of the beginning \\''),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"'of the beginning of the beginning chapter \\nsixteen']\\nNow, we want to perform Text to Speech. We expect text input\\nand speech output, so we use the text encoder pre-net and the\\nspeech decoder nets. To support multi-speaker TTS, SpeechT5\\nexpects to receive something called speaker embeddings. These\\nembeddings represent information about the speaker, such as\\ntheir voice and accent, later enabling SpeechT5 to generate\\nspeech with such style. The embeddings are extracted with x-\\nvectors, a technique that maps input audios of any length to a\\nﬁxed-dimensional embedding. SpeechT5 smartly leverages the\\nx-vector by concatenating it with the output of the speech-\\ndecoder pre-net, hence incorporating information about the\\nspeaker when decoding.\\nWe could use a random speaker embedding (e.g., using\\ntorch.zeros1, 512), but the results will be very noisy.\\nLuckily, there are some existing speaker embeddings online\\nthat we can leverage.\\nfrom transformers import \\nSpeechT5ForTextToSpeech\\nfrom genaibook.core import\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='get_speaker_embeddings\\nprocessor = \\nSpeechT5Processor.from_pretrained(\"microsoft/spee\\nmodel = \\nSpeechT5ForTextToSpeech.from_pretrained(\"microsof\\ninputs = processor(text=\"There are llamas all \\naround.\", return_tensors=\"pt\")\\nspeaker_embeddings = \\ntorch.tensor(get_speaker_embeddings()).unsqueeze\\nwith torch.inference_mode():\\n    spectrogram = \\nmodel.generate_speech(inputs[\"input_ids\"], \\nspeaker_embeddings)\\nimport numpy as np\\n# |echo: false\\nfrom matplotlib import pyplot as plt\\nplt.figure()\\nplt.imshow(np.rot90(np.array(spectrogram)))\\nplt.show()'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 9-15. The output from SpeechT5 is a log-mel\\nspectrogram.\\nAs you can observe in Figure 9-15, the model output is a log-mel\\nspectrogram rather than a waveform. Spectrograms are\\npowerful tools, but they also have their limitations. While\\nconverting waveforms to spectrograms using Short-Time\\nFourier Transforms is straightforward, the reverse process of\\nconverting spectrograms to waveforms is not. Unfortunately,\\nspectrograms don’t contain all the information required to\\nreconstruct the original sound. To understand this limitation,\\nlet’s look at the general formula for a sine wave:'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='F(t)=Asin(2πft+Φ)\\nA tells us the amplitude, f is the frequency, and t is the input\\ntime. All of this information is present in the spectrogram. One\\nelement is missing, Φ (Phi). Φ represents the phase, which\\nprovides additional information about the signal. While\\nmagnitude and pitch are the more relevant properties, phase\\ninformation is essential for accurate audio reconstruction. As a\\nresult, we need techniques to reconstruct the original\\nwaveform (including the phase information) from a\\nspectrogram. This is an excellent opportunity to learn about\\nvocoders.\\nA classical reconstruction approach is the Griﬃn-Lim\\nalgorithm, an iterative algorithm that reconstructs a waveform\\nfrom the predicted spectrogram. This algorithm is popular as\\nit’s simple and fast, but it can lead to low-quality audio. The\\nGriﬃn-Lim algorithm is good enough for some spectrogram\\ngenerations, but, unfortunately, if you look at the spectrogram\\ngenerated in the previous example, you might notice that the\\nimage quality could improve. Using classical techniques to\\nconvert it to waveforms will produce lots of noise and artifacts,\\nso we must dive into fancier techniques that involve neural\\nnetworks.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Obtaining training data for spectrogram to waveform\\nreconstruction is extremely easy, which has led to a rise in\\nrecent years of research on trainable models, called neural\\nVocoders, that receive some feature representations or\\nspectrograms and convert them to waveforms. WaveNet was\\none of the ﬁrst - a famous model from DeepMind that got high-\\nquality reconstructions. WaveNet, while being of high-quality, is\\nan auto-regressive model with slow results, making it unusable\\nfor real-world use. There has been work on top of WaveNet to\\nmake it much faster, but unfortunately, it required lots of\\noptimization and powerful GPUs.\\nGAN-based approaches have become popular high-quality real-\\ntime alternatives for spectrogram to waveform reconstruction.\\nAt a high level, they use an adversarial training approach in\\nwhich a model (called generator) receives mel spectrograms\\nand outputs waveforms, and a discriminator model determines\\nif the quality of the audio is close enough to the ground truth,\\nhence leading to both the generator and discriminator to\\nimprove. MelGAN and HiFiGANs are popular GAN-based\\napproaches. They are fast and parallelizable, have quality\\nmatching WaveNet’s, and have good open-source\\nimplementations. We’ll use HiFiGAN as a vocoder for SpeechT5.\\nHiFiGAN’s generator is a CNN with two discriminators that help\\nevaluate diﬀerent aspects of the audio, pushing the CNN to'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='generate high-quality audio. We can use SpeechT5HifiGan\\nwith transformers by passing the spectrogram directly\\n(alternatively, we can specify the vocoder parameter when\\ngenerating speech).\\nfrom transformers import SpeechT5HifiGan\\nvocoder = \\nSpeechT5HifiGan.from_pretrained(\"microsoft/speech\\nwith torch.inference_mode():\\n    # Alternatively\\n    # model.generate_speech(\\n    #   inputs[\"input_ids\"],\\n    #   speaker_embeddings,\\n    #   vocoder=vocoder)\\n    speech = vocoder(spectrogram)\\nRemember that you can play the audio doing Audio(array,\\nrate=sampling_rate) in a notebook or explore some\\ngenerations we’ve compiled at\\nhttps://huggingface.co/spaces/genaibook/audio_visualizations.\\nWe can generate a spectrogram with SpeechT5 and convert the\\nspectrogram into a waveform with HiFiGAN. SpeechT5 was\\ntrained in English, so it will perform poorly in other languages.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='It’s possible to ﬁne-tune it for other languages, but it will have\\nsome limitations, such as not supporting characters outside the\\nEnglish language. Fine-tuning for other languages also requires\\nobtaining speaker embeddings for non-English speakers, so the\\nmodel is expected to perform worse. If you played with\\ndiﬀerent speaker embeddings, the quality of the results highly\\ndepends on the speaker embeddings. Maybe we can do better?\\nAn ideal TTS setup is one in which we can have a single model\\n(rather than a spectrogram generator plus a vocoder), train it\\nend-to-end, be ﬂexible to use it for multiple speakers, generate\\nlong audios, and have fast inference. This leads us to a new\\nmodel called VITS, a parallel end-to-end method. At a high level,\\nVITS can be seen as a conditional Variational AutoEncoder. VITS\\ncombines multiple tricks, some of which are familiar to us. VITS\\nuses a transformer encoder as the main encoder and the\\nHIFiGAN generator as the decoder. Other components help\\nimprove quality and ﬂexibility to tackle the one-to-many\\nchallenge of TTS. During training, another trick was to compare\\nthe mel spectrogram rather than the ﬁnal raw waveform when\\ncomputing the reconstruction loss. This helps the training focus\\non improving the perceptual quality; if you remember, mel\\nspectrograms approximate how we perceive sound. The model\\nis pushed to generate better perceivable speech by\\nincorporating it in the reconstruction loss.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='VITS was released in 2021 by Kakao Enterprise and was among\\nthe state-of-the-art models. In 2023, Meta released Massively\\nMultilingual Speech (MMS), a massive multilingual dataset. This\\ndataset led to many exciting results. First, it contains data to\\nidentify among 4000 spoken languages (audio classiﬁcation\\ntask). They also released TTS and ASR data for more than 1100\\nlanguages. The authors built a new pre-trained Wav2Vec2\\nmodel and released a multilingual ASR ﬁne-tune for the 1100\\nlanguages. What does all of this have to do with TTS? The MMS\\nauthors also trained separate VITS models for each language,\\nleading to high-quality TTS models for many languages such as\\nVietnamese and Dutch, obtaining better results than the\\noriginal VITS models.  This is an excellent example of how\\nimproving training data can lead to better results. Let’s use this\\nmodel to generate some speech.\\nfrom transformers import VitsModel, \\nVitsTokenizer, set_seed\\ntokenizer = \\nVitsTokenizer.from_pretrained(\"facebook/mms-\\ntts-eng\")\\nmodel = \\nVitsModel.from_pretrained(\"facebook/mms-tts-\\neng\")\\n1 4'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='inputs = tokenizer(text=\"Hello - my dog is \\ncute\", return_tensors=\"pt\")\\nset_seed(555)  # make deterministic\\nwith torch.inference_mode():\\n    outputs = model(inputs[\"input_ids\"])\\noutputs.waveform[0]\\nGoing Beyond Speech with Bark\\nWe just learned how to generate speech with SpeechT5 and\\nVITS. This can be applied to many use cases, but other\\ngenerative audio applications go beyond speech. For example,\\nyou might want a model that can generate sounds like laughing\\nor crying. You could also want to have a model that can\\ngenerate songs.\\nThe third generative model we’ll visit is Bark (Figure 9-16), by\\nSuno AI, another transformer-based model. Bark is one of the\\nmost popular open generative audio models out there. Bark can\\ngenerate some sounds apart from speech. For example, you can\\nhave laugh or sigh in the prompt, and corresponding\\nsounds will be integrated into the speech. We can even make it\\na bit musical by using ♪  in the prompts. Bark is multilingual and'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='includes a library of voice presets (similar to the speaker\\nembeddings), which allows us to generate speech with diﬀerent\\nvoices.\\nFigure 9-5. Figure 9-16. The Bark pipeline uses three components: text-to-semantic\\ntokens, semantic-to-coarse tokens, and coarse-to-ﬁne-tokens.\\nBefore we explore the details of Bark, there’s one more concept\\nto cover: audio codecs. When compressing audio, the goal is to\\nreduce the ﬁle size (or bitrate) while keeping the sound quality\\nas high as possible. Researchers have been trying to neural\\nnetworks for this purpose for years, leading to advanced tools\\nlike SoundStream and EnCodec. EnCodec, developed by Meta, is\\na popular open-source neural codec that can compress audio in\\nreal-time with high quality.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Here’s how it works: EnCodec uses a three-step process. First,\\nthe encoder compresses the audio into a a latent\\nrepresentation. Next, a quantization layer turns this into a\\ncompact, more eﬃcient format (a compressed\\nrepresentation).  Finally, the decoder reconstructs the audio\\nfrom the compressed data. The quantized latent space is\\nrepresented by codebooks, each containing multiple possible\\nvectors. For example, an input audio might be represented\\nusing 32 codebook vectors (each with 1024 entries) in the\\nquantized latent space.\\nWith codecs under our toolkit, let’s go back to Bark. The goal of\\nBark is to receive text and map it into codebooks. Then, Bark\\nuses the decoder component of a neural codec to convert the\\ncodebooks to audio. There are four components that make this\\npossible:\\n1. Text model: An auto-regressive decoder transformer with\\na language modeling head on top that maps text prompts\\ninto high-level semantic tokens. Thanks to this, Bark can\\ngeneralize to sounds beyond the training data, such as\\nsound eﬀects and lyrics.\\n2. Coarse acoustics model: Same architecture as the text\\nmodel, but it maps the semantic tokens from the text model\\ninto the ﬁrst two audio codebooks.\\n1 5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='3. Fine acoustics model: A non-causal AutoEncoder\\ntransformer that iteratively predicts the following\\ncodebooks based on the sum of the initial ones. It outputs in\\ntotal two coarse codebooks plus six generated ones.\\n4. Codec: Once all eight codebook channels are predicted, the\\ndecoder part of the EnCodec model decodes the output\\naudio array.\\nThere are multiple conﬁgurable parameters. For example, we\\ncan modify the number of codebooks generated by the coarse\\nand ﬁne acoustics models. We can also play with the size of\\neach codebook, which is 1024 in the oﬃcial implementation.\\nThis architecture allows the model to generate new sounds,\\nspeech in multiple languages, and more. Let’s try it out.\\nfrom transformers import AutoModel, \\nAutoProcessor\\nprocessor = \\nAutoProcessor.from_pretrained(\"suno/bark-\\nsmall\")\\nmodel = AutoModel.from_pretrained(\"suno/bark-\\nsmall\").to(device)\\ninputs = processor(\\n    text=['),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='\"\"\"Hello, my name is Suno. And, uh — \\nand I like pizza. [laughs]\\n        But I also have other interests such \\nas playing tic tac toe.\"\"\"\\n    ],\\n    return_tensors=\"pt\",\\n).to(device)\\nspeech_values = model.generate(**inputs, \\ndo_sample=True)\\nThose are some nice results. Suppose we wanted to condition\\nthe output to sound according to a predeﬁned speaker. In that\\ncase, we can also use speaker embeddings from an oﬃcial\\nlibrary shared by the authors . It’s also possible to train these\\nspeaker embeddings on your own voice, hence being able to\\ngenerate synthetic audio following your prosody. Let’s use one\\nof the predeﬁned voices.\\nvoice_preset = \"v2/en_speaker_6\"\\ninputs = processor(\"Hello, my dog is cute\", \\nvoice_preset=voice_preset)\\naudio_array = \\n1 6'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='model.generate(**inputs.to(device))\\naudio_array = \\naudio_array.cpu().numpy().squeeze()\\nAudioLM and MusicLM\\nControlling speech generation with additional sounds is neat,\\nbut could we generate entire melodies? The answer is yes. Let’s\\nbegin the journey with AudioLM and MusicLM, two exciting\\nmodels from Google from 2023 that can perform audio and\\nmusic generation. This can be used for all kinds of applications,\\nsuch as generating noise eﬀects in videos, adding background\\nmusic to a podcast, or designing sounds for games.\\nAudioLM receives an audio recording that is a few seconds long\\nand then generates high-quality continuations that preserve the\\nspeakers’ identity and way of speaking. The model is trained\\nwithout transcripts and annotations, making it quite\\nimpressive. How does AudioLM achieve this? Conceptually,\\nAudioLM is similar to Bark but with a diﬀerent task: sound\\ncontinuation rather than Text to Audio.\\nAudioLM (Figure 9-17) ﬁrst uses w2v-BERT, a pre-trained model\\nthat maps the waveform to rich semantic tokens (similarly to\\nhow we used a language model to generate semantic tokens'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='from text in Bark). Then, a semantic model predicts the future\\ntokens, modeling the high-level structure of the audio sequence.\\nA second model, the coarse acoustic model, uses the generated\\nsemantic tokens and the past acoustic tokens to generate new\\nones. How are the past acoustic tokens generated? We can pass\\nthe input waveform to a codec and retrieve the codebooks (the\\nquantized latent representation). This helps conserve the\\nspeaker’s characteristics and generates more coherent audio. A\\nfourth model, the ﬁne acoustic model, adds more detail to the\\naudio, reﬁning it, improving the quality, and removing lossy\\ncompression artifacts from previous stages. Finally, the tokens\\nare fed into a neural codec to reconstruct the waveform.\\nAudioLM can also be trained in music, such as piano\\nrecordings, to generate coherent continuations that conserve\\nthe rhythm and melody. Although the underlying models are\\ndiﬀerent, you might notice that the stages are similar to Bark:\\nwe train a series of models that leads to generating high-quality\\ncodebooks and then use a neural codec (EnCodec in Bark and\\nSoundStream in AudioLM) to generate the ﬁnal waveform.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 9-6. Figure 9-17. The AudioLM model pipeline. AudioLM converts the input\\naudio to a sequence of tokens and performs audio generation using language\\nmodeling techniques. Image from the original paper\\nMusicLM (Figure 9-18) takes things further by focusing on high-\\nquality music generation that accurately matches text\\ndescriptions.  For example, you can have a text description\\nsuch as \"an intense rock concert with violins\" as a\\nprompt. MusicLM uses AudioLM’s multi-stage setup and\\nincorporates text conditioning.\\nIf obtaining high-quality labeled TTS data is complex, labeled\\nTTA can be much more complicated. Text to Audio systems\\nmight involve a much broader rang of audio types, including\\nenvironmental sounds and music. Annotating a diverse range\\nof sounds with high accuracy can be more challenging and\\n1 7'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='intensive. To approach this, MusicLM uses an additional model,\\nMuLan, which, similarly to CLIP for image-text pairs, can map\\ntexts and their corresponding audio to the same embedding\\nspace. Thanks to this, MuLAN removed the need for captions\\nduring training and enabled training on vast amounts of audio\\ndata. To be more concrete, during training, MusicLM uses the\\nembeddings computed from the audio to condition the models,\\nand during inference, MusicLM uses text embeddings.\\nNOTE\\nNote that none of these models are open-sourced at the moment of writing. LAION\\nreleased an alternative to MuLAN called CLAP . Although trained with 20 times less\\ndata than MuLan, it can generate diverse music samples. EnCodec is the open-source\\nalternative for SoundStream.\\n1 8'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 9-7. Figure 9-18. MusicLM incorporates text conditioning to the AudioLM\\narchitecture, to generate audio based on a prompt. Image from the original paper\\nAudioGen and MusicGen\\nIn parallel, in 2022 and 2023, Meta released multiple open\\nmodels for audio generation guided by text. AudioGen can\\ngenerate sound and environmental eﬀects (e.g., a dog barking\\nor a door knock). AudioGen follows a similar ﬂow as Bark and\\nAudioLM. It uses the text encoder from T5 to generate text\\nembeddings and condition the generation with them. The\\ndecoder auto-regressively generates audio tokens conditioned\\non the text and audio tokens from the previous steps. The ﬁnal\\naudio tokens can ﬁnally be decoded with a neural codec. The\\nopen-source version of AudioGen is a variation of the original'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='architecture; for the neural codec, they retrained an EnCodec\\non environmental sound data. If you feel this is familiar, it’s\\nbecause it is - we’re once again doing a similar process to Bark\\nand AudioLM. So, what can we do with AudioGen? Three tasks\\nwith a single model:\\nWith the full architecture, we can create text-conditioned\\naudio, e.g.\\xa0\"a dog barks while somebody plays the\\ntrumpet\"\\nIf we remove the text encoder, we can unconditionally\\ngenerate sounds.\\nWe can do audio continuation by using input audio tokens\\nfrom an existing audio.\\nBuilding upon AudioGen, Meta released MusicGen, which was\\ntrained to generate music conditioned on text and achieved\\nbetter results across diﬀerent metrics than MusicLM. MusicGen\\npasses the text descriptions through a text encoder (such as the\\nones from T5 or Flan T5) to obtain embeddings. Then, it uses an\\nLM to generate audio codebooks conditioned on the\\nembeddings. Finally, the audio tokens are decoded using\\nEnCodec to get the waveform. Multiple MusicGen models were\\nreleased. Let’s try the smallest variant of 300 million\\nparameters and load each component.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='NOTE\\nAudioGen was trained with public datasets (AudioSet, AudioCaps, etc.). MusicGen, on\\nthe other hand, was trained with 20K hours of Meta-owned and speciﬁcally licensed\\nmusic, combining an internal dataset, ShutterStock, and Pond5 music data.\\nfrom transformers import AutoProcessor, \\nMusicgenForConditionalGeneration\\nmodel = \\nMusicgenForConditionalGeneration.from_pretrained\\n    \"facebook/musicgen-small\"\\n).to(device)\\nprocessor = \\nAutoProcessor.from_pretrained(\"facebook/musicgen-\\nsmall\")\\ninputs = processor(\\n    text=[\"an intense rock guitar solo\"],\\n    padding=True,\\n    return_tensors=\"pt\",\\n).to(device)\\naudio_values = model.generate(\\n    **inputs, do_sample=True, \\nguidance_scale=3, max_new_tokens=256\\n)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='We’ve learned to use multiple audio generation models: Bark,\\nSpeechT5, and MusicGen. Depending on the API abstractions\\nyou expect, rather than loading the model and processor\\nindependently and doing all the inference code ourselves, we\\ncan use the text-to-audio and text-to-speech pipelines\\navailable in transformers. They abstract the logic away and are\\ngreat for running inference.\\nfrom transformers import pipeline\\npipe = pipeline(\"text-to-audio\", \\nmodel=\"facebook/musicgen-small\", \\ndevice=device)\\ndata = pipe(\"electric rock solo, very \\nintense\")\\nAudio Diﬀusion and Riﬀusion\\nLet’s explore diﬀusion approaches for audio generation. With\\nspectrograms, we have an informative visual representation of\\naudio that can serve as a blueprint to be converted back into\\nsound. If we only knew of a model that could generate images,\\nwe could do exciting things… Wait, we know diﬀusion models.\\nAs discussed in Chapter 4, we can use diﬀusion pipelines to\\ncreate images (both conditioned and unconditioned). Audio'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Diﬀusion extrapolates this straightforward idea to audio: we\\ncan pick thousands of mel spectrograms from a database of\\nsongs and then train an unconditional diﬀusion model to\\ngenerate mel spectrogram images, which can be converted to\\naudio afterward. Although it sounds surprisingly simple, this\\nyields some decent results. For example, teticio/audio-diﬀusion-\\nddim-256 is a model trained with 20,000 images from the\\nauthor’s liked songs. Let’s generate a song with this model,\\nwhose output is shown in Figure 9-19.\\nfrom diffusers import AudioDiffusionPipeline\\npipe = \\nAudioDiffusionPipeline.from_pretrained(\\n    \"teticio/audio-diffusion-ddim-256\"\\n).to(device)\\noutput = pipe()'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 9-19. teticio/audio-diﬀusion-ddim-256 is an unconditional\\ndiﬀusion model whose image outputs are spectrograms.\\nWe can access the spectrogram by inspecting the result from\\nAudioDiffusionPipeline (output.images). Fortunately,\\nAudioDiffusionPipeline handles the conversion of the\\nspectrogram to audio for us (by using Griﬃn-Lim under the\\nhood, which works given the spectrogram’s quality) and returns\\nthe corresponding audio (output.audios). Figure 9-20 shows\\nthe mel-spectrogram created by librosa from the audio output.\\nNote that this model is de-noising the spectrograms directly.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Alternatively, one could use an AutoEncoder to encode the\\nimages and work in the latent space (as done in Chapter 5),\\nsigniﬁcantly speeding up model training and inference. This\\ncould be an excellent exercise if you’d like to dive deeper.\\nNOTE\\nWhen you plot the spectrogram from output.images[0], you’ll observe\\ndiﬀerences compared to the one in Figure 9-20, which was obtained from the audio\\noutput. The model is trained to generate grayscale images, whereas the mel\\nspectrogram is in color. Additionally, the generated spectrogram is horizontally\\nﬂipped due to the structure of the training data. The displayed spectrogram has been\\nﬂipped to maintain consistency with others shown in the chapter.\\nWe can take this idea further and use a text-conditioned model\\nto generate spectrograms conditioned on text. Riﬀusion, for\\nexample, is a ﬁne-tuned version of Stable Diﬀusion that can\\ngenerate images of spectrograms based on a text prompt\\n(Figure 9-21). Although this idea sounds strange, it works\\nsurprisingly well.\\nfrom diffusers import StableDiffusionPipeline\\npipe = \\nStableDiffusionPipeline.from_pretrained(\\n    \"riffusion/riffusion-model-v1\",'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='torch_dtype=torch.float16\\n)\\npipe = pipe.to(device)\\nprompt = \"slow piano piece, classical\"\\nnegative_prompt = \"drums\"\\nspec_img = pipe(\\n    prompt, negative_prompt=negative_prompt, \\nheight=512, width=512\\n).images[0]'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 9-20. Mel spectrogram of an output from Riﬀusion, a\\nﬁne-tuned version of Stable Diﬀusion.\\nThe simplicity of using Stable Diﬀusion brings lots of\\nadvantages. Common tools such as image to image, inpainting,\\nnegative prompts, and interpolation work out of the box. For\\nexample, you can convert an acoustic solo into an electric\\nguitar solo or interpolate from typing to jazz to make cool\\nsound eﬀects. Let’s take the previously generated spectrogram\\nand use an img2img pipeline with a new prompt to convert the\\npiano to a guitar song.\\nfrom diffusers import \\nStableDiffusionImg2ImgPipeline\\npipe = \\nStableDiffusionImg2ImgPipeline.from_pretrained(\\n    \"riffusion/riffusion-model-v1\", \\ntorch_dtype=torch.float16\\n)\\npipe = pipe.to(device)\\nprompt = \"guitar, acoustic, calmed\"\\ngenerator = \\ntorch.Generator(device=device).manual_seed(1024)\\nimage = pipe(\\n    prompt=prompt,'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='image=spec_img,\\n    strength=0.7,\\n    guidance_scale=8,\\n    generator=generator,\\n).images[0]\\nDance Diﬀusion\\nAlthough generating spectrograms and converting them to\\naudio clips works decently, using images to train an audio\\nmodel is only somewhat intuitive. Rather than using a\\nspectrogram, we can explore training a model that can directly\\nwork with raw audio data (the array of numbers) rather than\\nimages. As done in Chapter 4, the UNet is a convolutional neural\\nnetwork with a series of downsampling layers followed by a\\nseries of upsampling layers. Although so far we’ve been using a\\nUNet that works for 2D data (by using UNet2DModel), we can\\nalso use a UNet that works with the raw audio instead, that is,\\nhave the UNet work with an array of ﬂoat numbers\\n(UNet1DModel).\\nDance Diﬀusion is an open-source family of models for\\nunconditional audio generation that generates audio\\nwaveforms directly. There are diﬀerent models trained with\\ndiﬀerent datasets. For example, harmonai/maestro-150k is a\\n1 9'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='model trained with 200 hours of piano clips, so it can\\nunconditionally generate piano sounds.\\nfrom diffusers import DanceDiffusionPipeline\\npipe = \\nDanceDiffusionPipeline.from_pretrained(\\n    \"harmonai/maestro-150k\", \\ntorch_dtype=torch.float16\\n)\\npipe = pipe.to(device)\\naudio = pipe(audio_length_in_s=5, \\nnum_inference_steps=50).audios[0]\\nThe setup for DanceDiffusion is very similar to the one we\\nsaw in Chapter 4. By changing the UNet2DModel and data, you\\ncan get some decent results out of the box. Setting a minimal\\ntraining loop for DanceDiffusion with your data can be\\nrelatively simple and requires little setup. The quality of these\\nmodels is ok but could clearly be better.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='More on Diﬀusion Models for Generative\\nAudio\\nOne of the signiﬁcant issues of diﬀusion models is that their\\ninference speed is very slow. Inspired by Stable Diﬀusion, the\\nDiﬀSound and AudioLDM models were designed to operate on a\\nlatent space. This latent space is the CLAP latent embedding\\nspace. Just like Mulan in MusicLM and CLIP in the image\\ndomain, CLAP is a model that maps texts and audio into a\\nshared latent space. This makes the diﬀusion process much\\nfaster and removes the need for labeled data. This is very\\nsimilar to Stable Diﬀusion but in a diﬀerent modality - we use\\nCLAP rather than CLIP; we still use UNet backbones (actually,\\nthe same speciﬁcations as Stable Diﬀusion); and we use a\\nVariational AutoEncoder to decode the latent space and\\ngenerate mel spectrograms. As done in our early TTS\\nadventures, we use a vocoder, HiFi-GAN, to synthesize the audio\\nwaveform from the spectrogram. This architecture design gives\\nAudioLLM a framework for doing multiple text-guided audio\\nmanipulations. For example, it can do inpainting, super-\\nresolution, and style transfer.\\nMusicLDM modiﬁes AudioLDM to focus entirely on music. CLAP\\nwould not work out of the box as it’s pre-trained on datasets'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='dominated by sound eﬀects and other sounds but not that much\\nmusic. The authors retrained CLAP on text-music pair datasets,\\nimproving understanding of music data. The authors also\\nretrained the HiFi-GAN vocoder on music data to ensure the\\nreconstructed waveforms correspond to the music. One of the\\nrisks with text to music generation is that, given the scarcity of\\npermissive data, the diﬀusion process is more likely to\\nreconstruct examples from the original training data. To avoid\\nthis, MusicLDM uses data augmentation techniques to\\nencourage the model to explore more space in the music data.\\nFOr example, they use mixup, a technique that combines two\\naudio samples by interpolating them to create a new one. This\\nhelps prevent overﬁtting and improve generalization.\\nEvaluating Audio Generation Systems\\nTTS and TTA, just like image generation, are tricky to evaluate\\nas there’s no single correct generation. On top of that, the audio\\nquality depends a lot on human perception: imagine that you\\nuse diﬀerent models to generate a song based on a prompt;\\ndiﬀerent persons will have diﬀerent preferences, making it\\nchallenging and expensive to compare models.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='There have been approaches to deﬁning objective metrics. The\\nFrechet Audio Distance (FAD) can be used to evaluate the\\nquality of audio models when there isn’t any reference audio.\\nThis metric is correlated with human perception and allows\\nidentifying plausible audio but might not necessarily evaluate\\nthat the generated audios adhere to the prompts. Another\\napproach, if there are reference audios, is to use a classiﬁer to\\ncompute class predictions for both the prediction and the\\nreference audio and then measure the KL divergence between\\nboth probability distributions. In the lack of reference data,\\nCLAP can map the input prompt and output audio to the same\\nembedding space and compute the similarity between the\\nembeddings. This approach would be strongly biased toward\\nthe pre-training data used for CLAP.\\nUltimately, human evaluation is always needed for TTS and TTA\\nsystems. As mentioned before, the generations’ quality is\\nsubjective, so multiple humans from diverse backgrounds\\nshould ideally evaluate the system’s quality. A way to do this is\\nto compute the mean opinion score (MOS), which requires\\nhumans to evaluate generations on a certain scale. Another\\napproach is to show humans the input prompt and two\\ndiﬀerent generations from diﬀerent models. Humans are then\\nrequired to say which of the two generations is preferred.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='What’s Next?\\nThe world of generative audio is experiencing a moment of\\nunprecedented innovation and growth. Recent advancements\\nin audio and speech have been substantial, yet the ﬁeld remains\\nconstantly evolving, continuously oﬀering new frontiers for\\nexploration.\\nFor example, developing real-time, high-quality audio\\ngeneration is an exciting emerging area. The prospect of\\ngenerating high-quality audio in real-time opens up many\\npossibilities for applications across domains, from accessibility\\ntools to game development. Recent releases such as Coqui’s\\nreleased XTTS, ElevenLabs TTS tools, and Kyutai’s Moshi are\\ngreat examples of this.\\nAnother emerging topic is uniﬁed modeling, which creates\\nmodels that can be ﬂexible among diﬀerent tasks. Models such\\nas SeamlessM4T , which introduce a scalable uniﬁed speech\\ntranslation system, are a great example of this area. This single\\nmodel can support various audio tasks, such as TTS, Speech to\\nSpeech translation, Text to Speech translation (generating\\nsynthetic speech in another language), and more. This chapter\\nexplored popular high-quality speech, audio, and music\\n2 0'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='generation models. The diﬀerences between these three data\\ndomains pose challenges for training a uniﬁed model to\\ngenerate audio in the three forms. However, recent model\\ntechniques like AudioLDM 2 are exploring this direction. These\\nmodels propose a uniﬁed language of audio that enables it to\\ncreate all of them and obtain decent results comparable or\\nbetter with the other task-speciﬁc models.\\nAudio challenges and discussions extend beyond modeling. Key\\nethical concerns include data provenance, copyright laws,\\nmemorization, and ownership. While cloning one’s voice is\\ntechnically impressive, it raises serious ethical questions when\\napplied to other people’s voices without consent. Training a\\nmusic model can be an exciting intellectual and creative\\nexperiment, but determining fair dataset usage remains an\\nopen question. For instance, creating a synthetic version of a\\ncelebrity’s voice for advertisements without their consent could\\nbe seen as a violation of their personal rights.\\nAdditionally, most recent ML research for audio has primarily\\nfocused on the English language, leaving signiﬁcant progress to\\nbe made in multilingualism. If speech recognition models are\\ndeveloped predominantly for a few languages, others may\\nreceive subpar or inaccessible services and tools. These ethical\\nconsiderations underscore the importance of responsible'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='development and application of ML technologies. Addressing\\nthese issues proactively can help foster trust and ensure that\\nthe beneﬁts of these technologies are equitably distributed.\\nProject Time: End-to-end\\nConversational System\\nAcross the chapters, you’ve learned to use transformers models\\nfor generating text (chapters 2 and 6), diﬀusion models for\\nimage generation (4 and 5), and models to transcribe and\\ngenerate speech (this chapter), as well as building Gradio demos\\n(chapter 5). In this challenge, the goal is to build an end-to-end\\nGradio app in which:\\nthe user can either write or speak a prompt\\nthe prompt can either be conversational or ask to generate\\nan image\\nthe model generates a response based on the prompt - it\\ncould be an image or text\\nthe model will output the image and text, as well as the\\ncorresponding generated speech in case of text\\nThere are many design decisions in this project. Which model\\nwould you pick? How do you balance quality and speed? How\\n2 1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='can you determine if a prompt is asking to generate an image?\\nThis project is very open-ended and will require you to use the\\nskills you’ve learned across the chapters. We suggest starting\\nwith a simple version and then iterating on it. You shouldn’t\\nneed to train any models for this project, but you can ﬁne-tune\\nmodels if you want to. The goal is to build a fun and interactive\\nconversational system that can generate images, text, and audio\\nbased on user prompts.\\nSummary\\nWhat an adventure! The generative audio space is living in an\\ninspiring moment, with new models popping up every few\\nweeks, higher-quality datasets being released, and new labs\\nentering the generative audio landscape. It’s normal to feel\\noverwhelmed by the number of diﬀerent models being used in\\nthe audio domain; after all, the ﬁeld is progressing extremely\\nfast. Insights from other modalities, such as language or\\ndiﬀusion models in the latent space, have inspired many of the\\ntools we’ve employed. Audio’s inherent complexity has led us to\\nuncover new components like vocoders for spectrogram-to-\\nwaveform reconstruction and neural codecs for audio\\ncompression and decompression. While this chapter has just\\n2 1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='introduced the audio domain, it has equipped us with the\\nfoundational knowledge to delve deeper into recent research.\\nIf you wish to explore the ﬁeld further, we suggest to research\\nthe following topics:\\nCTC: To learn about the CTC algorithm used by Wav2Vec2,\\nwe recommend reading the interactive blog post Sequence\\nModeling With CTC.\\nParlerTTS: ParlerTTS is a training and inference library (as\\nwell as a family of models) for TTS. The library is\\nlightweight and can generate high-quality and\\ncustomizable speech. We recommend exploring the\\nParlerTTS GitHub repository and trying their inference and\\ntraining examples.\\nVocoders: In this chapter, we brieﬂy introduced vocoders,\\nwhich do mel spectrogram to speech, such as HiFiGAN. We\\nsuggest reading about the WaveNet, MelGAN, and HiFi-GAN\\nvocoders for a more substantial overview. What are their\\ndiﬀerences? How are they evaluated?\\nModel optimization: Diﬀerent model optimization\\ntechniques can lead to much faster audio generation with\\nminimal quality degradation. We recommend reading the\\nblog posts AudioLDM 2, but faster ⚡  and Speculative\\nDecoding for 2x Faster Whisper Inference.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Other popular models: We covered many models in this\\nchapter. Apart from diving into them, we suggest exploring\\nother popular models, such as Tacotron 2, FastSpeech,\\nFastSpeech 2, TorToiSe TTS, and VALL-E. A high-level\\nunderstanding of these models will provide a complete\\npicture of the ecosystem.\\nAs we’ve encountered a multitude of datasets and models, Table\\n9-1 and Table 9-2 succinctly summarize the key resources for\\nfurther exploration.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Dataset Description Train Hours Recommended\\nUse\\nLibriSpeech Narrated\\naudiobooks\\nEnglish: 960 Benchmarking\\nand pre-training\\nmodels.\\nMultilingual\\nLibriSpeech\\nMultilingual\\nequivalent of\\nLibriSpeech\\nEnglish:\\n44659Total:\\n65000\\nBenchmarking\\nand pre-training\\nmodels.\\nCommon\\nVoice 13\\nCrowd-\\nsourced\\nmultilingual\\nwith varying\\nquality\\nEnglish:\\n2400Total:\\n17600\\nMultilingual\\nsystems\\nVoxPopuli Europan\\nParliament\\nrecording\\nEnglish:\\n543Total:\\n1800\\nMultilingual\\nsystems,\\ndomain-speciﬁc\\nuses, non-native\\nspeakers\\nGigaSpeech Multi-\\ndomain\\nEnglish from\\nEnglish:\\n10000\\nRobustness over\\nmultiple\\ndomains'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Dataset Description Train Hours Recommended\\nUse\\naudiobooks,\\npodcasts and\\nYouTube\\nFLEURS Parallel\\nmultilingual\\ncorpus\\n10 hours for\\neach of 102\\nlanguages\\nEvaluation in\\nmultilingual\\nsettings\\n(including \"low\\ndigital resou\\nrce\" setting)\\nMMS-\\nlabeled\\nNew\\nTestament\\nread out\\nloud\\n37000 hours\\nfor a total of\\n1100\\nlanguages\\nMultilingual\\nsystems\\nMMS-\\nunlabeled\\nRecordings\\nof stories\\nand songs\\n7700 hours\\nfor 3800\\nlanguages\\nMultilingual\\nsystems\\nTable 9-1. Summary of datasets.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Model Task Model Type Notes\\nWav2Vec2 English ASR Encoder\\ntransformer\\nwith CTC\\nTrained on\\nunlabeled\\nEnglish data.\\nCan easily be\\nﬁne-tuned.\\nHuBERT English ASR Encoder\\ntransformer\\nwith CTC\\nTrained on\\nunlabeled\\nEnglish data.\\nCan easily be\\nﬁne-tuned.\\nXLS-R Multilingual\\nASR\\nEncoder\\ntransformer\\nwith CTC\\nTrained on\\nunlabeled data\\nfor 128\\nlanguages.\\nWhisper Multilingual\\nASR\\nEncoder-\\ndecoder\\ntransformer\\nTrained on a\\nmassive\\namount of\\nmultilingual\\nlabeled data.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Model Task Model Type Notes\\nSpeechT5 ASR, TTS\\nand S2S\\nEncoder-\\ndecoder\\ntransformer\\nAdds pre and\\npost-nets to\\nmap speech\\nand text to the\\nsame space.\\nHiFiGAN Spectrogram\\nto Speech\\nGAN with\\nmultiple\\ndiscriminators\\nIt’s one type of\\nvocoder.\\nEnCodec and\\nSoundStream\\nAudio\\ncompression\\nEncoder-\\ndecoder\\nUses\\nquantized\\nlatent space.\\nBark Multilingual\\nTTA\\nMulti-stage\\nauto-\\nregressive\\nPredicts\\ncodebooks and\\nuses EnCodec\\nto reconstruct.\\nMuLan and\\nCLAP\\nMap text\\nand audio to\\nthe same\\nspace\\nTransformer\\nencoder for\\ntext and CNN\\nfor audio\\nCLAP is the\\nopen-source\\nreplication of\\nMuLan.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Model Task Model Type Notes\\nAudioLM Audio\\nContinuation\\nMulti-stage\\nauto-\\nregressive\\nSimilar\\nconceptually\\nwise to Bark,\\nbut uses audio\\ninput.\\nMusicLM TTM Combines\\nMuLan and\\nAudioLM\\nIncorporates\\nMuLan to\\nremove need\\nfor labeled\\ndata\\nAudioGen TTA Multi-stage\\nauto-\\nregressive\\nEnCodec is\\nretrained on\\nenvironmental\\nsound data\\nMusicGen TTM Same as\\nAudioGen\\nMultiple\\nvariants open-\\nsourced.\\nAudioLDM TTA Latent space\\ndiﬀusion\\n(same as\\nUses CLAP for\\nlatent space.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Model Task Model Type Notes\\nStable\\nDiﬀusion)\\nMusicLDM TTM Same as\\nAudioLDM\\nRetrains CLAP\\nand HiFiGAN\\non music\\ndomain.\\nTable 9-2. Summary of models.\\nExercises\\n1. What are the pros and cons of using waveforms vs\\nspectrograms?\\n2. What’s a spectrogram, and what’s a mel spectrogram?\\nWhich one is used in models?\\n3. Explain how CTC works. Why is it needed for encoder-\\nbased ASR models?\\n4. What would happen if the inference data had a sampling\\nrate of 8kHz while the model was trained with one of\\n16kHz?'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='5. How does adding an n-gram model to an encoder-based\\nmodel work?\\n6. What are the pros and cons of encoder-based vs encoder-\\ndecoder-based models for ASR?\\n7. In which case would you prefer to use CER over WER to\\nevaluate ASR?\\n8. Which are the six diﬀerent nets used by SpeechT5? Which\\nsetup would be needed to perform voice conversion?\\n9. What’s a vocoder? In which cases would you use one?\\n10. What’s the purpose of the EnCodec model?\\n11. How do TTA models leverage Mulan/CLAP to relax the need\\nfor labeled data?\\nYou can ﬁnd the solutions to these exercises in the GitHub\\nrepository of the book.\\nChallenges\\n12. The following code snippet creates a random array and a\\nWhisper feature extractor from scratch\\nimport numpy as np\\nfrom transformers import \\nWhisperFeatureExtractor'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='array = np.zeros((16000, ))\\nfeature_extractor = \\nWhisperFeatureExtractor(feature_size=100)\\nfeatures = feature_extractor(\\n    array, sampling_rate=16000, \\nreturn_tensors=\"pt\"\\n)\\nExplore the impact of changing feature_size, hop_length,\\nand chunk_length in the shape of the input features. Then,\\nlook at the default values of the WhisperFeatureExtractor\\nin its documentation, what each of them mean, and try\\ncalculating how many features would be generated for an audio\\nchunk.\\n13. Implement voice conversion with SpeechT5 so that an input\\naudio is spoken by a diﬀerent speaker.\\n14. Implement a small training pipeline for Dance Diﬀusion.\\nYou can use the code from Chapter 4 as a starting point.\\nReferences\\n1. Agostinelli, Andrea, et al.\\xa0MusicLM: Generating Music From\\nText. arXiv, 26 Jan.\\xa02023. arXiv.org,\\nhttp://arxiv.org/abs/2301.11325'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='2. Ao, Junyi, et al.\\xa0SpeechT5: Uniﬁed-Modal Encoder-Decoder\\nPre-Training for Spoken Language Processing. arXiv, 24 May\\n2022. arXiv.org, http://arxiv.org/abs/2110.07205\\n3. Ardila, Rosana, et al.\\xa0Common Voice: A Massively-\\nMultilingual Speech Corpus. arXiv, 5 Mar.\\xa02020. arXiv.org,\\nhttp://arxiv.org/abs/1912.06670\\n4. Babu, Arun, et al.\\xa0XLS-R: Self-Supervised Cross-Lingual\\nSpeech Representation Learning at Scale. arXiv, 16 Dec.\\xa02021.\\narXiv.org, http://arxiv.org/abs/2111.09296\\n5. Baevski, Alexei, et al.\\xa0Wav2vec 2.0: A Framework for Self-\\nSupervised Learning of Speech Representations. arXiv, 22\\nOct.\\xa02020. arXiv.org, http://arxiv.org/abs/2006.11477\\n6. Barrault et al., SeamlessM4T, 22 Aug.\\xa02022.\\nhttps://ai.meta.com/resources/models-and-libraries/seamless-\\ncommunication/\\n7. Bark. 2023. Bark, 17 Sept.\\xa02023. GitHub,\\nhttps://github.com/suno-ai/bark\\n8. Borsos, Zalán, et al.\\xa0AudioLM: A Language Modeling\\nApproach to Audio Generation. arXiv, 25 July 2023.\\narXiv.org, http://arxiv.org/abs/2209.03143\\n9. Chen, Ke, et al.\\xa0MusicLDM: Enhancing Novelty in Text-to-\\nMusic Generation Using Beat-Synchronous Mixup Strategies.\\narXiv, 3 Aug.\\xa02023. arXiv.org, http://arxiv.org/abs/2308.01546'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='10. Conneau, Alexis, et al.\\xa0FLEURS: Few-Shot Learning\\nEvaluation of Universal Representations of Speech. arXiv, 24\\nMay 2022. arXiv.org, http://arxiv.org/abs/2205.12446\\n11. Conneau, Alexis, et al.\\xa0Unsupervised Cross-Lingual\\nRepresentation Learning for Speech Recognition. arXiv, 15\\nDec.\\xa02020. arXiv.org, http://arxiv.org/abs/2006.13979\\n12. Copet, Jade, et al.\\xa0Simple and Controllable Music Generation.\\narXiv, 8 June 2023. arXiv.org, http://arxiv.org/abs/2306.05284\\n13. Défossez, Alexandre, et al.\\xa0High Fidelity Neural Audio\\nCompression. arXiv, 24 Oct.\\xa02022. arXiv.org,\\nhttp://arxiv.org/abs/2210.13438\\n14. Gandhi, Sanchit. A Complete Guide to Audio Datasets. 15\\nDec.\\xa02022. https://huggingface.co/blog/audio-datasets.\\nAccessed 17 Sept.\\xa02023.\\n15. Gandhi, Sanchit, et al.\\xa0ESB: A Benchmark For Multi-Domain\\nEnd-to-End Speech Recognition. arXiv, 24 Oct.\\xa02022.\\narXiv.org, http://arxiv.org/abs/2210.13352.1\\n16. Gandhi, Sanchit, et al.\\xa0Hugging Face Audio Course.\\nhttps://huggingface.co/learn/audio-course. Accessed 17\\nSept.\\xa02023.\\n17. Hollemans, Mathijs. A Complete Guide to Audio Datasets. 8\\nFeb.\\xa02023. https://huggingface.co/blog/speecht5. Accessed 17\\nSept.\\xa02023.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='18. Hsu, Wei-Ning, et al.\\xa0HuBERT: Self-Supervised Speech\\nRepresentation Learning by Masked Prediction of Hidden\\nUnits. arXiv, 14 June 2021. arXiv.org,\\nhttp://arxiv.org/abs/2106.07447\\n19. Huang, Qingqing, et al.\\xa0MuLan: A Joint Embedding of Music\\nAudio and Natural Language. arXiv, 25 Aug.\\xa02022. arXiv.org,\\nhttp://arxiv.org/abs/2208.12415\\n20. Kim, Jaehyeon, et al.\\xa0Conditional Variational Autoencoder\\nwith Adversarial Learning for End-to-End Text-to-Speech.\\narXiv, 10 June 2021. arXiv.org,\\nhttp://arxiv.org/abs/2106.06103\\n21. Kong, Jungil, et al.\\xa0HiFi-GAN: Generative Adversarial\\nNetworks for Eﬃcient and High Fidelity Speech Synthesis.\\narXiv, 23 Oct.\\xa02020. arXiv.org,\\nhttp://arxiv.org/abs/2010.05646\\n22. Kreuk, Felix, et al.\\xa0AudioGen: Textually Guided Audio\\nGeneration. arXiv, 5 Mar.\\xa02023. arXiv.org,\\nhttp://arxiv.org/abs/2209.15352\\n23. Liu, Haohe, et al.\\xa0AudioLDM: Text-to-Audio Generation with\\nLatent Diﬀusion Models. arXiv, 9 Sept.\\xa02023. arXiv.org,\\nhttp://arxiv.org/abs/2301.12503\\n24. Panayotov, Vassil, et al.\\xa0Librispeech: An ASR Corpus Based\\non Public Domain Audio Books. 2015 IEEE International\\nConference on Acoustics, Speech and Signal Processing'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='(ICASSP), 2015, pp.\\xa05206–10. IEEE Xplore,\\nhttps://doi.org/10.1109/ICASSP.2015.7178964\\n25. Patry, Nicolas. Making automatic speech recognition work\\non large ﬁles with Wav2Vec2 in 🤗  Transformers. 1 Feb.\\xa02022.\\nhttps://huggingface.co/blog/asr-chunking. Accessed 17\\nSept.\\xa02023.\\n26. Pratap, Vineel, et al.\\xa0Scaling Speech Technology to 1,000+\\nLanguages. arXiv, 22 May 2023. arXiv.org,\\nhttp://arxiv.org/abs/2305.13516\\n27. Radford, Alec, et al.\\xa0Robust Speech Recognition via Large-\\nScale Weak Supervision. arXiv, 6 Dec.\\xa02022. arXiv.org,\\nhttp://arxiv.org/abs/2212.04356\\n28. Von Platen, Patrick. Boosting Wav2Vec2 with N-Grams in 🤗\\nTransformers. 12 Jan.\\xa02022.\\nhttps://huggingface.co/blog/wav2vec2-with-ngram. Accessed\\n17 Sept.\\xa02023.\\n29. Wu, Yusong, et al.\\xa0Large-Scale Contrastive Language-Audio\\nPretraining with Feature Fusion and Keyword-to-Caption\\nAugmentation. arXiv, 7 Apr.\\xa02023. arXiv.org,\\nhttp://arxiv.org/abs/2211.06687\\n30. Yang, Dongchao, et al.\\xa0Diﬀsound: Discrete Diﬀusion Model\\nfor Text-to-Sound Generation. arXiv, 28 Apr.\\xa02023. arXiv.org,\\nhttp://arxiv.org/abs/2207.09983.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='31. Zeghidour, Neil, et al.\\xa0SoundStream: An End-to-End Neural\\nAudio Codec. arXiv, 7 July 2021. arXiv.org,\\nhttp://arxiv.org/abs/2107.03312\\n You might be wondering about decode and mono in the Audio feature. decode\\nspeciﬁed if the data should be decoded (returned as an array of ﬂoats) or not\\n(returned as a bytes). mono speciﬁes if the audio is mono (one channel) or if there\\nare multiple channels. We’ll explore examples of these in the next sections.\\n To be precise, vibrations produce changes in the air (or other mediums), which lead\\nto sound. These vibrations cause a sound wave that leads to pressure changes in our\\nears.\\n Explaining Fourier Transformers is outside the scope of the book. There are many\\neducational resources for this such as 3Blue1Brown’s video on the topic.\\n There are some nuances to this. When we calculate the Fourier Transform of a real\\nsignal, its absolute value is symmetric. This leads to having a mirrored plot in the\\nfrequency domain. For explanation purposes, we plot the ﬁrst half.\\n Baevski, Alexei, et al.\\xa0wav2vec 2.0: A Framework for Self-Supervised Learning of\\nSpeech Representations. arXiv, 22 Oct.\\xa02020. arXiv.org, https://arxiv.org/abs/2006.11477\\n Hsu, Wei-Ning, et al.\\xa0HuBERT: Self-Supervised Speech Representation Learning by\\nMasked Prediction of Hidden Units. arXiv, 14 Jun.\\xa02021. arXiv.org,\\nhttps://arxiv.org/abs/2106.07447\\n Diving into the speciﬁcs of their training losses and architectures is outside the\\nbook’s scope, but we suggest reviewing the papers for those interested.\\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Babu, Arun, et al.\\xa0XLS-R: Self-supervised Cross-lingual Speech Representation\\nLearning at Scale. arXiv, 17 Nov.\\xa02021. arXiv.org, https://arxiv.org/abs/2111.09296\\n When the system incorporates the language model internally, it’s called deep fusion.\\nIn the case of CTC with n-gram, the language model is external, and we call it shallow\\nfusion.\\n Empirically, people have found that setting return_timestamps=True helps\\nreduce hallucinations in long-form evaluation. There’s a high-level explanation about\\nit in this forum.\\n Gandhi, Sanchit, et al.\\xa0ESB: A Benchmark For Multi-Domain End-to-End Speech\\nRecognition. arXiv, 24 Oct.\\xa02022. arXiv.org, https://arxiv.org/abs/2210.13352\\n The selection of models in this section was based in a couple of factors, including\\ntheir popularity, size, and quality. SpeechT5 is versatile to handling multiple tasks.\\nBark is one of the best open models for generating expressive speech. Diﬀusion-\\nbased techniques, although not used as much, have periods of great popularity.\\n Recall that Text to Speech is a type of Text to Audio task.\\n You can ﬁnd all MMS-based models in https://huggingface.co/models?other=mms\\n How to quantize the encoder’s output is a design decision. The EnCodec authors\\nused a technique called residual vector quantization\\n You can ﬁnd the speaker library in https://suno-\\nai.notion.site/8b8e8749ed514b0cbf3f699013548683?\\nv=bc67cﬀ786b04b50b3ceb756fd05f68c\\n MusicCaps, the evaluation dataset used for MusicLM, is open-sourced and can be\\nfound at https://huggingface.co/datasets/google/MusicCaps\\n8 \\n9 \\n 0 \\n 1 \\n 2 \\n 3 \\n 4 \\n 5 \\n 6 \\n 7'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Wu, Yusong, et al.\\xa0Large-scale Contrastive Language-Audio Pretraining with Feature\\nFusion and Keyword-to-Caption Augmentation. arXiv, 12 Nov.\\xa02022. arXiv.org,\\nhttps://arxiv.org/abs/2211.06687\\n If you peek into the generated audio data, you will notice there are two arrays. This\\nis because DanceDiffusion was trained with stereo sound.\\n Barrault et al., SeamlessM4T, 22 Aug.\\xa02022. https://ai.meta.com/resources/models-and-\\nlibraries/seamless-communication/\\n Hint: You can try heuristics, zero-shot classiﬁcation, or sentence similarity, for\\nexample.\\n 8 \\n 9 \\n 0 \\n 1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Chapter 10. Rapidly Advancing Areas\\nin Generative AI\\nA NOTE FOR EARLY RELEASE READERS\\nWith Early Release ebooks, you get books in their earliest form\\n—the authors’ raw and unedited content as they write—so you\\ncan take advantage of these technologies long before the oﬃcial\\nrelease of these titles.\\nThis will be the 10th chapter of the ﬁnal book. Please note that\\nthe GitHub repo will be made active later on.\\nIf you have comments about how we might improve the content\\nand/or examples in this book, or if you notice missing material\\nwithin this chapter, please reach out to the editor at\\njleonard@oreilly.com.\\nThe generative AI landscape is moving very fast. Since we\\nbegan working on this book, we’ve witnessed the release of new\\nmodels like GPT-4, Llama 3, Gemini, and Sora. In addition to\\nthese, numerous new base LLMs, audio models, and diﬀusion\\ntechniques have emerged. As mentioned in the preface, the\\nbook focuses on general principles and fundamentals that'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='provide generalizable skills and understanding that will allow\\nyou to follow the ﬁeld as it keeps evolving.\\nBefore wrapping the book, we want to provide a glimpse into\\nsome of the most exciting and rapidly advancing areas within\\ngenerative AI. This chapter oﬀers a high-level overview of these\\ntopics and resources to allow you to dive further if you ﬁnd\\nthem interesting. Rather than aiming to make you proﬁcient in\\nthe topics, think of this chapter as a guide to continue your\\nlearning as you go forward.\\nPreference Optimization\\nIn the ﬁne-tuning transformers chapter, we trained a chat\\nmodel based on the Open Assistant dataset of conversations in a\\nsupervised fashion. In the chapter, we used traditional ﬁne-\\ntuning, but there’s been a strong wave of models that integrate\\npreferences into the models. These models are trained to\\ngenerate responses that are aligned with certain expectations.\\nFor example, some people might want to train very helpful\\nmodels that will always try to help, regardless of the request.\\nOther companies might want to train models that are more\\nneutral and avoid them generating toxic outputs. When a model\\nsays that it can’t help, this happens due to its preference'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='optimization. Preference optimizations is used to steer an LLM\\ntowards a desired behavior, which could be anything from\\ngenerating less buggy code and generating text in a\\nconversational style to refusing generating content about\\ncertain topics.\\nReinforcement Learning with Human Feedback (RLHF), one of\\nthe methods used for preference-tuning, switches a bit the\\ntraditional ﬁne-tuning process. Just as we did in Chapter 6, the\\nﬁrst step is to ﬁne-tune the model via supervised ﬁne-tuning.\\nWith RLHF, the ﬁne-tuned model is then used to train a reward\\nmodel. For each prompt, the model will generate multiple\\npotential options. Then, a judge ranks the options, and the\\nreward model is trained to predict the judged score. The judge\\nare usually humans, but there has been a tendency to use other,\\nlarge models as judges (for example, the RLAIF paper  uses\\nLLMs for ranking). The ﬁnal stage is to use the reward model to\\nfurther ﬁne-tune the original supervised ﬁne-tuned model, so\\nthe model learns to generate outputs that resemble the high-\\nscoring (according to judges) examples. RLHF was a critical\\ncomponent for the Llama 2 chat model as well as for ChatGPT.\\n1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 10-1. Figure 10-1. RLHF three core components: pre-training, ﬁne-tuning, and\\nhuman feedback.\\nWhile the concept of introducing a reward model in the ﬁne-\\ntuning process is intriguing, it also adds a layer of complexity,\\nas can be observed in Figure 10-1. This has spurred a signiﬁcant\\namount of research aimed at replacing the reward model with\\na new loss function. Recent studies, such as Direct Preference\\nOptimization  (DPO), Identity Preference Optimization  (IPO),\\nand Kehneman-Tversky Optimization  (KTO), are notable\\nexamples of this ongoing exploration. These methods entirely\\nremove the RL component, which is known to be unstable and\\nchallenging to train.\\n2 3 \\n4'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='RLHF can also be applied to diﬀusion models. Denoising\\ndiﬀusion policy optimization  (DDPO) is a way to augment\\ndiﬀusion models using Reinforcement Learning to ﬁne-tune the\\nmodel and improve the quality of the generated images. RLHF\\ncan also be applied to diﬀusion models, as shown in Figure 10-\\n2.\\nFigure 10-2. Figure 10-2. RLHF applied for diﬀusion models.\\nThese are some additional reads in the topic:\\nThe Hugging Face RLHF introductory blog post is a great\\nstep-by-step overview of RLHF.\\nSebastian Rashka’s RLHF blog post provides a good\\noverview of RLHF and its alternatives.\\n5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='To learn about RLHF in the context of diﬀusion models, we\\nrecommend reading the DDPO blog post or Tanishq\\nAbraham’s blog.\\nTo learn more about DPO, IPO, and KTO, we recommend\\nreading the HF blog post.\\nContitutional AI is another approach for aligning a LLM to\\na set of values. The HF blog post is a good resource to dive\\ninto it.\\nLong Contexts\\nMost LLMs discussed in the book can handle a context of up to a\\nfew thousand tokens, with Llama 3.1 shining at 128,000 and\\nsome going up to a few hundred thousand tokens. Proprietary\\nmodels have achieved much longer contexts, such as Gemini\\nhandling up to 2 million tokens and Anthropic Claude\\nsupporting 200,000 tokens. Handling extremely long contexts\\ncan be very useful for RAG systems (such as the one\\nimplemented in the project of the ﬁne-tuning transformers\\nchapter) or for systems that do code generation or\\nunderstanding, where an entire codebase could be used as\\ncontext.\\nAs the input grows too long, multiple issues arise:'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='LLMs require more VRAM to be able to process long\\ncontexts.\\nThe quality of the generations tends to degrade as the\\ncontext grows. As models were not trained on such long\\ncontexts, they may not be able to capture the dependencies\\nbetween tokens.\\nGeneration becomes slow. The traditional attention\\nmechanism is a bottleneck as it requires quadratic\\ncomplexity.\\nOne solution is to use window attention, which limits the\\nnumber of tokens fed to the LLM. You can think of window\\nattention as a sliding window through the input text. This keeps\\nthe GPU usage capped, but the quality still degrades as there\\nmay be tokens that carry essential information before the\\nbeginning of the window. Window attention can be adapted to\\ntake into account the initial tokens using methods such as\\nAttention Sinks, which is great for multi-round dialogues. A\\ndiﬀerent approach is to make the attention mechanism more\\neﬃcient. There are many proposals for sub-quadratic scaling,\\nsuch as Longformer  (which combines window techniques with\\nglobal attention features), and Flash Attention (which optimizes\\nmemory transfers by fusing operations). Flash\\nAttentionootnote:[Dao, Tri, et al.\\xa0FlashAttention: Fast and\\nMemory-Eﬃcient Exact Attention with IO-Awareness. arXiv, 27\\n6'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='May, 2022. arXiv.org, https://arxiv.org/abs/2205.14135] has\\nbecome a very popular solution as it makes space requirements\\nlinear during inference.\\nWhile the mentioned approaches focus on making the attention\\nalgorithm faster or more computationally eﬃcient, there has\\nalso been research on extending pre-trained LLMs with rotary\\nposition embeddings (RoPE) scaling . Can we pick Llama and\\nmake it handle more tokens than what it was pre-trained for?\\nRoPE changes how positional information is incorporated into\\ntransformers, which allows them to capture long-range\\ndependencies. By doing minimal ﬁne-tuning, we can extend the\\ncontext window. For example, Meta was able to extend the\\ncontext window from the original LLaMA model from 2048\\ntokens to 32768 tokens. There is another wave of research that\\nis also exploring arbitrarily long contexts. Ring Attentionootnot:\\n[Liu, Hao, et al.\\xa0Ring Attention with Blockwise Transformers for\\nNear-Inﬁnite Context. arXiv, 3 Oct.\\xa02023. arXiv.org,\\nhttps://arxiv.org/abs/2310.01889] and Inﬁni-Attention  are two\\nexamples of research in the direction of being able to handle\\ninﬁnite context.\\n7 \\n8'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 10-3. Figure 10-3. Needle In A Haystack is a method to evaluate in-context\\nretrieval on long contexts. Learn more about it at\\nhttps://github.com/gkamradt/LLMTest_NeedleInAHaystack\\nThere are parallel ongoing eﬀorts in the ecosystem to use\\nalternative architectures altogether. One of them is using RNNs:\\nRWKV  is a family of open-source models that achieve eﬃcient\\ninference thanks to linear attention while preserving very\\neﬃcient and parallelizable training. Another alternative is\\nusing State Space Models (SSM). Mamba uses SSMs to achieve\\nlinear memory scaling with respect to the number of tokens\\nand has very fast inference.\\nSome recommended reads on this topic include:\\n9'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The Attention Sinks blog post explains how Attention Sinks\\nwork and the results of diﬀerent experiments.\\nThe FlashAttention and FlashAttention 2 papers.\\nThere have been many concurrent eﬀorts to extend context\\nusing RoPE, from a practitioner jumping into ML to\\nresearch from Meta.\\nTo learn more about RWKV, we recommend reading its\\nannouncement blog post, its paper or a more recent paper\\nwith architectural improvements.\\nTo learn about SSMs and Mamba, we recommend reading\\nThe Annotated S4 and Mamba: The Hard Way.\\nMixture of Experts\\nIn recent years, Mixture of Experts (MoE) has emerged as a\\ncompelling approach for LLMs, with the most notable release\\nbeing Mixtral 8x7B by the Mistral team in December 2023 . In\\nthe context of transformer models, MoEs are very similar to\\ndense (traditional) transformers, but they oﬀer advantages in\\ntraining eﬃciency and production scalability. Given a ﬁxed\\namount of compute to train a model, MoEs will get you further\\nalong the training curve than dense models. In large-scale\\nusage, MoEs can be more eﬃcient in handling many requests\\nper second.\\n1 0'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='One of the key elements of MoE models is the replacement of\\nsome or all of the feed-forward networks in transformer blocks\\nwith sparse MoE blocks. Each MoE block is a collection of\\n\"expert\" networks, with each expert being a diﬀerent model\\n(usually another feed-forward network). Given a token, only\\nsome experts will dynamically activate,  and the rest will be\\nidle. To learn which experts to activate, MoEs use a gate\\nnetwork (or router) that dynamically assigns tokens to diﬀerent\\nexperts during training and inference. This gate network acts as\\na traﬃc controller, ensuring tokens are distributed eﬀectively\\namong the experts. This makes the gate a critical component of\\nMoEs, and much research is aimed at training better gate\\nnetworks.\\n1 1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 10-4. Figure 10-4. A simpliﬁed traditional transformer block and its MoE\\nequivalent having two experts\\nIt’s essential to clarify that the number of experts in MoEs does\\nnot directly lead to a linear parameter increase. For instance, in\\nthe case of Mixtral 8x7B, the model’s name might suggest eight\\nexperts of 7 billion parameters each, and hence 56 billion\\nparameters, but in reality, it comprises 47 billion parameters'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='due to shared components, and the eight refers to the number\\nof experts for each MoE block! Remember that only the feed-\\nforward networks are replaced with MoE blocks. The rest of the\\nnetwork, such as the attention blocks, is still shared.\\nThis means that to load the model, you will need a GPU to hold\\n47B parameters. On the other hand, for a given token, only\\nsome experts will activate (2 for Mixtral). Due to this, the\\nnumber of activated parameters is much lower (12 billion for\\nMixtral). This makes MoEs less interesting for local inference\\n(because you need a lot of GPU memory) but very compelling\\nfor production setups, where you may receive many requests at\\nthe same time. Given that fewer parameters per token will be\\nactivated for each request being handled in parallel, an MoE\\nshould be able to handle more requests than a dense model.\\nA second misconception about MoEs is that the experts become\\nspecialized in diﬀerent tasks or subsets of data. Experts are\\ntrained with a loss function that ensures the token generation\\nload is distributed evenly between experts, ensuring all experts\\nare being used. Gated experts work more like a load balancer of\\nsorts, and there is no evidence of high-level task specialization.\\nMoEs have became widely popular in the ecosystem due to\\ntheir impressive quality and production properties. Apart from'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='the well-known Mixtral 8x7B model, there is also Snowﬂake\\nArctic Instruct, Databricks DBRX, Mixtral 8x22B, DeepSeek MoE\\nand Qwen 1.5 MoE.\\nTo learn more about the topic, we suggest reading the following\\nresources:\\nThe Hugging Face Introductory blog to MoEs provides a\\nhigh-level overview of the MoE research and an\\nintroductory explanation of how they work.\\nThe Switch Transformers paper dives into many challenges\\nand design decisions for building and training MoEs. It’s an\\nexcellent paper that helps understand the design questions\\nfaced when working with MoEs.\\nThe DeepSeekMoE paper introduces some novel ideas for\\nMoEs, such as segmenting experts into smaller ones and\\nshared experts that will always activate. This paper is a\\ngreat read for understanding some of the cutting-edge\\nresearch in the ﬁeld.\\nThe Mixtral paper is a nice read on how Mixtral 8x7B was\\ntrained. It does not introduce new architecture or training\\nideas, but it’s a good read for understanding how MoEs are\\ntrained in practice. This is particularly interesting given\\nthat Mixtral was the ﬁrst high-quality openly available MoE\\nmodel.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Optimizations and Quantizations\\nOptimization techniques were traditionally sought after for two\\ndiﬀerent reasons:\\nTo maximize model performance in high-load scenarios,\\nsuch as chat UIs or generative APIs used by thousands of\\nusers. Serving more users per server and time unit\\nminimizes costs and allows more users to enjoy the service.\\nTo reduce training time or training resources. For large\\nmodels, a moderate reduction in the amount of memory\\nused or in the time it takes to complete a training step can\\nresult in dramatic speedups and the use of a lot less\\nhardware. Even for small labs or individual practitioners\\ntraining smaller models, faster training allows faster\\niteration cycles and more experiments to be completed.\\nIn addition, the past few years have shown the community’s\\noverwhelming interest in running all kinds of models, including\\nLLMs, on consumer hardware. There are many reasons for this:\\nto experiment with models without an API clock ticking over\\nyour head, to understand how models work by looking at\\ninternal activations, to run tasks privately on your computer\\nand even without an Internet connection, to ﬁne-tune with your'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='data without spending a lot of money and eﬀort on cloud\\nservices, etc.\\nAs a consequence, the community is coming up with a large\\nnumber of optimization techniques and clever tricks. We’ve\\nalready brieﬂy explored quantization, a collection of techniques\\nthat aim to reduce the precision of the model parameters (with\\nminimal impact on quality) to reduce memory and ﬁt models\\non consumer hardware. Another widespread technique we just\\nmentioned in this chapter is Flash Attention, which not only\\nallows longer context windows but also reduces memory\\nconsumption of all types of transformers-based models\\n(including language models and many other generative models,\\nas we saw in the diﬀusion chapter).\\nAnother interesting speedup technique is speculative decoding,\\nalso called assisted generation, which applies to regressive\\nmodels such as LLMs. This method uses two models with the\\nsame architecture for the same task. One of the models is very\\nsmall and fast, while the other is large and has much better\\nquality. The small model generates several tokens as usual, and\\nonce a few of them are collected, they are passed together to the\\nlarge model for conﬁrmation. The large model veriﬁes them in\\na single pass instead of going through the generation loop. Only\\nthe tokens conﬁrmed by the large model are used (the rest are'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='discarded), but as long as the small and large models agree\\nfrequently, this is faster than using the large model alone. This\\nis especially useful for structured text such as code. Memory\\nconsumption will be larger because we will use two models\\ninstead of one, but throughput will be higher.\\nA somewhat similar method is called Medusa decoding. This is a\\nﬁne-tuning specialization that adds new heads to an existing\\nmodel so a sequence of several tokens can be generated at once.\\nLike in the previous case, several candidates are examined so\\nthat the generated text is the same as if Medusa was not used.\\nHere are some resources to learn more about this active topic:\\nThis post by Merve Noyan is a gentle introduction to\\nquantization, and in turn, it provides additional references\\nto dive into the topic.\\nThe llama.cpp codebase contains multiple quantization and\\noptimization techniques, including custom kernels, to\\naccelerate inference on consumer hardware, including\\nWindows and Apple Silicon.\\nQuantized, ready-to-use versions of multiple LLMs are\\nprepared and shared by community members such as\\nTheBloke (Tom Jobbins). The models this solo community\\nmember prepared have been downloaded millions of times.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The Hugging Face documentation about speculative\\nmethods discusses speculative decoding and how to use it.\\nThis blog post by Joao Gante is a great introduction to\\nassisted generation / speculative decoding.\\nLossy variants of speculative decoding are also possible. In\\nthis post, Vivien Tran-Thien does a fantastic job of\\nexploring this direction.\\nData\\nWhile much of this book has focused on models and their\\napplications, the data used to train these models is a crucial\\naspect of Machine Learning. Most pre-trained models are\\ntrained on vast amounts of web data. Still, by ﬁltering this data\\nor generating synthetic data with other LLMs, smaller models\\ncan achieve performance comparable to much larger ones.\\nHigh-quality data can signiﬁcantly enhance the performance of\\neven small models, such as Microsoft Phi 3. FineWeb is a high-\\nquality ﬁltered dataset of open web data released as a fully\\nopen-source dataset and has a technical report that shows how\\nthe tokens were ﬁltered for high quality.\\nAlthough synthetic data is not new, Phi opened the doors to\\nexploring large-scale datasets (billions of tokens) created'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='entirely with LLMs. The ﬁrst iteration of Phi, a 1.3B parameter\\nmodel, was trained with 6 billion tokens of high-quality web\\ndata and 1 billion tokens of textbooks and exercises generated\\nwith GPT-3.5. Cosmopedia is an initial eﬀort that released a\\ndataset of 25 billion tokens of synthetic data covering diﬀerent\\ntopics and was created with Mixtral 8x7B.\\nTools like Argilla’s distilabel make it easy to create synthetic\\ndata and AI feedback, which can be used for both pre-training\\nand RLHF. While Phi and Cosmopedia focus on large amounts of\\nsynthetic data for pre-training, ML models are increasingly\\nused to create preference datasets and scale up model\\nevaluation.\\nApart from the Cosmopedia release blog, we recommend\\nreading Eugene Yan’s blog post on generating and using\\nsynthetic data, the TinyStories paper and Phi’s original paper,\\nTextbooks Are All You Need, as well as the follow-up reports for\\nPhi 1.5 and Phi 3\\nFor other modalities, such as image or audio - we need not only\\nthe image and audio datasets but also text datasets that either\\ncaption or transcribe the data. Web-scale datasets with text-\\nimage pairs have been released such as LAION 2B (used to train\\nthe early models of Stable Diﬀusion), COYO 700M and'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='DataComp 1B. However, scraping billions of unﬁltered images\\nfrom the open internet may include inappropriate or illegal\\ncontent, so models trained on this unﬁltered data can pose open\\nquestions regarding copyright and fair use, as well as safety\\nchallenges. Alternative datasets with openly licensed Creative\\nCommons image-text pairs, such as CommonCanvas, have been\\nreleased to mitigate such challenges. For audio, open datasets\\nfor speech such as Mozilla’s CommonVoice, and for sound\\neﬀects such as FreeSound and Free Music Archive are well\\nadopted. Internet-scraped and licensed datasets also exist for\\naudio, but there’s no centralized dataset index equivalent to\\nCOYO or DataComp in the image domain.\\nOne Model to Rule Them All\\nThere are three main approaches to using LLMs and other\\ngenerative models for your speciﬁc use case, listed here in\\ndecreasing order of complexity:\\n1. Train a new model from scratch.\\n2. Fine-tune a pre-trained model for your use case.\\n3. Use an existing model with prompt engineering or RAG.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='The choice among these approaches depends on your resources\\nand priorities. Training a model from scratch is now\\nprohibitively expensive for most companies due to the high\\ncosts involved. Fortunately, the rise of high-quality open-source\\nmodels has made it easier to obtain high-quality results with\\nﬁne-tuning. Models like BloombergGPT (for ﬁnance),\\nAstroLLaMA (for Astronomy), and BioMistral (for medical\\ndomains) have demonstrated that ﬁne-tuning a strong pre-\\ntrained model with domain-speciﬁc data can sometimes yield\\nbetter results than using a model out of the box.\\nThat said, as models’ zero-shot capabilities improve, their\\nperformance might be good enough for many use cases without\\nany ﬁne-tuning. Depending on the project requirement, the\\ntime invested in data quality and iterating on the model could\\nbe better invested in the end product. As an example, when\\nMeta release Llama 3 and its very high quality instruct version,\\nthe community struggled to achieve better results with ﬁne-\\ntuning, as the model was already very strong out of the box.\\nAnother important consideration is how you deploy and\\nconsume the model. Options include self-hosting, using a cloud\\nprovider’s out-of-the-box solutions, or leveraging the API\\nprovided by the model trainer (e.g., OpenAI or Cohere). The best\\nchoice depends on your speciﬁc needs. The community is'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='increasingly adopting tools (like langchain and llamaindex) that\\nfacilitate easy switching between diﬀerent solutions, preventing\\ndependency on a single model provider. We recommend\\nbuilding your system in a way that allows easy evaluation and\\nswapping of models as needed.\\nComputer Vision\\nComputer Vision (CV) is a vast ﬁeld with a rich history that\\npredates ML techniques. It tries to derive meaningful\\ninformation from images and apply it to make decisions or\\nactions, such as guiding an autonomous vehicle, detecting\\ndefects in a factory line, counting objects, or monitoring traﬃc.\\nML methods revolutionized the CV ﬁeld, and, in turn, continued\\nimprovement on CV tasks sparked research on increasingly\\npowerful representation learning methods that gave rise to the\\ngenerative image explosion.\\nTraditionally, Computer Vision is approached as a set of distinct\\ntasks because it’s easier to break down those tasks into smaller\\nproblems than attempting to solve vision understanding in one\\ngo. Computer Vision tasks include:'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Image Classiﬁcation is the problem of deciding which one\\namong a set of categories best describes the content of a\\ngiven image. Progress on this task has increased\\ndramatically since 2009, when the open ImageNet dataset\\nwas published alongside the ImageNet Large Scale Visual\\nRecognition Challenge (ILSVRC). In 2012, a deep neural\\nnetwork created by Alex Krizhevsky, Ilya Sutskever and\\nGeoﬀrey E. Hinton won that year’s challenge showing an\\nunbelievable performance 41% better than the second best\\nsolution, after years of crawling marginal progress. This\\nevent is traditionally regarded as the beginning of the deep\\nlearning revolution, and it shook the way ML and CV tasks\\nwere approached.\\nObject Detection is the task of ﬁnding speciﬁc types of\\nobjects or subjects inside an image. Given an input image,\\nan object detection model generates rectangles (bounding\\nboxes) around the objects belonging to classes the model\\nrecognizes and provides a conﬁdence score about the\\nprobability that the object inside each box belongs to the\\npredicted class.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 10-5. Figure 10-5. Example of Object Detection\\nSegmentation goes one step beyond detection. This task\\nattempts to solve the problem of classifying each individual\\npixel inside an image according to a set of predeﬁned\\nclasses. A segmentation model trained on urban\\nphotographs, for example, would predict the pixels in an\\nimage corresponding to a road, a tree, people walking\\ndown the street, or cars. This is called semantic\\nsegmentation when there’s no distinction between multiple\\nobjects of the same class (all pixels belonging to persons\\nwould be assigned the same label identiﬁer) and panoptic\\nsegmentation when the model is capable of discerning'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='diﬀerent instances of distinct objects that belong to the\\nsame class.\\nDepth Estimation estimates how far objects are in an\\nimage, given just a single image with no additional\\ninformation. This is called monocular depth estimation to\\ndistinguish from other systems that use stereo inputs (2\\nimages) or other types of additional data. Monocular depth\\nestimation is useful for computer graphics, 3D, gaming,\\nphotography, or artistic tasks.\\nFigure 10-6. Figure 10-6. Example of Depth Estimation\\nImage classiﬁcation, detection, and segmentation models\\nwere traditionally trained on a speciﬁc set of known\\nclasses. For example, the ImageNet challenge was'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='conducted on 1,000 categories (even though the original\\ndataset contains about 22,000 classes). This poses a scale\\nproblem: to train models that understand the world, we’d\\nneed to use suﬃcient training data to cover all possible\\nclasses and nuanced distinctions between them. Zero-shot\\ntasks refer to the ability of a model to perform these tasks\\nwithout having been trained for a speciﬁc set of classes. For\\nexample, the CLIP model is trained from image-text pairs\\ndownloaded from the Internet, so it gets a good\\nunderstanding of image and text features that usually go\\ntogether. As we saw in the chapter about Compressing\\nInformation, a trained CLIP model can be used to classify\\nan image among a set of arbitrary classes the user provides,\\nwithout having ever been trained on images classiﬁed in\\nthose categories.\\nAs larger models are trained on increasingly larger amounts of\\ndata, they can solve tasks in a zero-shot fashion without being\\nexplicitly trained for them. In addition, these large models learn\\nto use very rich and descriptive representations, making it easy\\nto ﬁne-tune for speciﬁc tasks. Furthermore, multimodal models\\nthat combine text and visual representation (also known as\\nvision language model) can answer natural-language questions\\nabout image data so that they can be used in various workﬂows\\ninstead of task-speciﬁc specialized models. This begs the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='question, are we on the verge of another CV revolution, where\\nit may be more fruitful to train on vast datasets rather than\\nfocusing on speciﬁc tasks? Or will smaller ﬁne-tuned models\\nkeep an edge over generalist ones?\\n3D Computer Vision\\nWhile traditional Computer Vision primarily focuses on 2D\\ndata, such as images and videos, there is a growing interest in\\nunderstanding, interpreting, and generating 3D data. 3D\\nComputer Vision is widely applied in robotics, augmented\\nreality, healthcare, video production, gaming, and autonomous\\ndriving. Traditionally, 3D data has been represented using\\nmeshes—a collection of vertices, edges, and faces that deﬁne\\nthe shape of an object. However, Machine Learning techniques\\noften struggle with meshes, prompting the exploration of\\nalternative representations such as neural radiance ﬁelds\\n(NeRFs) and Gaussian Splatting.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 10-7. Figure 10-7. A mesh can be generated even from a single image.\\nUnlike Natural Language Processing, the 3D ML ecosystem is\\nsmall and highly research-oriented. As a result, many open\\ntools are experimental and in the early stages of development.\\nThe ﬁeld is rapidly evolving; for example, the ﬁrst NeRF paper\\nwas published in 2020, and Gaussian Splatting emerged in 2023.\\nFor those interested in exploring this ﬁeld further, here are\\nsome resources:\\nFrank Dellaert has published blog posts in 2020, 2021, and\\n2022 with a recap of what has happened in the NeRF\\necosystem. We suggest to read them chronologically.\\nHugging Face has a free course on Machine Learning for\\n3D. The course is a high-level, practical overview of'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='generative ML techniques that can be applied at diﬀerent\\nsteps of the 3D rendering pipeline.\\nVideo Generation\\nAs image generation models have established themselves as\\nviable use cases in generative Machine Learning, a natural\\ninquiry and research frontier is video. Video is, after all, a\\nsequence of images moving fast enough to create the illusion to\\nthe human brain that it is in motion. As such, one approach for\\nvideo generation is to leverage pre-trained image-generation\\nmodels to generate temporally and visually coherent sequences\\nof pictures. Frameworks such as AnimateDiﬀ derive a motion\\nprior to steer models like Stable Diﬀusion to produce\\ntemporally coherent videos. Pipelines such as Deforum create\\nanimations with camera control, while embracing frame\\nvariations as part of the aesthetic.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 10-8. Figure 10-8. Example of video generation\\nVideo-to-video techniques are also a way to leverage generative\\nmodels in the video space. By leveraging an existing video and\\nproviding it with a new style or new subjects, generative AI can\\ntransform videos or turn sketches into animations. The Gen-1\\npaper  by RunwayML showcases an eﬃcient and performant\\nmethod for video-to-video transformations. This model is\\ncurrently part of their commercial oﬀerings.\\nHowever, native techniques for generating novel videos more\\neﬃciently are a research frontier with fast-paced\\ndevelopments. One challenge in training native video\\ngeneration models is the diﬃculty of producing video-text pairs\\nthat can be semantically meaningful and eﬃcient to train.\\n1 2 \\n1 3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='To overcome this challenge, models such as CogVideo , by\\nTsinghua University (the ﬁrst open access text-to-video model)\\nand Make-a-Video, released by Meta AI in 2022, employ\\ntechniques to pair uncaptioned videos with text-to-image\\ndatasets and methods. Make-A-Video, in particular, overcomes\\nthe video-text pair data limitation by learning visual-text\\nunderstanding from image-text pairs and combining that\\nknowledge with motion understanding learned from\\nunsupervised and non-captioned videos. Therefore, the model\\ncan learn what things look like and how they move without\\nexplicit video-text pairing, enabling text-to-video and image-to-\\nvideo tasks.\\nThis technique applied on Stable Diﬀusion enabled further\\niterations on more eﬃcient text-to-video open models such as\\nModelScope Text-to-Video by Alibaba. Scaling up the size of\\nvideo datasets to train such models has also been achieved with\\nthe Stable Video Diﬀusion model , which trains a video latent\\ndiﬀusion model and pairs it with a Stable Diﬀusion text-to-\\nimage model. The image-to-video variant of Stable Video\\nDiﬀusion was released with open-weights and increased model\\nquality compared to Make-a-Video and ModelScope samples.\\nAdditional scaled-up approaches for video generation have also\\nbeen announced, starting with Sora, the super high quality,\\n1 3 \\n1 4'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='realistic and temporally coherent text-to-video model by\\nOpenAI. While the model is impressive, very few technical\\ndetails have been revealed in its technical report. In this same\\nspace, Google DeepMind unveiled Veo - its Sora competitor. Kuai\\nreleased Kling, the ﬁrst Sora-level model with public access and\\nRunwayML released Gen-3 a few weeks after.\\nOn the open-source and open-weights side, THUDM group in\\nTsinghua University released CogVideoX 2B and CogVideoX 5B,\\na family of open-weights models that can generate realistic and\\ntemporally coherent text-to-video on pair with the the close-\\nsource alternatives referenced above. There is also an active\\nproject on providing an open-source replication of Sora with\\nOpen-Sora, which displays a fast-paced progress.\\nMultimodality\\nIn the previous chapters, we explored using generative models\\nfor several diﬀerent modalities, such as text, image, and audio.\\nSome models we examined, like Stable Diﬀusion, have input\\nconditions in one modality (text) and generate outputs in\\nanother modality (images). However, models that take in a\\nsingle modality as input and produce their outputs in a single\\nmodality (here, we can also include Text to Speech and Speech'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='to Text models) are not typically what the community refers to\\nregarding multimodality and multimodal models.\\nMultimodality usually means that a single model can either\\nprocess inputs or yield outputs in more than one modality at\\nonce. Let’s go through some of the most recent advancements in\\nmultimodality.\\nCLIP (Contrastive Language-Image Pre-Training) is a model\\narchitecture introduced by OpenAI in 2021 trained on millions\\nof images and descriptive captions of those images. Once\\ntrained, the model can take in both image and text as inputs,\\nand these inputs are encoded to live on the same semantically\\nrelevant vector space. This characteristic allows the model to\\nperform a wide range of zero-shot tasks, such as image\\nclassiﬁcation, and semantically compare image-to-image or\\ntext-to-text. An introduction to CLIP was presented in Chapter 3.\\nWe also saw in Chapter 5 how the CLIP text-encoder is a\\ncomponent of the Stable Diﬀusion model.\\nBLIP (Bootstrapping Language-Image Pre-training for Uniﬁed\\nVision-Language Understanding and Generation) is a\\nframework and model architecture introduced by Salesforce in\\n2022 that, like CLIP, is trained on image-text pairs. However, the\\nmodel was trained to further decode the output into text. The\\nmultimodal input of either text, image, or both allows the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='model to perform zero-shot tasks such as image captioning and\\nvisual question-answering. Follow-up works such as BLIP-2\\nfurther reﬁned this concept by combining a frozen image\\nencoder and a large language model.\\nFigure 10-9. Figure 10.9. Example of BLIP captioning\\nVLMs (Visual Language Models), sometimes called VLLMs\\n(Visual Large Language Models), can take in both images and\\ntexts as inputs and provide text outputs. A seminal work in this\\nspace is the Flamingo paper  by DeepMind in 2022, but the\\nmodel was not released. Open-source replications such as\\nIdeﬁcs do exist. However, training visual language models from\\nscratch has proven costly. Approaches where an already\\nexisting pre-trained large language model (LLM) is further ﬁne-\\n1 5 \\n1 6'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='tuned to take in image outputs through a frozen (non-trainable)\\nimage encoder have proven eﬃcient and performant.\\nArchitectures like BLIP-2 pioneered such an approach, but they\\nstill had relatively narrow domain applications (captioning,\\nquestion-answering, etc.) and did not retain all the capabilities\\nof the LLMs. LLaVA, an architecture introduced by Microsoft\\nResearch in 2023, allows this vision encoder and LLM\\nconnection while retaining all its reasoning abilities. This\\napproach brought in an explosion of techniques and models in\\nthe open-source community, from tiny and eﬃcient models to\\nstate-of-the-art VLMs competitive with commercial models. To\\nkeep up with the space, one can follow the Open VLM\\nLeaderboard or the Image-Text-to-Text trending models on\\nHugging Face. To get hands-on experience in this area, you can\\nstart with VLM inference on Transformers. Commercial models\\nsuch as OpenAI’s GPT-4 or Anthropic’s Claude also have a VLM\\nvision component.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure 10-10. Figure 10-10. VLM interface.\\nUsing the same logic as VLMs, other modalities can be achieved\\nby leveraging a frozen encoder. For example, Gazelle is a joint\\nspeech-language model that leverages a pre-trained LLM\\n(Mistral 7B) and a frozen audio encoder (Wav2Vec2). The model\\ngets super-charged as it’s able to process and reason directly\\nfrom an audio input.\\nMultimodal output models are the next frontier. Research and\\ncommercial models in this space are advancing, and we expect\\na new wave of open multimodal output models soon. Some\\nfoundational research in this space is the Chameleon\\narchitecture, by Meta AI, which can take in text and image\\ninputs and also generate text and image outputs. It has zero-'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='shot capabilities for tasks such as image instruction following,\\nimage editing, image captioning, question answering, and\\nothers. Architectures for even broader multimodal input/output\\ncapabilities such as Uniﬁed-IO and Uniﬁed-IO 2 by Allen\\nInstitute for AI have been presented, but they have yet to be\\nscaled signiﬁcantly. As for the commercial models, OpenAI\\nreleased in May 2024 a multimodal GPT-4o model that can take\\nin image, text, and audio and output the same modalities of\\nimage, text, and audio.\\nCommunity\\nAs you’ve probably realized throughout the chapter and the\\nbook, the pace of the Machine Learning ecosystem is moving\\nvery fast. The best way to keep up with the latest research and\\ndevelopments is to be part of the community. There are many\\nways to do this, such as joining Discord servers or Reddit\\ncommunities, sharing your work with others, reading papers,\\nand following researchers and practitioners on X (formerly\\nTwitter). Surprisingly, we’ve moved from a world where all\\nresearch was happening in traditional labs to very impactful\\nresearch in decentralized setups. EleutherAI, Nous Research,\\nBigCode, and LAION are all examples of the latter, and\\nbecoming involved with their eﬀorts can be as simple as joining'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='their Discord servers. Many of these communities also hold\\npaper reading sessions, great async chat discussions, and\\nhackathons. Some of these communities were started by\\nindividual community contributors who began their journey by\\ntinkering with open models to scratch their own itch to solve\\nthe problems that mattered to them. Despite the high cost of\\ntraining big models from scratch, plenty of research and\\ndevelopment opportunities exist for anyone interested in the\\nﬁeld. There has never been a better time to get started than\\nnow.\\n Lee, Harrison, et al.\\xa0RLAIF: Scaling Reinforcement Learning from Human Feedback\\nwith AI Feedback. arXiv, 1 Sept 2023. arXiv.org, https://arxiv.org/abs/2309.00267\\n Rafailov, Rafel, et al.\\xa0Direct Preference Optimization: Your Language Model is Secretly\\na Reward Model. arXiv, 29 May 2023. arXiv.org, https://arxiv.org/abs/2305.18290\\n Gheshlaghi Azar, Mohammad, et al.\\xa0A General Theoretical Paradigm to Understand\\nLearning from Human Preferences. arXiv, 29 May 2023. arXiv.org,\\nhttps://arxiv.org/abs/2310.12036\\n Ethayarajh, Kawin, et al.\\xa0KTO: Model Alignment as Prospect Theoretic Optimization.\\narXiv, 2 Feb., 2024. arXiv.org, https://arxiv.org/abs/2402.01306\\n Black, Kevin, et al.\\xa0Training Diﬀusion Models with Reinforcement Learning. arXiv, 22\\nMay 2023. arXiv.org, https://arxiv.org/abs/2305.13301\\n1 \\n2 \\n3 \\n4 \\n5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Beltagy, Iz, et al.\\xa0Longformer: The Long-Document Transformer. arXiv, 10 Apr., 2020.\\narXiv.org, https://arxiv.org/abs/2004.05150\\n Su, Jianlin, et al.\\xa0RoFormer: Enhanced Transformer with Rotary Position Embedding.\\narXiv, 20 April 2021. arXiv.org, https://arxiv.org/abs/2104.09864\\n Munkhdalai, Tendsuren, et al.\\xa0Leave No Context Behind: Eﬃcient Inﬁnite Context\\nTransformers with Inﬁni-attention. arXiv, 10 April 2024. arXiv.org,\\nhttps://arxiv.org/abs/2404.07143\\n You can learn more about RWKV at https://github.com/BlinkDL/RWKV-LM?\\ntab=readme-ov-ﬁle\\n It is rumored that GPT-4 is also using an MoE architecture\\n The number of activated experts is a conﬁguration parameter that can be modiﬁed.\\n Esser, Patrick, et al.\\xa0Structure and Content-Guided Video Synthesis with Diﬀusion\\nModels. arXiv, 6 Feb., 2023. arXiv.org, https://arxiv.org/abs/2302.03011\\n Hong, Wenyi, et al._ CogVideo: Large-scale Pretraining for Text-to-Video Generation\\nvia Transformers_. arXiv, 29 May 2022. arXiv.org, https://arxiv.org/abs/2205.15868\\n Blattmann, Andreas, et al.\\xa0Stable Video Diﬀusion: Scaling Latent Video Diﬀusion\\nModels to Large Datasets. arXiv, 25 Nov., 2023. arXiv.org,\\nhttps://arxiv.org/abs/2311.15127\\n Li, Junnan, et al.\\xa0BLIP-2: Bootstrapping Language-Image Pre-training with Frozen\\nImage Encoders and Large Language Models. arXiv, 30 Jan.\\xa02023. arXiv.org,\\nhttps://arxiv.org/abs/2301.12597\\n6 \\n7 \\n8 \\n9 \\n 0 \\n 1 \\n 2 \\n 3 \\n 4 \\n 5'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Alayrac, Jean-Baptiste, et al.\\xa0Flamingo: a Visual Language Model for Few-Shot\\nLearning. arXiv, 29 Apr.\\xa02022. arXiv.org, https://arxiv.org/abs/2204.14198\\n 6'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Appendix A. Open-Source Tools\\nThis book wouldn’t have been possible without open source.\\nMost of the subjects we discussed and the majority of Machine\\nLearning research rely on open-source contributions. Not to\\nmention the production toolchain we used, with open-source\\nsoftware such as Jupyter Notebook, Quarto, nbdev, and many\\nmore.\\nIn this appendix, we will explore a variety of open-source tools\\nfor the Machine Learning practitioner. Some of these we’ve\\nused in the book, while others are good to know about. By\\nmaking yourself familiar with these tools, you’ll be well-\\nequipped to extend the applications and techniques you just\\nlearned.\\nThe Hugging Face Stack\\nThroughout this book, you already became familiar with the\\ncore libraries of the Hugging Face stack. The two main libraries\\nused were:\\nt r a n s f o r m e r s : The main library to train and run inference\\nwith transformers-based models across diﬀerent'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='modalities. It provides diﬀerent levels of abstraction, from\\nthe high-level pipeline and Trainer to supporting\\nrunning your own P y T o r c h  training loops.\\nd i ﬀ u s e r s : Similarly to t r a n s f o r m e r s , the d i ﬀ u s e r s  library\\nallows running pre-trained diﬀusion-based models.\\nAlthough it’s mostly known for its image generation\\ncapabilities, the library also supports audio, video, and 3D.\\nBoth libraries have a opinionated design that prioritizes\\nusability, simplicity and customizability. What does this mean\\nfor end users? First, both libraries aim to oﬀer consistent\\nspeciﬁcations across models. Whether you’re using Llama or\\nGemma, switching between them should ideally be a single-line\\ncode change. While both libraries have many features for fast\\ninference, models are always loaded with the highest precision\\nand lowest optimization by default. This ensures usability\\nacross diﬀerent platforms and avoids complex installations, but\\nalso means that models will be slower out-of-the-box unless\\noptimizations are conﬁgured.\\nTwo additional Hugging Face libraries are commonly used as\\nwell: * a c c e l e r a t e : allows running PyTorch code in distributed\\nsettings, both for training and inference. Whether you want to\\nrun the model on a GPU, multiple GPUs, a GPU with CPU\\noﬄoading, or entirely on a CPU, a c c e l e r a t e  abstracts away all'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='the complexities. It’s used under the hood by both t r a n s f o r m e r s \\nand d i ﬀ u s e r s , so most users don’t need to learn much about the\\na c c e l e r a t e  APIs. * p e f t : This library enables parameter eﬃcient\\nﬁne-tuning techniques to ﬁne-tune models with lower\\ncomputational and storage costs. It’s well integrated with\\nt r a n s f o r m e r s  and d i ﬀ u s e r s . Although the book mostly explores\\nLoRA, there are many other methods such as p-tuning, preﬁx\\ntuning, IA3, OFT, and DoRA.\\nData\\nd a t a s e t s : This book heavily relies on d a t a s e t s , a popular\\nlibrary for accessing, sharing, and processing open datasets\\nfor multiple modalities. Just as t r a n s f o r m e r s  and d i ﬀ u s e r s ,\\nd a t a s e t s  provides a consistent API, allowing users to easily\\nswap between diﬀerent datasets for a given modality.\\na r g i l l a : Argilla is a tool for building high-quality datasets. It\\nprovides a simple UI where humans can rate data, useful\\nfor tasks like comparing model generations (important for\\nRLHF), creating datasets for classical NLP tasks (e.g., entity\\nrecognition), or creating evaluation datasets.\\nd i s t i l a b e l : With the rise of synthetic data generation, new\\ntools like d i s t i l a b e l  have emerged. It allows creating\\npipelines to generate synthetic data.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Wrappers\\nAs the ecosystem has grown, diﬀerent community and research\\ntools have been built around t r a n s f o r m e r s :\\na x o l o t l : This tool streamlines model ﬁne-tuning: you just\\nneed to create a simple conﬁguration ﬁle to set up your\\nﬁne-tuning task. It supports common dataset formats and\\nmodel architectures.\\nu n s l o t h : Unsloth aims to provide extremely fast ﬁne-tuning\\non top of t r a n s f o r m e r s , using a FastLanguageModel class\\nthat incorporates optimized kernels.\\ns e n t e n c e - t r a n s f o r m e r s : As discussed in the Transformers\\nchapter, transformer models can also be used to compute\\nembeddings for a whole sentence, paragraph or document.\\ns e n t e n c e - t r a n s f o r m e r s  provides simple APIs to compute\\nembeddings with pre-trained models, or to ﬁne-tune your\\nown models.\\nt r l : With the rise of RLHF, t r l  provides a simple API for ﬁne-\\ntuning and aligning transformer and diﬀusion models.\\nChapter 6 showed how to do Supervised Fine-Tuning (SFT),\\nbut t r l  contains many other methods such as reward\\nmodeling and direct preference optimization (DPO).'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Local Inference\\nOne major advantage of open models is that you can run many\\nof them locally on your own hardware, oﬀering beneﬁts such as\\nprivacy, customizability, and local integrations, such as using a\\ncode model as a local IDE extension. Depending on your use\\ncase, diﬀerent tools can be used:\\nl l a m a .c p p : Allows doing LLM inference on a variety of\\nhardware. It supports diﬀerent quantization techniques\\n(from 1.5 bits to 8 bits) and has massive community\\nadoption. It’s usually used to either chat with a LLM locally\\nor to set up a local endpoint for use by another local\\nservice.\\nTransformers.js: Allows running models directly in the\\nbrowser without the need for a server. This can be very\\nuseful to easily deploy services with low latency and no\\ninference costs, or for privacy-ﬁrst use cases such as\\nwriting call transcriptions or subtitles in real-time.\\nDeployment Tools\\nWhile running inference for a single query can be simple,\\ndeploying a LLM in production is more complex. There are'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='many tools available for this, two popular ones are:\\nvLLM: A simple library for LLM serving that is ﬂexible and\\nwell integrated with popular models.\\nTGI: A production-ready toolkit for LLM deployment.\\nOther options include l m d e p l o y  and NVIDIA’s TensorRT-LLM.\\nWith so many alternatives, one might wonder which one to\\npick. Our suggestion is to explore them and ﬁnd out which one\\nbest ﬁts your use cases. All of them are in active development,\\nwith diﬀerent levels of model coverage, community adoption,\\nscalability, integration with cloud services and self-hosted\\nenvironments, etc.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Appendix B. LLM Memory\\nRequirements\\nModels come in all sizes! Llama 3.1, for example, was released\\nwith 8B, 70B, and 405B variants. To load and use an LLM, you\\nneed enough memory to store the model. The number of\\nparameters and their precision, among other factors, inﬂuence\\nthe memory requirements for an LLM.\\nWhat can you do if you do not have enough memory? * Reduce\\nthe precision of the model you are using. Rather than using\\nfloat16, you can use int8. * Use a smaller model. There are\\nmany high-quality small models. * Unload parts of the model that\\nyou are not using. This can be done with CPU RAM oﬄoading, a\\ncommon technique to reduce a model’s memory requirements at\\nthe cost of slower inference speeds. What happens if there is not\\nenough memory? We can then store the remaining model parts\\non the disk and load them as needed. Fortunately for us, the\\na c c e l e r a t e  library takes care of this and does +device_map=\\n\"auto\" +, which will automatically oﬄoad parts of the model as\\nneeded.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Inference Memory Requirements\\nYou can roughly estimate the memory requirements as follows:\\nGPUMemory needed=Number of Parameters×bytes per parameter\\nwhere the precision is the number of bytes used to store each\\nparameter. Without going into too much detail, the following table\\nshows the memory needed to load 2B, 8B, 70B, and 405B models\\nusing diﬀerent data types (ﬂoat32, ﬂoat16, int8, int4, and int2):\\nModel ﬂoat32 ﬂoat16 int8 int4\\n2B 8 GB 4 GB 2 GB 1 GB\\n8B 32 GB 16 GB 8 GB 4 GB\\n70B 280 GB 140 GB 70 GB 35 GB\\n405B 1.62 TB 810 GB 405 GB 202.5 G\\nTable 12-1. Inference memory requirements for diﬀerent models\\nand data types.\\nFor reference, an H100 has 80GB of memory, so loading Llama 3.1\\n405B would require at least a full node (of 8 H100s) to load the\\nmodel in 8-bit integers. This is a very rough estimate for loading'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='the model. You also need to consider the memory required for the\\ninput and output tensors and the memory needed for the\\nintermediate computations. For example, long sequences require\\nmore memory than short sequences, especially as we go to over\\n100 thousand tokens.\\nAt the moment of writing, we can quantize models to int8 with\\nminimal loss in performance. Techniques that go lower than 8\\nbits per parameter come with performance degradation and are\\nan active area of research.\\nTraining Memory Requirements\\nCalculating training requirements can become trickier as they\\ndepend on the implementation details of the model and training\\nscript. The memory requirements can also signiﬁcantly change\\ndepending on the batch size, the number of tokens in the dataset\\nsamples, the training technique (e.g., full ﬁne-tuning vs.\\xa0PEFT),\\nand the training parallelism setup.\\nThe details of the memory requirements of training are out of the\\nscope of this book. However, we can provide some rough\\nestimates for the memory requirements of training LLMs. The\\nfollowing table shows rought GPU requirements for ﬁne-tuning\\nLlama:'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Model Full Fine-Tuning LoRA QLoRA\\n8B 60 GB 16 GB 6B GB\\n70B 500 GB 160 GB 48 GB\\n405B 3.25 TB 950 GB 250 GB\\nTable 12-2. Training memory requirements for diﬀerent models\\nand training techniques.\\nFurther Reading\\nIf you’re interested in learning more about the memory\\nrequirements of LLMs, you can check the following resources:\\nTransformer Math 101 by EleutherAI, which explains in detail\\nthe memory requirements for training a model in diﬀerent\\nsetups.\\nBreaking down GPU VRAM consumption is a short blog post\\nthat explains the diﬀerent components that consume GPU\\nmemory.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Appendix C. End-to-End Retrieval-\\nAugmented Generation\\nA popular application of LLMs is using them for content\\ngeneration based on both input prompts and externally\\nretrieved information. In this appendix, we will demonstrate\\nhow to build a pipeline that leverages a pre-trained LLM and a\\npre-trained sentence transformer to generate content based on\\nuser input and a set of documents. We’ve explored the building\\nblocks for this throughout the book. Chapter 2 discussed text\\ngeneration with LLMs and how to use sentence transformers\\nfor encoding text. Chapter 6 also contained a project where we\\nbuilt a minimal RAG pipeline.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Figure C-1. Figure 13-1. A RAG pipeline\\nLet’s discuss the components of a RAG system:\\n1. The user inputs a question.\\n2. The pipeline retrieves the most similar documents to the\\nquestion.\\n3. The pipeline passes both the question and the retrieved\\ndocuments to the LLM.\\n4. The pipeline generates a response.\\nProcessing the Data\\nAs with any ML project, the ﬁrst step is loading and processing\\nthe data. We’ll keep it simple by focusing on a single topic.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Imagine we want our model to generate content related to the\\nEuropean Union AI Act, which is unlikely to be part of the LLM’s\\ntraining data because the model we’ll use was trained before\\nwork on the AI Act started. First, we’ll load the document.\\nimport urllib.request\\n# Define the file name and URL\\nfile_name = \"The-AI-Act.pdf\"\\nurl = \\n\"https://artificialintelligenceact.eu/wp-\\ncontent/uploads/2021/08/The-AI-Act.pdf\"\\n# Download the file\\nurllib.request.urlretrieve(url, file_name)\\nprint(f\"{file_name} downloaded \\nsuccessfully.\")\\nThe-AI-Act.pdf downloaded successfully.\\nThe document is likely too long to be processed in one go, so\\nwe’ll split it into smaller chunks and embed each chunk\\nseparately. Each chunk will be a separate d o c u m e n t  we’ll\\ncompare against the user input. For simplicity, we’ll use some\\npre-processing tools from l a n g c h a i n , a library that provides'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='utility functions to create RAG systems. For example, it has a\\nhandy PyPDFLoader class  that extracts text from PDFs and\\nhandles chunking.\\nFirst, install the necessary dependencies:\\n!pip install langchain_community pypdf \\nlangchain-text-splitters\\nNow, let’s load and preprocess the document using\\nPyPDFLoader:\\nfrom langchain_community.document_loaders \\nimport PyPDFLoader\\nloader = PyPDFLoader(file_name)\\ndocs = loader.load()\\nprint(len(docs))\\n108\\nPyPDFLoader splits the PDF into one document per page,\\nwhich leads to 108 documents in this case. We’ll split them into\\neven smaller chunks. LangChain provides classes that help with\\ndiﬀerent types of text splitting. We’ll use\\n1'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='RecursiveCharacterTextSplitter, which has two key\\nparameters:\\nchunk_size: The number of characters in each chunk. In\\ngeneral, it’s a good idea to connect this with the maximum\\nnumber of tokens the embedding model can handle, which\\nis low for most sentence transformers. Otherwise, you risk\\nhaving part of the document being truncated.\\nchunk_overlap: The number of characters each chunk\\noverlaps with the previous one. This is useful to avoid\\nsplitting sentences in the middle. We’ll arbitrarily set it to\\n100 characters (a ﬁfth of the chunk size we chose).\\nfrom langchain_text_splitters import \\nRecursiveCharacterTextSplitter\\ntext_splitter = \\nRecursiveCharacterTextSplitter(\\n    chunk_size=500, chunk_overlap=100\\n)\\nchunks = text_splitter.split_documents(docs)\\nprint(len(chunks))\\n854'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"Let’s save the text chunks to an array:\\nchunked_text = [chunk.page_content for chunk \\nin chunks]\\nThis is what one of the chunks looks like:\\nchunked_text[404]\\n('user or for own use on the Union market for \\nits intended '\\n 'purpose;  \\\\n'\\n '(12) ‘intended purpose’ means the use for \\nwhich an AI system is '\\n 'intended by the provider, \\\\n'\\n 'including the specific context and \\nconditions of use,  as '\\n 'specified in the information \\\\n'\\n 'supplied by the provider in the \\ninstructions for use, promotional '\\n 'or sales materials \\\\n'\\n 'and statements, as well as in the technical \\ndocumentation;  \\\\n'\\n '(13) ‘reasonably foreseeable misuse’ means \\nthe use of an AI system '\\n 'in a way tha t is not in')\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Embedding the Documents\\nNow that we have the documents (our chunks), we need to\\ncreate their embeddings. We’ll be using a sentence transformer\\nmodel as a r e t r i e v e r , which acts like a search engine to ﬁnd the\\nmost relevant snippets to a given question. This process relies\\non computing the similarity between the embeddings of the\\nuser query and the embeddings of the documents in our\\ncollection. To pre-compute all the document embeddings we’ll\\nuse a pre-trained sentence transformer model, using the\\nexample from the exercises section of Chapter 2. The following\\nsnippet loads a pre-trained sentence transformer model,\\nBAAI/bge-small-en-v1.5, and uses it to encode two\\nsentences.\\nfrom sentence_transformers import \\nSentenceTransformer, util\\nsentences = [\"I\\'m happy\", \"I\\'m full of \\nhappiness\"]\\nmodel = SentenceTransformer(\"BAAI/bge-small-\\nen-v1.5\")\\n# Compute embedding for both sentences\\nembedding_1 = model.encode(sentences[0],'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"convert_to_tensor=True)\\nembedding_2 = model.encode(sentences[1], \\nconvert_to_tensor=True)\\nSentence transformers return a single embedding for the whole\\nsentence. Although transformers models usually output one\\nembedding per token, sentence transformers are trained to pool\\nthe token embeddings into a single sentence embedding that\\ncaptures the semantic meaning of the text.\\nembedding_1.shape\\ntorch.Size([384])\\nYou can then compare the documents based on the cosine\\nsimilarity:\\nutil.pytorch_cos_sim(embedding_1, \\nembedding_2)\\ntensor([[0.8367]], device='cuda:0')\\nThe cosine similarity, as we saw in Chapter 3, is just the dot\\nproduct of the two embedding vectors:\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"embedding_1 @ embedding_2\\ntensor(0.8367, device='cuda:0')\\n: \\n\\\\{.cell 0=e 1=c 2=h 3=o 4=: 5=o 6=f 7=f 8=’ ’ 9=o\\n10=u 11=t 12=p 13=u 14=t 15=: 16=o 17=f 18=f}\\n# alternative\\nimport torch\\ntorch.dot(embedding_1, embedding_2)\\ntensor(0.8367, device='cuda:0')\\n: \\nNow that we know how to embed a sentence, let’s embed all\\nthe documents:\\nchunk_embeddings = model.encode(chunked_text, \\nconvert_to_tensor=True)\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='This returns a 384-dimensional embedding vector for each\\nchunk:\\nchunk_embeddings.shape\\ntorch.Size([854, 384])\\nRetrieval\\nWith the embedded documents, we can retrieve the most\\nrelevant ones to a given question. We’ll use the same approach\\nas before to calculate the cosine similarity between the question\\nand each document.  Fortunately, the similarity computation\\ndoes not require iteration: it can be performed very eﬃciently\\nusing the built-in PyTorch matrix multiplication primitives.\\ndef search_documents(query, top_k=5):\\n    # Encode the query into a vector\\n    query_embedding = model.encode(query, \\nconvert_to_tensor=True)\\n    # Calculate cosine similarity between the \\nquery and all document chunks\\n    similarities = \\n2'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='util.pytorch_cos_sim(query_embedding, \\nchunk_embeddings)\\n    # Get the top k most similar chunks\\n    top_k_indices = \\nsimilarities[0].topk(top_k).indices\\n    # Retrieve the corresponding document \\nchunks\\n    results = [chunked_text[i] for i in \\ntop_k_indices]\\n    return results\\nLet’s try an example. We’ll truncate the output in the book to\\nkeep it short, but you can run the code in your local\\nenvironment to see the full output.\\nsearch_documents(\"What are prohibited ai \\npractices?\", top_k=2)\\n(\\'TITLE  II \\\\n\\'\\n \\'PROHIBITED  ARTIFICIAL  INTELLIGENCE  \\nPRACTICES  \\\\n\\'\\n \\'Article 5  \\\\n\\'\\n \\'1. The following artificial intelligence'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"practices shall be '\\n 'prohibited:  \\\\n'\\n '(a) the placing on the market, putting into \\nservice o')\\n('low or minimal risk. The list of prohibited \\npractices in Title II '\\n 'comprises all those AI systems \\\\n'\\n 'whose use is considered unacceptable as \\ncontravening Unio n '\\n 'values, for instance by violating \\\\n'\\n 'fundame')\\nThe model correctly retrieves relevant information from the\\ninput question.\\nGeneration\\nThe next step is to generate a response based on the question\\nand the retrieved documents. Let’s use our good old friend, the\\ninstruct version of SmolLM. Feel free to experiment with other\\nmodels.\\nfrom transformers import pipeline\\nfrom genaibook.core import get_device\\n3\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='device = get_device()\\ngenerator = pipeline(\\n    \"text-generation\", \\nmodel=\"HuggingFaceTB/SmolLM-135M-Instruct\", \\ndevice=device\\n)\\nWe’ll use an instruct model with a chat template. As discussed\\nin Chapter 6, t r a n s f o r m e r s  has utilities that format the prompt\\nto meet the model expectations. We’ll want to add the retrieved\\ndocuments to the prompt in the RAG case.\\ndef generate_answer(query):\\n    # Retrieve relevant chunks\\n    context_chunks = search_documents(query, \\ntop_k=2)\\n    # Combine the chunks into a single \\ncontext string\\n    context = \"\\\\n\".join(context_chunks)\\n    # Generate a response using the context\\n    prompt = \\nf\"Context:\\\\n{context}\\\\n\\\\nQuestion: \\n{query}\\\\nAnswer:\"\\n    # Define the context to be passed to the'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='model\\n    system_prompt = (\\n        \"You are a friendly assistant that \\nanswers questions about the AI Act. \"\\n        \"If the user is not making a \\nquestion, you can ask for clarification\"\\n    )\\n    messages = [\\n        {\"role\": \"system\", \"content\": \\nsystem_prompt},\\n        {\"role\": \"user\", \"content\": prompt},\\n    ]\\n    response = generator(messages, \\nmax_new_tokens=300)\\n    return response[0][\"generated_text\"][2]\\n[\"content\"]\\nLet’s try one example:\\nanswer = generate_answer(\"What are prohibited \\nai practices in the EU act?\")\\nprint(answer)'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"('The EU Act prohibits the use of artificial \\nintelligence practices '\\n 'that are harmful to individuals, such \\nas:\\\\n'\\n '\\\\n'\\n '* The placing on the market, putting into \\nservice or use of an A I '\\n 'system that is subliminal, that is, it is \\nnot intended to be used '\\n 'for any purpose other than to deceive or \\nmanipulate individuals.\\\\n'\\n '* The use of A I systems that are designed \\nto deceive or '\\n 'manipulate individuals, such as those used \\nin advertising, '\\n 'marketing, or customer service.\\\\n')\\nThe model generates a response based on the input question\\nand correctly retrieves information. We’re using a tiny\\ngenerative model, so scaling to a larger model can help us\\nobtain higher-quality generations and grow the context length,\\nincreasing the number of retrieved documents we can pass to\\nthe context.\"),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Production-level RAG\\nThe code we’ve shown is a simple example of a RAG system. In\\na production-level system, you’d need to consider several\\nadditional factors:\\nChunking: One of the challenges with real data is that the\\ndocuments can be very long. Finding the right chunk size is\\na design decision that depends on the data and the model:\\ntoo small snippets will truncate ideas, and too large will\\ndilute them. You can learn more about splitting at 5 Levels\\nOf Text Splitting.\\nSmaller Embeddings: Large embeddings can be memory-\\nintensive. Active research is being conducted to make\\nembeddings smaller while maintaining their quality. If you\\nare interested in these topics, we recommend reading\\nabout Matryoshhka Embedding Models and Embeddings\\nQuantization.\\nRe-ranking: The retrieval step is crucial in RAG systems.\\nOur retrieval models are fast and essential for comparing\\nthousands or millions of documents, but they are not\\nnecessarily the most accurate. Once we retrieve the top_k\\ndocuments, we can re-rank them using a slower but higher-\\nquality model. You can learn more about re-ranking in'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='sentence_transformers documentation or in the Deep Dive\\ninto Cross-encoders and Re-ranking.\\nEmbedding Model Evaluation: There are dozens of\\nsentence transformer models. To choose the best one for\\nyour use case, we suggest checking out the Massive Text\\nEmbedding Benchmark, which has information such as\\nmodel size, embedding dimensions, and quality across\\ndozens of tasks. Note that we usually want very small and\\nfast models for retrieval tasks, so that should be a key\\nfactor in your decision.\\nProduction Components: In real-world usage, you might\\nwant to integrate other components such as query\\nrewriting, PII redaction, caching, and input guardrails to\\navoid your model being used inappropriately. Chip Huyen\\nhas an excellent post, Building a Generative AI Platform, to\\nlearn more about this.\\nFinally, there are many open-source tools for building RAG\\nSystems. Here are some recommendations to check out:\\nColBERT: a fast and accurate BERT-based retrieval model.\\nRAGatouille: a system to use and train retrieval models.\\nThere are diﬀerent open vector databases, such as Milvus,\\nWaviate and Qdrant, which may prove useful when you\\nhave to work with massive datasets. Diving into vector'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='databases is outside the scope of this book, but they are also\\na quickly growing ﬁeld. You can see a comparison from\\n2023 in the Picking a vector database blog post.\\n Learn more about it in\\nh t t p s : / / p y t h o n . l a n g c h a i n . c o m / v 0 . 1 / d o c s / m o d u l e s / d a t a _ c o n n e c t i o n / d o c u m e n t _ l o a d e r s / p d \\nf / \\n You can also use s e n t e n c e _ t r a n s f o r m e r s  convenient semantic_search method for\\nthis use case.\\n You can learn more about how this model was trained in\\nh t t p s : / / h u g g i n g f a c e . c o / b l o g / s m o l l m \\n1 \\n2 \\n3'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='About the Authors\\nOmar Sanseviero is the Lead of Developer Advocacy\\nEngineering at Hugging Face, where he builds collaborations\\nwith diﬀerent libraries in the ML Ecosystem. Omar has\\nextensive engineering experience working in Google in Google\\nAssistant and TensorFlow Graphics. Omarâ \\x80 \\x99 s work at Hugging\\nFace is at the intersection of community, engineering, and\\nproduct, allowing him to have a horizontal understanding of\\nthe ML ecosystem and trends.\\nPedro Cuenca is a Machine Learning Engineer at Hugging Face\\nworking on diﬀusion software, models, and applications. He\\nhas 20+ years of software development experience in ﬁelds like\\nInternet applications (in Spain, he helped create the ﬁrst\\ninteractive educational portal, the ﬁrst book store, and the ﬁrst\\nfree ISP) and, more recently, iOS. As a co-founder and CTO of\\nLateNiteSoft, he worked on the technology behind Camera+, a\\nsuccessful iPhone photography app. He created deep-learning\\nmodels for tasks such as photography enhancement and super-\\nresolution. He was also involved in the development and\\noperations behind dalle-mini. He brings a practical vision of\\nintegrating AI research into real-world services and the\\nchallenges and optimizations involved.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='Apolinario Passos is a Machine Learning Art Engineer at\\nHugging Face working across diﬀerent teams on multiple\\nmachine learning for art and creativity use-cases. Apolinario\\nhas 10+ years of professional and artistic experience,\\nalternating between holding art exhibitions, coding, and\\nproduct management, having been a Head of Product in World\\nData Lab. Apolinario aims to ensure that the ML ecosystem\\nsupports and makes sense for artistic use cases.\\nJonathan Whitaker is a data scientist and deep learning\\nresearcher focused on generative modelling. He created and\\ntaught the AIAIART course and is working on a new version\\ncalled The Generative Landscape, which covers many of the\\ntopics this book hopes to address. He also wrote the Hugging\\nFace diﬀusion models class and is working with Jeremy Howard\\non the ongoing FastAI course Stable Diﬀusion from the\\nFoundations. Jonathan also works as a consultant, currently\\npart-time as a Builder-In-Residence with PlaygroundAI.'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='C o l o p h o n \\nThe animal on the cover of Hands-On Generative AI with\\nTransformers and Diﬀusion Models is the giant African\\nswallowtail butterﬂy (Papilio antimachus).\\nThe giant African swallowtail is one of the largest species of\\nbutterﬂy, with a wingspan of up to 9–10 inches (around the size\\nof, say, a dinner plate or vinyl record). Yet, for all of their\\nimpressive size, relatively little is known about these colossal\\ninsects.\\nFirst discovered in 1782, swallowtails live in the tropical\\nrainforests of west and central Africa, where they spend most of\\ntheir time in the forest canopy; males have occasionally been\\nobserved mud-puddling at the forest ﬂoor, a behavior in which\\nbutterﬂies aggregate on wet organic matter such as soil or dung\\nin a quest for nutrients.\\nDue to their diet, giant African swallowtails are highly toxic,\\nand they have no known predators. Though there have been\\nsome reports of a population decline resulting from habitat\\ndestruction and poaching (specimens are highly prized and can\\nfetch prices of over $1,000), the IUCN has listed the giant\\nAfrican swallowtail as Data Deﬁcient: more information is'),\n",
       " Document(metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content='needed in order for a conservation assessment to be made.\\nMany of the animals on O’Reilly covers are endangered; all of\\nthem are important to the world.\\nThe cover illustration is by Karen Montgomery. The series\\ndesign is by Edie Freedman, Ellie Volckhausen, and Karen\\nMontgomery. The cover fonts are Gilroy Semibold and Guardian\\nSans. The text font is Adobe Minion Pro; the heading font is\\nAdobe Myriad Condensed; and the code font is Dalton Maag’s\\nUbuntu Mono.'),\n",
       " Document(metadata={'source': 'data\\\\Hybrid_Deep_Learning_Model_Based_on_GAN_and_RESNET_for_Detecting_Fake_Faces (1).pdf'}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/381571903\\nHybrid Deep Learning Model Based on GAN and RESNET for Detecting Fake\\nFaces\\nArticle\\xa0\\xa0in \\xa0\\xa0IEEE Access · January 2024\\nDOI: 10.1109/ACCESS.2024.3416910\\nCITATIONS\\n11\\nREADS\\n509\\n4 authors, including:\\nSoha Safwat\\n17 PUBLICATIONS\\xa0\\xa0\\xa0103 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAyat Mahmoud Marzouk\\nاﻟﻤﺒﻜﺮة  ﻟﻠﻄﻔﻮﻟﺔ  اﻟﺘﺮﺑﻴﺔ  ﻛﻠﻴﺔ \\n4 PUBLICATIONS\\xa0\\xa0\\xa022 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nIbrahim Eldesoky\\nBeni-Suef University\\n20 PUBLICATIONS\\xa0\\xa0\\xa0184 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Ibrahim Eldesoky on 26 June 2024.\\nThe user has requested enhancement of the downloaded file.'),\n",
       " Document(metadata={'source': 'data\\\\Hybrid_Deep_Learning_Model_Based_on_GAN_and_RESNET_for_Detecting_Fake_Faces (1).pdf'}, page_content='Received 10 May 2024, accepted 17 June 2024, date of publication 19 June 2024, date of current version 26 June 2024.\\nDigital Object Identifier 10.1 109/ACCESS.2024.3416910\\nHybrid Deep Learning Model Based on GAN\\nand RESNET for Detecting Fake Faces\\nSOHA SAFWAT1, AYAT MAHMOUD2, IBRAHIM ELDESOUKY FATTOH\\n3, AND FARID ALI\\n4\\n1Software Engineering and Information Technology Department, Faculty of Engineering and Technology, The Egyptian Chinese University, Cairo 4541312, Egypt\\n2Department of Computer Science, Faculty of Computer Science, MSA University, Cairo 3750311, Egypt\\n3Department of Computer Science, Faculty of Computers and Artificial Intelligence, Beni-Suef University, Beni Suef 2722165, Egypt\\n4Department of Information Technology, Faculty of Computers and Artificial Intelligence, Beni-Suef University, Beni Suef 2722165, Egypt\\nCorresponding author: Farid Ali (fared.ali@fcis.bsu.edu.eg)\\nABSTRACT While human brains have the ability to distinguish face characteristics, the use of advanced\\ntechnology and artificial intelligence blurs the difference between actual and modified images. The evolution\\nof digital editing applications has led to the fabrication of very lifelike false faces, making it harder for\\nhumans to discriminate between real and made ones. Because of this, techniques like deep learning are\\nbeing used increasingly to distinguish between real and artificial faces,producing more consistent and\\naccurate results. In order to detect fraudulent faces, This paper introduces a pioneering hybrid deep learning\\nmodel, which merges the capabilities of Generative Adversarial Networks (GANs) and the Residual Neural\\nNetwork (RESNET) architecture, aimed at detecting fake faces. By integrating GANs’ generative strength\\nwith RESNET’s discriminative abilities, the proposed model offers a novel approach to discerning real\\nfrom artificial faces. Through a comparative analysis, the performance of the hybrid model is evaluated\\nagainst established pre-trained models such as VGG16 and RESNET 50. Results demonstrate the superior\\neffectiveness of the hybrid model in accurately detecting fake faces, marking a notable advancement in\\nfacial image recognition and authentication. The findings on a benchmark dataset show that the proposed\\nmodel obtains outstanding performance measures, including precision 0.79, recall 0.88, F1-score 0.83,\\naccuracy 0.83, and ROC AUC Score 0.825. The study’s conclusions highlight the hybrid model’s strong\\nperformance in identifying fake faces, especially when it comes to accuracy, precision, and memory\\neconomy. By combining the generative capacity of GANs with the discriminative capabilities of RESNET,\\nthis solves the problems caused by more complex fake face generation approaches.With significant potential\\nfor use in identity verification, social media content moderation, cybersecurity, and other areas, the study\\nseeks to advance the field of false face identification. In these situations, being able to accurately discriminate\\nbetween real and altered faces is crucial. Notably, our suggested model adds Channel-Wise Attention\\nMechanisms to RESNET50 at the feature extraction phase, which increases its effectiveness and boosts\\nits overall performance.\\nINDEX TERMS RESNET, generative adversarial networks, deep learning, real and fake faces, face\\ndetection, channel-wise attention.\\nI. INTRODUCTION\\nImages and movies with fake facial expressions produced\\nthrough digital modification techniques have recently drawn\\nThe associate editor coordinating the review of this manuscript and\\napproving it for publication was Anandakumar Haldorai\\n.\\nincreasing public criticism [1]. Deepfake is a term for\\nartificial intelligence-produced, realistic-sounding, but fake,\\nvisuals, audio, and videos [2]. Deepfake is now more realistic\\nand simpler to create because of recent improvements in\\ndeepfake generation. Deepfake has posed serious threat\\nto society, and our right to privacy, necessitating the\\nVOLUME 12, 2024\\n\\n 2024 The Authors. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 86391'),\n",
       " Document(metadata={'source': 'data\\\\Hybrid_Deep_Learning_Model_Based_on_GAN_and_RESNET_for_Detecting_Fake_Faces (1).pdf'}, page_content='S. Safwat et al.: Hybrid Deep Learning Model Based on GAN and RESNET for Detecting Fake Faces\\ndevelopment of deepfake detection techniques to counter\\nthese concerns [3], [4]. An individual known as Deepfakes [5]\\nused publicly accessible artificial intelligence application to\\nproduce pornographic videos in December 2017 in which real\\nfaces were replaced with fake faces in photos and videos.\\nDeepfakes is a user of the Reddit social media network [6].\\nThe substitution of an individual’s appearance, especially\\nfaces, using artificial intelligence algorithms is known as\\n‘‘Deepfaking’’. A particular type of synthetic media known\\nas ‘‘deepfake’’ employs deep learning-based software to\\nproduce deceptive films, recordings, and/or photos. It entails\\nswapping out one person’s face in a photo or video with\\nanother person’s likeness to produce a realistic imitation with\\nthe aim of deceiving viewers or altering content’s genuine\\nmessage [7]. The majority of deepfake detection techniques\\nrely on features and machine learning techniques. Deepfake\\ngeneration advances, a dearth of high-quality datasets, and a\\nlack of benchmarks are some of the remaining difficulties in\\ndeepfake detection. Deepfake detection trends for the future\\nmay include robust, efficient, and systematic detection tech-\\nniques as well as high-quality datasets [8]. GANs technology\\nhas made it possible to produce extremely lifelike face images\\nthat are visually challenging to differentiate real faces [9].\\nThe generation process and discriminator, which are the\\ntwo parts of a Generative Adversarial Network, collaborate\\nto produce untrue photos which might be challenging to\\ndifferentiate from real photos. As the discriminator is trained\\nto distinguish between fake photos and real photos, the\\ngenerator produces the fake pictures [10]. The generator tries\\nto create more convincing photos with the aim of tricking\\nthe discriminator throughout training process, whereas the\\ndiscriminator gets better at spotting untrue images. GANs\\nare utilized for creating images of individuals, animals, and\\nobjects, but they may also be used to create fraudulent images\\nfor malicious purposes [11]. What is worse, humans struggle\\nto recognize these convincing deep fake images, audios,\\nand films. Therefore, it is crucial, imperative, and necessary\\nto differentiate true media from deepfakes. Therefore, it is\\nessential to create a reliable model that can precisely\\ndifferentiate between real and fake photos. Due to the recent\\nspike in the risk of fraudulent operations, numerous methods\\nto identify phony face photos have been developed to solve\\nthis issue [12]. These techniques can be roughly divided\\ninto two groups: one group relies on manually created\\ncharacteristics and depends on the statistical properties of the\\nphotos. The other group makes use of deep learning methods\\nthat utilize cutting-edge neural networks to find patterns and\\ncharacteristics in the photos [13]. This paper is organized in\\nsix main sections. Section I states the research challenge, and\\nemphasizes the importance of the subject and the goals of\\nthe investigation. An overview of the pertinent background\\ninformation and associated studies is provided in Section II.\\nThe materials and methods used are presented in Section III.\\nThe suggested model is presented in Section IV, together with\\ninformation on its architecture, design, and implementation.\\nThe implementation results and their discussion are presented\\nin Section V. Conclusions and key contributions to the field\\nand the directions for future work are outlined in Section VI.\\nII. BACKGROUND AND RELATED WORK\\nThe deployment of realistic Deepfake images could be\\ndangerous for people’s privacy, democratic processes, and the\\nnation’s security [14]. The creation of trustworthy tools for\\nspotting hazardous Deepfake material is essential. Machine\\nlearning methods and feature-based ones make up the two\\nprimary types of Deepfakes detection techniques [6]. To dis-\\ntinguish between deepfakes, machine learning methods,\\nparticularly deep learning, are frequently used. Feature-based\\nalgorithms exploit specific properties found in Deepfake\\nmedia to identify them. As there is a critical need to stop\\nthe spread of damaging media, this study concentrates on\\nmachine learning methods to identify deepfakes. Machine\\nlearning methods are divided into two primary categories:\\nstandard techniques and deep techniques [6]. Traditional\\nmachine learning techniques involve strategies to analyze\\ndata along with producing predictions or classes depending\\non statistical models and algorithms [12]. It is used in\\nSVM and RF-based Deepfake detection techniques. Based\\non statistical models, these methods seek to analyze the data\\nand produce predictions or classes (groups). Traditional ML\\nfrequently necessitates hand-engineering features. However,\\ndue to their speed, ease of use, and robustness against\\nnoisy datasets, these techniques are still often used in\\nnumerous applications. Support Vector Machine (SVM) is\\na machine learning technique used for regression analysis\\nand categorization. SVM can be used in Deepfake detection\\nto discriminate between genuine and fake content. SVM\\nmay be trained using a dataset of actual and Deepfake\\nphotos and videos [7] for Deepfake identification, where it\\nlearns to differentiate between the two classes. Once taught,\\nit can be used to determine the category of upcoming,\\nundiscovered movies or photographs. To identify more than\\ntwo classes of Deepfakes, several SVMs would need to be\\ntrained, which is one of the key drawbacks of this method.\\nHowever, because SVM is a binary classifier which means\\nit operates or differentiate between only two classes [15].\\nA machine learning approach called random forest (RF) can\\nbe used for classification, regression, and other applications.\\nRandom forest is used as a classifier in deep fake detection\\nto differentiate between real and fraudulent content. Since it\\ncan handle an enormous number of characteristics and can\\ndetermine which characteristic are considered more crucial\\nfor classification, random forest may serve as a beneficial\\nmethod in deep fake detection. Furthermore, compared to\\nother classifiers, it is less susceptible to overfitting, which\\nmakes it more resistant to noisy or defective data [16]\\nDeepFaceLab (2019) [17] is software application used to\\nmanipulate facial images. A Russian smartphone application\\nnamed FaceApp, for instance, has the capability to generate\\ndeceptive photographs that appear older than the subjects\\nactually are. A piece of software called Deepfakes can be used\\nto swap out a human face with that of any other person or\\n86392 VOLUME 12, 2024'),\n",
       " Document(metadata={'source': 'data\\\\Hybrid_Deep_Learning_Model_Based_on_GAN_and_RESNET_for_Detecting_Fake_Faces (1).pdf'}, page_content='S. Safwat et al.: Hybrid Deep Learning Model Based on GAN and RESNET for Detecting Fake Faces\\nanimal. With the aid of machine learning and human image\\nsynthesis, DeepFaceLab is a Windows program that lets users\\nreplace faces in videos [18]. The article investigates how\\nundiscovered medical deepfakes might affect patient safety\\nas well as the assets of hospitals. To create techniques for\\nidentifying such attacks, the researchers carried out a case\\nstudy. Support Vector Machine, Random Forest, and Decision\\nTree were among the eight machine learning algorithms\\nthat were put to the test [19]. Deep learning techniques,\\nas opposed to traditional machine learning models, can\\ndiscover Deepfake properties and have grown to be a\\npopular way for identifying Deepfakes. These techniques\\ninclude GAN, CNN, and RNN as examples. Furthermore,\\ncompared to other techniques, deep learning-based detection\\nalgorithms typically produce higher levels of accuracy [13].\\nConvolutional neural networks (CNNs), recurrent neural\\nnetworks (RNNs), and long short-term memory (LSTM)\\nnetworks are only a few of the deep learning methods that are\\npresented in the article, cited in [7] for various applications.\\nBy identifying genuine from false photos, these techniques\\ncan be utilized to identify Deepfakes. Below is an overview\\nof how various techniques can be used to identify Deepfake\\ncontent. A deep neural network model called the CNN\\ncomprises some hidden layers, an input layer, and output\\nlayer. The hidden layers take inputs from top layer and\\nconvolution the input values. The matrix multiplication or dot\\nproduct is used in this convolution procedure. Then, further\\ntransformations like pooling layers are used together with\\na nonlinearity activation function like the Rectified Linear\\nUnit (RELU). By computing the outputs using functions like\\nmaximum pooling or average pooling, pooling layers seek\\nto reduce the complexity of the input data [20]. Multiple\\nlayers make up ANNs, involving one input layer, some hidden\\nlayers, and one output layer.Input data sets are utilized as\\ninputs in Artificial Neural Networks, which the network\\nendeavors to classify. Signal spread occurs via connections,\\nknown as edges, between the interconnected points or\\nsynthetic neurons in ANNs, which has an architecture like\\nthat of the human brain. After processing the signals, each\\nneuron sends the signals received to the neurons connected\\nto it. An edge and neuron-related weight is used to modify\\nthe intensity of the signal at a link [16]. Therefore, it is\\ncrucial to understand not only the deep learning methods\\nstated before, but also the traditional neural network (NN) and\\nhow it relates to traditional machine learning. The traditional\\nNN is a popular variety of neural network that is used in\\ntasks involving supervised learning like classification and\\nregression. Traditional neural network (NN) is made up of\\nsome hidden layers, one input layer, and one output layer. The\\nhidden layers contain nodes which calculate weighted inputs\\nand provide an output. Artificial Neural Networks (ANNs)\\nare based on the core principle that the human brain functions\\nin a similar manner.\\nThe Deep InceptionNet Learning Algorithm Introduced\\nby [21], is used to detect deepfake images. The study\\nachieves a noteworthy accuracy of 93% when compared to\\nother convolutional networks, demonstrating the algorithm’s\\neffectiveness in differentiating between true and altered\\ncontent.\\nIn [22], the author reviews the literature on several\\ndeep learning strategies for identifying created fake faces.\\nThe author highlights the importance of reliable detection\\nmethods given the quick advancement of AI-driven mul-\\ntimedia alteration. In order to create a more precise and\\nsuccinct deepfake detection system, methods including CNN,\\nXception Network, Recurrent Neural Networks (RNN), and\\nLong Short Term Memory (LSTM) are investigated.\\nThe goal of the DeepFakeDG project by [23] was to\\ncreate a web application that uses machine learning and\\ndeep learning techniques to identify falsified information.\\nThe study tackles the issues raised by deepfake algorithms by\\nutilizing methods like face swapping and behavioral analysis,\\nhighlighting the possible uses of deepfake detection in legal\\nand law enforcement settings.\\nExamining Vision Transformers (ViTs) for multiclass\\ndeepfake picture detection is a unique approach to the\\nrapidly changing field of facial modification technology,\\nas suggested by [24]. The study is the first to take into\\naccount the StyleGAN2 and Stable Diffusion problems. ViTs\\noutperform conventional CNN-based models in terms of\\ndetection accuracy, precision, and recall.\\nThe authors of [25] concentrate on the use of artificial\\nintelligence (AI), machine learning, and neural networks in\\nconjunction with deep learning approaches to classify actual\\nand fake human faces. The study’s impressive accuracy,\\nattained by using deep learning algorithms like ResNet50,\\nhighlights the promise of these methods in differentiat-\\ning between real and fake facial photos. In summary,\\nthe literature review highlights the ongoing progress in\\ndeepfake detection techniques, tackling the various issues\\nbrought about by developing multimedia manipulation\\ntechnologies.\\nIII. MATERIALS AND METHODS\\nA. RESIDUAL NEURAL NETWORK\\nResidual Neural Network, or ResNet, is a deep learning\\narchitecture that was proposed by [26]. It is widely used\\nin computer vision tasks and has achieved state-of-the-art\\nperformance on various image recognition challenges [27],\\n[28]. The main idea behind ResNet is the introduction of\\nresidual connections [29], which allow for the efficient\\ntraining of very deep neural networks. ResNet architecture\\ntypically consists of several convolutional layers followed by\\nresidual blocks. A residual block is composed of multiple\\nconvolutional layers with shortcut connections bypassing\\nthese layers [30]. This structure enables the network to learn\\nresidual functions representing the difference between the\\ninput and the desired output, making the learning process\\nmore efficient [30]. The ResNet architecture consists of\\nmultiple layers [31], including convolutional layers, residual\\nblocks, and an output layer as Figure 1 shows. The input\\nrepresents initial input image or feature map. The input\\nVOLUME 12, 2024 86393'),\n",
       " Document(metadata={'source': 'data\\\\Hybrid_Deep_Learning_Model_Based_on_GAN_and_RESNET_for_Detecting_Fake_Faces (1).pdf'}, page_content='S. Safwat et al.: Hybrid Deep Learning Model Based on GAN and RESNET for Detecting Fake Faces\\npasses through a convolutional layer, which applies a set of\\nlearnable filters to extract features from the input. A residual\\nblock consists of two or more convolutional layers with\\nshortcut connections. The input to the block is passed\\nthrough the convolutional layers, and the output is added\\nto the original input through the shortcut connection. This\\nbypass allows the network to learn the residual function—\\nthe difference between the input and the desired output.\\nThe residual function makes it easier to train very deep\\nnetworks.\\n1) CHANNEL-WISE ATTENTION MECHANISMS\\nAn important development in deep learning architectures is\\nChannel-Wise Attention Mechanisms, especially in convolu-\\ntional neural networks (CNNs), where the ability to recognize\\ncomplex patterns is critical. During the feature extraction\\nprocess, these methods selectively highlight pertinent feature\\nchannels while suppressing noise and unnecessary data.\\nChannel-Wise Attention Mechanisms provide numerous\\nbenefits to performance when they are incorporated into\\nthe feature extraction stage of RESNET50, a well-known\\nCNN architecture that is distinguished by its deep layers\\nand skip connections. First of all, they allow for selective\\nfeature focus, which makes sure the network highlights\\nimportant characteristics that are essential for differentiating\\nbetween real and modified images, such those found in false\\nface identification tasks. Additionally, these technologies\\nsupport adaptive feature representation, which enables the\\nmodel to dynamically modify feature representations in\\nresponse to input data, hence boosting discriminative skills\\nand capturing subtle variations.Moreover, by reducing the\\nimpact of unimportant changes, their integration strengthens\\ngeneralization, promoting robustness against adversarial\\nperturbations and enhancing performance on unknown data.\\nSurprisingly, these performance gains are attained with\\nmerely a slight rise in computing complexity, making\\nChannel-Wise Attention Mechanisms suitable for practical\\nimplementation in real-world scenarios without substantial\\noverhead. As a result, its incorporation into RESNET50\\ngreatly increases its performance in jobs requiring accurate\\nfeature extraction, such as false face identification, among\\nothers [32].\\nAfter several residual blocks, the network typically applies\\nglobal average pooling, which computes the average value\\nof each feature map. This reduces the spatial dimensions\\nof the feature maps and aggregates the learned information\\nacross the entire image. Finally, the global average pooled\\nfeatures are passed through a fully connected layer or a\\nsoftmax layer to produce the desired output, such as class\\nprobabilities.\\nB. GENERATIVE ADVERSARIAL NETWORKS (GANs)\\nGAN is a type of deep learning architecture that is used\\nfor generating new data samples, such as images introduced\\nin [17]. A typical GAN consists of two components: generator\\nFIGURE 1. ResNet architecture.\\nand discriminator, where both networks compete with each\\nother. The generator is the heart of the GAN, where it\\nattempts to generate fake data that looks real by learning the\\nfeatures from the real data. The discriminator evaluates the\\ngenerated data with the real data and classifies whether\\nthe generated data looks real or not and provides feedback to\\nthe generator to improve its data generation. The goal of the\\ngenerator is to generate data that can trick the discriminator.\\nThe architecture of the basic model of GAN is shown in\\nFigure 2.\\nIV. PROPOSED MODEL AND DATASET\\nA thorough explanation of the proposed model and its\\nmethod for identifying real and fake faces is provided\\nin this section. A critical problem in face recognition\\nis addressed by the proposed model in this study. The\\nmodel can be useful in areas like security and criminal\\ninvestigation because it can distinguish between actual and\\nfraudulent photos effectively. The important elements of the\\nproposed model, including the use of machine learning, deep\\nlearning methods, and spatial domain features, will be briefly\\ndiscussed. The methodologies used in the study will also be\\n86394 VOLUME 12, 2024'),\n",
       " Document(metadata={'source': 'data\\\\Hybrid_Deep_Learning_Model_Based_on_GAN_and_RESNET_for_Detecting_Fake_Faces (1).pdf'}, page_content='S. Safwat et al.: Hybrid Deep Learning Model Based on GAN and RESNET for Detecting Fake Faces\\nFIGURE 2. Architecture of GAN.\\nexamined, with a focus on their advantages and potential\\ndrawbacks.\\nA. DATASET\\nThe dataset used in this study was introduced in [1], The Real\\nand Fake Face Detection dataset is a widely used benchmark\\ndataset contains 2,041 face images, 1,081 images labeled as\\nreal images such as Figure 3(a) and 960 images are labeled as\\nfake images such as Figure 3(b). The benchmark is used for\\nassessing the effectiveness of various face detection models in\\ndistinguishing between real and fake images. Different digital\\nimage manipulation methodologies are also used to generate\\nthe fake images in this dataset like face swap, face2face, and\\nDeepfakes.\\nB. PROPOSED MODEL STEPS AND ARCHITECTURE\\nThe proposed architecture consists of six phases. The overall\\narchitecture of the proposed model is shown in Figure 4.\\nThe first phase is data preprocessing. First, data cleaning is\\napplied to check the dataset for any corrupt or mislabeled\\nimages. Any problematic images are removed to ensure data\\nintegrity and prevent the model from learning from incorrect\\nor noisy samples. Then, data augmentation techniques are\\napplied to increase the dataset’s size and diversity. Common\\naugmentations include rotation, flipping, scaling, and random\\ncrops. This step helps the model become more robust and\\nmore able to generalize better on unseen data. Next, all\\nimages are resized to a consistent size that can be fed\\nFIGURE 3. Example of real and fake face used in training phase.\\ninto the deep learning model. Deep learning models, such\\nas ResNet, typically require images of fixed dimensions.\\nCommon choices are 224 × 224 pixels. At last, the pixel\\nvalues of the images are normalized to bring them to a\\ncommon scale. The most common approach is to scale\\nthe pixel values to the range [0, 1]. This step helps the\\nmodel converge faster during training and prevents issues\\nrelated to different pixel value scales. In this research\\nendeavor, modifications were introduced to the standard\\nResNet model architecture to optimize its suitability for\\na specific analytical task. Initially, the ResNet50 model,\\na pre-trained Convolutional Neural Network (CNN), was\\nselected as the base framework. To tailor the model to feature\\nVOLUME 12, 2024 86395'),\n",
       " Document(metadata={'source': 'data\\\\Hybrid_Deep_Learning_Model_Based_on_GAN_and_RESNET_for_Detecting_Fake_Faces (1).pdf'}, page_content='S. Safwat et al.: Hybrid Deep Learning Model Based on GAN and RESNET for Detecting Fake Faces\\nextraction objectives, a departure was made from the standard\\npractice of solely discarding the final classification layers.\\nA ResNet50 model serves as the foundational Convolutional\\nNeural Network (CNN) architecture throughout the feature\\nextraction stage. On the other hand, attention mechanisms\\nare incorporated into the ResNet backbone, in contrast to\\nconventional methods that discard the final classification\\nlayers. In particular, to capture attention-weighted feature\\nrepresentations, attention modules are introduced after par-\\nticular convolutional layers. By dynamically adjusting the\\nsignificance of various spatial regions within the feature\\nmaps, these attention modules allow the model to concentrate\\non pertinent facial characteristics and manipulation artifacts.\\nInstead, substantive enhancements were introduced to aug-\\nment both training efficiency and predictive accuracy. This\\ninvolved incorporating additional convolutional layers into\\nthe ResNet architecture, fine-tuning them to discern intricate\\nfeatures within the image dataset. Additionally, optimization\\nof kernel sizes was performed to better capture nuanced\\npatterns in the data. Innovative regularization techniques,\\nincluding dropout and batch normalization, were deployed\\nto mitigate overfitting risks and enhance the model’s\\ngeneralization ability. The efficacy of these modifications\\nwas rigorously evaluated through systematic experimen-\\ntation, providing empirical evidence of their substantial\\nimpact on training efficacy and predictive performance.\\nSubsequently, the modified ResNet model, enriched with\\nbespoke alterations, was employed for feature extraction,\\nresulting in robust feature representations. These extracted\\nfeatures, imbued with tailored modifications, served as input\\nfor downstream analytical tasks, including classification,\\nclustering, and feature similarity analysis. Through orches-\\ntrated enhancements, this study distinguishes itself from the\\nconventional ResNet framework, underscoring its superior\\nefficacy and adaptability for the targeted analytical domain.\\nThe third phase involves generating fake faces using GAN.\\nThis is done through training a GAN to generate realistic\\nfake face images. The generator network takes random noise\\nas input and generates fake face images. The discriminator\\nnetwork tries to distinguish between real and fake faces. The\\nGAN is then trained using a combination of adversarial and\\nreconstruction losses to ensure realistic fake face generation.\\nThe fourth phase includes the proposed hybrid model.\\nFirst, it takes the feature representations obtained from the\\nCNN as input. Then additional layers (e.g., fully connected\\nlayers) are added to the CNN’s feature representation. The\\noutput of the additional layers is then connected to the\\nGAN’s discriminator network. Next, the combined model\\nis trained by freezing the CNN layers and updating the\\nGAN’s discriminator and additional layers. If necessary, the\\nentire hybrid model is fine-tuned. The fifth phase is Training,\\nin which a labeled dataset of real and fake face images is used\\nfor training. The hybrid model is trained using a suitable loss\\nfunction (e.g., binary cross-entropy) to classify real and fake\\nfaces. Accordingly, the hybrid model’s weights are updated\\nusing backpropagation and gradient descent. The sixth phase\\nis Evaluation, in which the performance of the hybrid model\\nis assessed on a separate validation or test dataset. Metrics\\nsuch as accuracy, precision, recall, and F1-score to are\\ncalculated to evaluate the model’s effectiveness in fake face\\ndetection.\\nV. RESULTS AND DISCUSSIONS\\nIn this section, the obtained results of many experiments and\\nthe proposed model are introduced, but initially the section\\nbriefly describes the different measures used to evaluate the\\nperformance of these models.\\nSensitivity (Recall): sensitivity measures the proportion of\\ntrue positives that are correctly identified as such. In other\\nwords, it is the probability that a test will correctly identify a\\npositive case.\\nSensitivity = TP/(TP + FN). (1)\\nwhere TP is True Positive, FN is False Negative.\\nPrecision: Precision measures the fraction of positive\\npredictions that are actually positive\\nPrecision = TP/TP + FP. (2)\\nwhere FP is False Positive.\\nAccuracy: accuracy measures the fraction of predictions\\nthat are correct, regardless of whether they are positive or\\nnegative.\\nAccuracy = (TP + TN)/(TP + FP + FN + TN). (3)\\nF1 Measure: is a weighted average of precision and recall.\\nIt is calculated by taking the harmonic mean of precision and\\nrecall.\\nF1Measure = 2 ∗ (precision ∗ Recall)/(precision + Recall).\\n(4)\\nIn order to guarantee the stability and applicability of the\\nsuggested hybrid deep learning model, a careful data division\\nstrategy is adopted, dividing the dataset into 70% for training\\nand 30% for testing. This partitioning technique allowed for\\nthorough evaluation of our model’s performance, letting it\\nlearn from most of the data while undergoing a thorough anal-\\nysis on a different, unseen pieces. During the training phase,\\nwe also used a k-fold cross-validation (CV) technique is\\nalso employed, which entailed splitting the training data into\\nseveral folds and training and verifying the model iteratively.\\nThe model’s capacity to generalize across various training\\ndata subsets was further guaranteed by this method. The 30%\\nset aside for testing functioned as an independent dataset,\\nunaltered during model development, to simulate real-world\\ncircumstances and improve the model’s applicability. The\\nObjective of the integration of data splitting and k-fold CV\\nis reinforce the dependability of the results and highlight the\\nmodel’s efficiency in a variety of situations.\\nIn this research, many experiments were applied on the\\ndataset to compare their results with the proposed model.\\nFirstly, we applied the VGG16 which is a deep convolutional\\n86396 VOLUME 12, 2024'),\n",
       " Document(metadata={'source': 'data\\\\Hybrid_Deep_Learning_Model_Based_on_GAN_and_RESNET_for_Detecting_Fake_Faces (1).pdf'}, page_content='S. Safwat et al.: Hybrid Deep Learning Model Based on GAN and RESNET for Detecting Fake Faces\\nFIGURE 4. The overall architecture of the proposed model.\\nTABLE 1. VGG16 results.\\nTABLE 2. ResNet-50 results.\\nneural network architecture known for its simplicity and\\neffectiveness, is applied. It consists of 16 weight layers,\\nincluding convolutional and fully connected layers. It follows\\na repeated pattern of using small 3 × 3 convolutional filters\\nfollowed by max-pooling layers. VGG16’s main contribution\\nis in demonstrating the benefits of using deep networks for\\nimage classification. The results obtained from the VGG16\\nnetwork is reported in Table 1 and Figure 5.\\nTable 2 and Figure 6 show the results yielded from the\\nsecond experiment, in this experiment, ResNet-50 is used to\\nclassify the real faces and fake faces\\nThe results of the third experiment that yielded from the\\nproposed model, which is the hybrid between the ResNET-50\\nand the GAN algorithm is presented in Table 3 and Figure 7.\\nThe hybrid model attempts to find the optimum generated\\nTABLE 3. Results of the proposed model.\\nimages from GAN starting from 100 image and the best value\\nwhen using 400 images. The training and testing ratio used 70\\n% and 30 %.\\nThe model underwent exhaustive training over 100 epochs,\\neach spanning approximately 10 hours, on a workstation\\nequipped with a single NVIDIA GPU, 16 GB of RAM, and\\na 6-core Intel i7 processor. Parameter selection, including an\\ninput size of (224, 224) and a batch size of 64, was guided\\nby rigorous experimentation. Custom layers seamlessly\\nintegrated into the model augmented its discriminative\\ncapabilities, leveraging the robustness of the ResNet-50\\narchitecture pretrained on ImageNet. Additionally, data\\naugmentation techniques, such as rotation, width and height\\nchanges, and horizontal flips, were employed to enhance\\nthe model’s ability to identify complex elements in facial\\nphotographs. The utilization of the Adam optimizer with\\nbinary crossentropy loss contributed to improved accuracy.\\nDespite the ResNet model’s known demand for a substantial\\nnumber of parameters, resulting in a bulky size, the proposed\\nVOLUME 12, 2024 86397'),\n",
       " Document(metadata={'source': 'data\\\\Hybrid_Deep_Learning_Model_Based_on_GAN_and_RESNET_for_Detecting_Fake_Faces (1).pdf'}, page_content='S. Safwat et al.: Hybrid Deep Learning Model Based on GAN and RESNET for Detecting Fake Faces\\nFIGURE 5. False positive rate vs true positive rate for VGG 16 network.\\nTABLE 4. Architecture of VGG16 and ResNET-50.\\nmodel structure and parameters represent the culmination\\nof iterative refinement and experimentation, reflecting the\\nbest configuration achieved through exhaustive optimization\\nefforts. Acknowledging the potential for further enhance-\\nments in overall accuracy and research outcomes through\\nmodifications to the ResNet model, future work will explore\\nthe application of optimization techniques or heuristic\\nmethods to systematically identify optimal parameters.\\nFrom the previous results, it can be noticed that the results\\nobtained from ResNET- 50 are better than VGG16 due to\\nresidual connections. As a way to enhance the results of the\\nResNET-50, the proposed model was applied by hybridizing\\nResNET-50 with GAN algorithms. As shown in table 3, the\\naccuracy results of the proposed model reached above 83%\\nwhich is better than the results obtained from the ResNET-\\n50 network by nearly 10%. Figure 8 shows an overall\\ncomparison between the proposed hybrid model and VGG16\\nand ResNET-50.\\nA comparison between the proposed model in this research\\nand a model that uses ResNET 18 implemented in [1] is\\npresented in Table 5.\\nWhen compared to previous studies, the third model—a\\nhybrid that combines RESNET50 with a GAN algorithm—\\nshows better results, especially when compared to more\\nconventional models like VGG16 and stand-alone deep archi-\\ntectures like RESNET50. The hybrid model’s noteworthy\\nsuccess can be ascribed to a number of important features that\\nalso improve its fake face detecting ability.\\nFirstly, the hybrid model makes use of both the generative\\nand discriminative components’ advantages. The discrimi-\\nnative core is RESNET50, which is renowned for its deep\\nand efficient feature extraction capabilities. As a result,\\nthe model can distinguish between minute characteristics\\nand patterns linked to both authentic and synthetic facial\\nfeatures. In addition to introducing a generative component,\\nthe addition of a GAN allows the model to identify fake faces\\nthat already exist as well as potential variants or new instances\\nof synthetic faces that might appear in the future. Second, the\\nGAN component improves the model’s generalization over\\na wide variety of fictitious face variants by introducing a\\nnovel type of data augmentation during training. The hybrid\\nmodel gains exposure to a wider dataset by producing realistic\\nsynthetic faces. This can potentially mitigate the risk of\\noverfitting and enhance its resilience in real-world situations\\n86398 VOLUME 12, 2024'),\n",
       " Document(metadata={'source': 'data\\\\Hybrid_Deep_Learning_Model_Based_on_GAN_and_RESNET_for_Detecting_Fake_Faces (1).pdf'}, page_content='S. Safwat et al.: Hybrid Deep Learning Model Based on GAN and RESNET for Detecting Fake Faces\\nFIGURE 6. False positive rate vs true positive rate for ResNet-50 network.\\nwhere the emergence of false faces can be very unpredictable\\nand dynamic. In addition, the effectiveness of the hybrid\\nmodel emphasizes how crucial it is to take the complete\\ncontext of false face detection into account. The hybrid\\nmodel’s incorporation of GAN-generated images helps it to\\nbetter understand the subtleties of facial structure, expression,\\nand realism—factors crucial in distinguishing sophisticated\\nfake faces that may elude the detection capabilities of\\nsimpler models—even though deep learning architectures\\nlike VGG16 and RESNET50 are skilled at capturing\\nintricate features. Moreover, the success of the hybrid model\\nimplies that constraints seen in traditional models may\\nbe addressed by carefully combining discriminative and\\ngenerative approaches. This discovery highlights the value\\nof hybrid architectures in pushing the limits of accuracy\\nand dependability in fake face identification and creates\\nnew research opportunities. In summary, the third model\\noutperforms the others because it combines the generative\\nskills of a GAN with the discriminative power of RESNET50\\nin a synergistic manner. This special combination not only\\nimproves feature discrimination but also presents a fresh way\\nto deal with the problems caused by constantly changing\\nfake face creation methods. The hybrid model’s effectiveness\\noffers important insights for next image processing and\\nartificial intelligence research and applications as the field of\\nfake face identification advances.\\nTABLE 5. Comparison between the proposed model and model that uses\\nResNET 18.\\nIn many cases, using a single architecture like RESNET50\\nor VGG16 can produce poor results compared to a hybrid\\nmodel that combines a GAN and a RESNET50 (Resid-\\nual Network). The qualities of both components working\\ntogether give rise to this advantage. Due to its deep\\ndesign, RESNET50 excels in extracting detailed features\\nfrom images, whereas GANs have the capacity to create\\nsynthetic data instances that mimic the training dataset.\\nThis hybridization combines the data generating power of\\nGANs with the feature extraction power of RESNET50\\nto provide more diversified and informative features for\\nclassification problems. The GAN-RESNET50 hybrid model\\nalso contributes to the augmentation and improvement of\\nthe data. GANs can produce extra synthetic data, resolving\\nproblems with insufficient training data and improving the\\nmodel’s capacity to generalize to new data. The retrieved\\ncharacteristics are also refined by the GAN’s ability to\\ndifferentiate between real and produced data, potentially\\nimproving their suitability for classification tasks. The hybrid\\nVOLUME 12, 2024 86399'),\n",
       " Document(metadata={'source': 'data\\\\Hybrid_Deep_Learning_Model_Based_on_GAN_and_RESNET_for_Detecting_Fake_Faces (1).pdf'}, page_content='S. Safwat et al.: Hybrid Deep Learning Model Based on GAN and RESNET for Detecting Fake Faces\\nFIGURE 7. False positive rate vs true positive rate for the proposed model.\\nFIGURE 8. Comparison between proposed model vs VGG16 and ResNET-50.\\nmodel captures underlying data distributions by utilizing\\nGANs for unsupervised pretraining, leading to more efficient\\nfeature extraction during the next fine-tuning stage. This\\nmethod is very useful in situations where there are noisy\\nor unbalanced datasets. To improve class separation during\\nclassification, the GAN can produce synthetic samples for\\nminority classes or clean noisy training data. The hybrid tech-\\nnique is also effective for domain adaptation tasks where there\\nare distribution mismatches between the source and target\\ndomains. The model can improve its performance in the target\\ndomain by domain adapting features from the source domain\\nto it. A powerful ensemble effect results from the interaction\\n86400 VOLUME 12, 2024'),\n",
       " Document(metadata={'source': 'data\\\\Hybrid_Deep_Learning_Model_Based_on_GAN_and_RESNET_for_Detecting_Fake_Faces (1).pdf'}, page_content='S. Safwat et al.: Hybrid Deep Learning Model Based on GAN and RESNET for Detecting Fake Faces\\nFIGURE 9. Confusion matrix.\\nof GANs with RESNET50. When discriminative features\\nlearnt by RESNET50 are paired with the diversity added\\nby GAN-generated data, model performance is frequently\\nimproved. It is important to stress that the effectiveness of\\nthis hybrid technique depends on a number of variables,\\nincluding dataset qualities, task difficulty, the success of\\nGAN training, and architectural decisions. To determine\\nwhether a hybrid GAN-RESNET50 model actually performs\\nbetter than standalone models like RESNET50 or VGG16\\nfor a particular job, careful testing and analysis are required.\\nThere are a number of reasons for the suggested hybrid\\ndeep learning model’s performance, and ways to make\\nit even better are constantly sought. The model’s initial\\nsuccess can be attributed to its clever use of GANs and\\nthe RESNET architecture, which combines the advantages\\nof both technologies. The synergistic effect between the\\ngenerative capability of GANs and the discriminative skills of\\nRESNET improves the model’s ability to distinguish between\\nauthentic and fake faces.\\nThere are strong benefits to improving detection per-\\nformance when attention processes are included into a\\nResNet-based phony face detection model. Selective focus is\\nmade possible by attention mechanisms, which let the model\\nfocus on important parts of the input image while ignoring\\nunimportant parts. This improves prediction accuracy by\\nallowing the model to prioritize the examination of particular\\nvisual features or manipulation artifacts indicative of forging\\nin the context of fake face detection. Moreover, by highlight-\\ning pertinent areas of the input image, attention techniques\\nimprove feature representation and help conventional CNN\\narchitectures pick up on minute details or patterns connected\\nto phony faces. This feature augmentation increases the\\nmodel’s resilience to changes in lighting, facial emotions, and\\nimage quality in addition to improving detection accuracy.\\nFurthermore, by producing attention maps that clarify which\\nelements of the input image impact predictions and facilitate\\ncomprehension of the model’s decision-making process,\\nattention mechanisms enhance interpretability. Furthermore,\\nattention mechanisms serve as a regularization strategy,\\npreventing overfitting by motivating the model to suppress\\nnoise or extraneous data and concentrate on relevant features.\\nThis improves generalization capacity and guarantees more\\ndependable detection performance across a variety of datasets\\nand real-world situations.\\nConfusion matrices were used to assess the effectiveness\\nof three classification techniques: VGG-16, ResNet-50,\\nand the Proposed System, as shown in Figure 9. With\\n195 true positives and 183 true negatives, VGG-16 showed\\na balanced performance; nevertheless, its false positive and\\nfalse negative rates were rather high at 119 and 115,\\nrespectively. With 230 true positives, ResNet-50 showed\\nbetter sensitivity; however, this came at the expense of a larger\\nfalse negative rate of 88. On the other hand, the Proposed\\nSystem demonstrated significant improvements in accuracy\\nand sensitivity, obtaining 300 true positives and 280 true\\nnegatives. Furthermore, the Proposed System showed a\\nbetter trade-off between true positive and false positive rates\\nthan both VGG-16 and ResNet-50, despite having a little\\nhigher false positive rate of 90. These results indicate that\\nthe Proposed System has a strong advantage over existing\\napproaches such as VGG-16 and ResNet-50 in classification\\ntasks, demonstrating its potential for enhanced performance.\\nThe dedication to ongoing development is in line with how\\nfake face production technology are developing. The goal\\nis to improve the model’s performance and dependability\\nin real-world applications including identity confirma-\\ntion, social media content moderation, and cybersecurity\\nby fine-tuning its decision-making processes, addressing\\nVOLUME 12, 2024 86401'),\n",
       " Document(metadata={'source': 'data\\\\Hybrid_Deep_Learning_Model_Based_on_GAN_and_RESNET_for_Detecting_Fake_Faces (1).pdf'}, page_content='S. Safwat et al.: Hybrid Deep Learning Model Based on GAN and RESNET for Detecting Fake Faces\\npotential biases, and increasing the diversity of training\\ndata.\\nVI. CONCLUSION\\nIn this research, the authors provide a novel hybrid deep\\nlearning model to tackle the rising problem of recognizing\\nfake faces in an era of deepfake technology and increasingly\\nsophisticated picture alteration techniques. In order to\\ndevelop a reliable and precise method for distinguishing real\\nfrom fake facial photos, the study made use of the features\\nof the RESNET architecture after applying Channel-Wise\\nAttention Mechanisms and GANs. On a benchmark dataset,\\nthe suggested model performed superbly, obtaining high pre-\\ncision, recall, F1-score, accuracy, and ROC AUC score. These\\nfindings highlight the model’s efficiency and dependability\\nin the critical task of detecting fake faces. The contribution\\nis significant because it has the potential to be used in many\\nother fields, such as cybersecurity, identity verification, and\\nsocial media content control. In these domains, the ability to\\ndiscriminate between real and altered faces is crucial, and\\nour hybrid model provides a potent tool for tackling this\\nproblem. Future research in the field of fake face detection\\nshould focus on a few crucial areas to further improve the\\ncapabilities of hybrid deep learning models. For example,\\nnew deep learning architectures should be investigated, and\\noptimization techniques should be investigated to increase\\nthe model’s precision, recall, and overall accuracy. Increased\\ndetection performance can be facilitated by state-of-the-art\\nstructures and well calibrated parameters. Finally; future\\nwork could certainly explore cross database evaluations to\\nfurther validate the generalizability of the proposed model\\nacross different datasets and scenarios.\\nREFERENCES\\n[1] N. A. S. Eldien, R. E. Ali, and F. A. Moussa, ‘‘Real and fake face\\ndetection: A comprehensive evaluation of machine learning and deep\\nlearning techniques for improved performance,’’ in IEEE MTT-S Int.\\nMicrow. Symp. Dig., Jul. 2023, pp. 315–320.\\n[2] Y . Zhu, C. Zhang, J. Gao, X. Sun, Z. Rui, and X. Zhou, ‘‘High-compressed\\ndeepfake video detection with contrastive spatiotemporal distillation,’’\\nNeurocomputing, vol. 565, Jan. 2024, Art. no. 126872.\\n[3] A. Gandhi and S. Jain, ‘‘Adversarial perturbations fool deepfake detec-\\ntors,’’ in Proc. Int. Joint Conf. Neural Netw. (IJCNN), Jul. 2020, pp. 1–8.\\n[4] M. M. El-Gayar, M. Abouhawwash, S. S. Askar, and S. Sweidan, ‘‘A novel\\napproach for detecting deep fake videos using graph neural network,’’\\nJ. Big Data, vol. 11, no. 1, p. 22, Feb. 2024.\\n[5] O. B. Newton and M. Stanfill, ‘‘My NSFW video has partial occlusion:\\nDeepfakes and the technological production of non-consensual pornogra-\\nphy,’’Porn Stud., vol. 7, no. 4, pp. 398–414, Oct. 2020.\\n[6] W.-D. Zhou, L. Dong, K. Zhang, Q. Wang, L. Shao, Q. Yang, Y .-M. Liu,\\nL.-J. Fang, X.-H. Shi, C. Zhang, R.-H. Zhang, H.-Y . Li, H.-T. Wu, and\\nW.-B. Wei, ‘‘Deep learning for automatic detection of recurrent retinal\\ndetachment after surgery using ultra-widefield fundus images: A single-\\ncenter study,’’Adv. Intell. Syst., vol. 4, no. 9, Sep. 2022, Art. no. 2200067.\\n[7] A. M. Almars, ‘‘Deepfakes detection techniques using deep learning:\\nA survey,’’J. Comput. Commun., vol. 9, no. 5, pp. 20–35, 2021.\\n[8] X. Chang, J. Wu, T. Yang, and G. Feng, ‘‘DeepFake face image detection\\nbased on improved VGG convolutional neural network,’’ in Proc. 39th\\nChin. Control Conf. (CCC), Jul. 2020, pp. 7252–7256.\\n[9] Y . Fu, T. Sun, X. Jiang, K. Xu, and P. He, ‘‘Robust GAN-face detection\\nbased on dual-channel CNN network,’’ in Proc. 12th Int. Congr. Image\\nSignal Process., Biomed. Eng. Informat. (CISP-BMEI), Oct. 2019, pp. 1–5.\\n[10] N.-T. Do, I.-S. Na, and S.-H. Kim, ‘‘Forensics face detection from\\nGANs using convolutional neural network,’’ in Proc. ISITC, 2018,\\npp. 376–379.\\n[11] F. F. Kharbat, T. Elamsy, A. Mahmoud, and R. Abdullah, ‘‘Image feature\\ndetectors for deepfake video detection,’’ in Proc. IEEE/ACS 16th Int. Conf.\\nComput. Syst. Appl. (AICCSA), Nov. 2019, pp. 1–4.\\n[12] J. Parmar, S. Chouhan, V . Raychoudhury, and S. Rathore, ‘‘Open-world\\nmachine learning: Applications, challenges, and opportunities,’’ ACM\\nComput. Surv., vol. 55, no. 10, pp. 1–37, Oct. 2023.\\n[13] B. Bayar and M. C. Stamm, ‘‘A deep learning approach to universal image\\nmanipulation detection using a new convolutional layer,’’ in Proc. 4th ACM\\nWorkshop Inf. Hiding Multimedia Secur., Jun. 2016, pp. 5–10.\\n[14] B. Chesney and D. Citron, ‘‘Deep fakes: A looming challenge for privacy,\\ndemocracy, and national security,’’ Calif. L. Rev., vol. 107, p. 1753,\\nJan. 2019.\\n[15] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\\nrecognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\\nJun. 2016, pp. 770–778.\\n[16] O. A. Montesinos López, A. Montesinos López, and J. Crossa, ‘‘Funda-\\nmentals of artificial neural networks and deep learning,’’ in Multivariate\\nStatistical Machine Learning Methods for Genomic Prediction. Cham,\\nSwitzerland: Springer, 2022, pp. 379–425.\\n[17] X. Wu, D. Hong, J. Chanussot, Y . Xu, R. Tao, and Y . Wang, ‘‘Fourier-\\nbased rotation-invariant feature boosting: An efficient framework for\\ngeospatial object detection,’’ IEEE Geosci. Remote Sens. Lett., vol. 17,\\nno. 2, pp. 302–306, Feb. 2020.\\n[18] C. Clarke, J. Xu, Y . Zhu, K. Dharamshi, H. McGill, S. Black, and\\nC. Lutteroth, ‘‘FakeForward: Using deepfake technology for feedforward\\nlearning,’’ in Proc. CHI Conf. Hum. Factors Comput. Syst., Apr. 2023,\\npp. 1–17.\\n[19] S. Solaiyappan and Y . Wen, ‘‘Machine learning based medical image\\ndeepfake detection: A comparative study,’’ Mach. Learn. Appl., vol. 8,\\nJun. 2022, Art. no. 100298.\\n[20] S. Tufail, H. Riggs, M. Tariq, and A. I. Sarwat, ‘‘Advancements and\\nchallenges in machine learning: A comprehensive review of models,\\nlibraries, applications, and algorithms,’’ Electronics, vol. 12, no. 8, p. 1789,\\nApr. 2023.\\n[21] P. Theerthagiri and G. B. Nagaladinne, ‘‘Deepfake face detection using\\ndeep InceptionNet learning algorithm,’’ in Proc. IEEE Int. Students’ Conf.\\nElectr., Electron. Comput. Sci. (SCEECS), Feb. 2023, pp. 1–6.\\n[22] R. Chauhan, ‘‘Deep learning-based methods for detecting generated fake\\nfaces,’’Authorea Preprints, 2023.\\n[23] D. AbdElminaam, N. Sherif, Z. Ayman, M. Mohamed, and M. Hazem,\\n‘‘DeepFakeDG: A deep learning approach for deep fake detection and\\ngeneration,’’J. Comput. Commun., vol. 2, no. 2, pp. 31–37, Jul. 2023.\\n[24] M. A. Arshed, S. Mumtaz, M. Ibrahim, C. Dewi, M. Tanveer, and\\nS. Ahmed, ‘‘Multiclass AI-generated deepfake face detection using patch-\\nwise deep learning model,’’ Computers, vol. 13, no. 1, p. 31, Jan. 2024.\\n[25] F. M. Salman and S. S. Abu-Naser, ‘‘Classification of real and fake human\\nfaces using deep learning,’’ Tech. Rep., 2022.\\n[26] J. C. Neves, R. Tolosana, R. Vera-Rodriguez, V . Lopes, H. Proença, and\\nJ. Fierrez, ‘‘GANprintR: Improved fakes and evaluation of the state of the\\nart in face manipulation detection,’’ 2019, arXiv:1911.05351.\\n[27] Z. Zhang, Z. Lei, M. Omura, H. Hasegawa, and S. Gao, ‘‘Den-\\ndritic learning-incorporated vision transformer for image recognition,’’\\nIEEE/CAA J. Autom. Sinica, vol. 11, no. 2, pp. 539–541, Feb. 2024.\\n[28] H. M. T. Khushi, T. Masood, A. Jaffar, S. Akram, and S. M. Bhatti,\\n‘‘Performance analysis of state-of-the-art CNN architectures for brain\\ntumour detection,’’ Int. J. Imag. Syst. Technol., vol. 34, no. 1, Jan. 2024,\\nArt. no. e22949.\\n[29] E. Hassan, M. S. Hossain, A. Saber, S. Elmougy, A. Ghoneim, and\\nG. Muhammad, ‘‘A quantum convolutional network and ResNet (50)-\\nbased classification architecture for the MNIST medical dataset,’’ Biomed.\\nSignal Process. Control, vol. 87, Jan. 2024, Art. no. 105560.\\n[30] H. Wang and L. Ma, ‘‘Image generation and recognition tech-\\nnology based on attention residual GAN,’’ IEEE Access, vol. 11,\\npp. 61855–61865, 2023.\\n[31] S. Duan, W. Pan, Y . Leng, and X. Zhang, ‘‘Two ResNet mini archi-\\ntectures for aircraft wake vortex identification,’’ IEEE Access, vol. 11,\\npp. 20515–20523, 2023.\\n[32] C. Chen and B. Li, ‘‘An interpretable channelwise attention mechanism\\nbased on asymmetric and skewed Gaussian distribution,’’ Pattern Recog-\\nnit., vol. 139, Jul. 2023, Art. no. 109467.\\n86402 VOLUME 12, 2024\\nView publication stats'),\n",
       " Document(metadata={'source': 'data\\\\Journal_of_Artificial_Intelligence_Research.pdf'}, page_content=\"Journal of Artificial Intelligence\\nResearch\\nDiscipline Artificial intelligence\\nLanguage English\\nEdited by Shaul Markovitch\\nPublication details\\nHistory 1993-present\\nPublisher AAAI Press\\nOpen access Yes\\nImpact factor 2.441 (2019)\\nStandard abbreviations\\nISO 4 J. Artif. Intell. Res.\\nMathSciNet J. Artificial Intelligence Res.\\nIndexing\\nCODEN JAIRFR (https://cassi.cas.org/s\\nearching.jsp?searchIn=codens\\n&exactMatch=y&c=WIy460-R_\\nDY&searchFor=JAIRFR)\\nISSN 1076-9757 (https://www.worldca\\nt.org/search?fq=x0:jrnl&q=n2:10\\n76-9757)\\nLCCN sn94003860 (https://lccn.loc.go\\nv/sn94003860)\\nOCLC no. 299637127 (https://www.worldc\\nat.org/oclc/299637127)\\nJournal of Artificial Intelligence Research\\nThe Journal of Artificial Intelligence Research\\n(JAIR) is an open access peer-reviewed scientific\\njournal covering research in all areas of artificial\\nintelligence.\\nIt was established in 1993 as one of the first scientific\\njournals distributed online.[1] Paper volumes are\\nprinted by the AAAI Press. The Journal for Artificial\\nIntelligence Research (JAIR) is one of the premier\\npublication venues in artificial intelligence. JAIR also\\nstands out in that, since its launch in 1993, it has been\\n100% open-access and non-profit.\\nThe Journal of Artificial Intelligence Research (JAIR)\\nis dedicated to the rapid dissemination of important\\nresearch results to the global artificial intelligence (AI)\\ncommunity. The journal's scope encompasses all areas\\nof AI, including agents and multi-agent systems,\\nautomated reasoning, constraint processing and search,\\nknowledge representation, machine learning, natural\\nlanguage, planning and scheduling, robotics and\\nvision, and uncertainty in AI.\\nThe journal is abstracted and indexed by Inspec,\\nScience Citation Index, and MathSciNet. According to\\nthe Journal Citation Reports, the journal has a 2019\\nimpact factor of 2.441.[2]\\nAccording to the SciMago Journal and Country Rank,\\nthe journal is ranked 8th among all open access\\ncomputer science journals with an H-index of 112.[3]\\nHistory\\nContent\\nAbstracting and indexing\"),\n",
       " Document(metadata={'source': 'data\\\\Journal_of_Artificial_Intelligence_Research.pdf'}, page_content='Links\\nJournal homepage (http://www.jair.org)\\nIssue Archive (https://www.jair.org/index.ph\\np/jair/issue/archive)\\nHowever, according to Google Scholar in 2021 it only\\nhas an h5-index of 38, which suggests some potential\\nissues in measuring its impact.[4]\\n1. Steven Minton, Michael P. Wellman (1999). \"JAIR at Five\" (http://www.aaai.org/ojs/index.ph\\np/aimagazine/article/view/1458). AI Magazine. 20 (2): 83–91. doi:10.1609/aimag.v20i2.1458\\n(https://doi.org/10.1609%2Faimag.v20i2.1458).\\n2. \"Journals Ranked by Impact: Computer Science, Artificial Intelligence\". 2019 Journal\\nCitation Reports. Web of Science (Science ed.). Thomson Reuters. 2019.\\n3. \"Journal Rankings on Computer Science\" (https://www.scimagojr.com/journalrank.php?area\\n=1700&openaccess=true&order=h&ord=desc). www.scimagojr.com. Retrieved 2020-08-24.\\n4. \"Google Scholar Metrics\" (https://scholar.google.com/citations?hl=en&view_op=search_ven\\nues&vq=Journal+of+Artificial+Intelligence+Research&btnG=). scholar.google.com.\\nRetrieved 2021-02-10.\\nOfficial website (http://www.jair.org)\\nRetrieved from \"https://en.wikipedia.org/w/index.php?\\ntitle=Journal_of_Artificial_Intelligence_Research&oldid=1283251314\"\\nReferences\\nExternal links'),\n",
       " Document(metadata={'source': 'data\\\\Journal_of_Machine_Learning_Research.pdf'}, page_content='Journal of Machine Learning\\nResearch\\nDiscipline Machine learning\\nLanguage English\\nEdited by Francis Bach, David Blei\\nPublication details\\nHistory 2000–present\\nPublisher JMLR, Inc. and Microtome\\nPublishing (United States)\\nOpen access Yes\\nImpact factor 4.091 (2018)\\nStandard abbreviations\\nISO 4 J. Mach. Learn. Res.\\nIndexing\\nCODEN JMLRAJ (https://cassi.cas.org/s\\nearching.jsp?searchIn=codens\\n&exactMatch=y&c=WIy460-R_\\nDY&searchFor=JMLRAJ)\\nISSN 1532-4435 (https://www.worldca\\nt.org/search?fq=x0:jrnl&q=n2:15\\n32-4435) (print)\\n1533-7928 (https://www.worldca\\nt.org/search?fq=x0:jrnl&q=n2:15\\n33-7928) (web)\\nLCCN 00212568 (https://lccn.loc.gov/0\\n0212568)\\nOCLC no. 712803341 (https://www.worldc\\nat.org/oclc/712803341)\\nLinks\\nJournal homepage (https://www.jmlr.org/)\\nOnline archive (https://dl.acm.org/journal/jm\\nlr)\\nJournal of Machine Learning Research\\nThe Journal of Machine Learning Research is a peer-\\nreviewed open access scientific journal covering\\nmachine learning. It was established in 2000 and the\\nfirst editor-in-chief was Leslie Kaelbling.[1] The\\ncurrent editors-in-chief are Francis Bach (Inria) and\\nDavid Blei (Columbia University).\\nThe journal was established as an open-access\\nalternative to the journal Machine Learning. In 2001,\\nforty editorial board members of Machine Learning\\nresigned, saying that in the era of the Internet, it was\\ndetrimental for researchers to continue publishing their\\npapers in expensive journals with pay-access archives.\\nThe open access model employed by the Journal of\\nMachine Learning Research allows authors to publish\\narticles for free and retain copyright, while archives are\\nfreely available online.[2]\\nPrint editions of the journal were published by MIT\\nPress until 2004 and by Microtome Publishing\\nthereafter. From its inception, the journal received no\\nrevenue from the print edition and paid no subvention\\nto MIT Press or Microtome Publishing.[1]\\nIn response to the prohibitive costs of arranging\\nworkshop and conference proceedings publication with\\ntraditional academic publishing companies, the journal\\nlaunched a proceedings publication arm in 2007[3] and\\nnow publishes proceedings for several leading machine\\nlearning conferences, including the International\\nConference on Machine Learning, COLT, AISTATS,\\nand workshops held at the Conference on Neural\\nInformation Processing Systems.\\nHistory'),\n",
       " Document(metadata={'source': 'data\\\\Journal_of_Machine_Learning_Research.pdf'}, page_content='1. Shieber, Stuart (6 March 2012). \"An efficient journal\" (https://blogs.harvard.edu/pamphlet/20\\n12/03/06/an-efficient-journal/). The Occasional Pamphlet. Retrieved 12 February 2017.\\n2. \"Editorial Board of the Kluwer Journal, Machine Learning: Resignation Letter\" (http://sigir.or\\ng/files/forum/F2001/sigirFall01Letters.html). SIGIR Forum. 35 (2). 2001.\\n3. Lawrence, Neil (30 March 2015). \"Proceedings of Machine Learning Research\" (http://invers\\neprobability.com/2015/03/30/proceedings-of-machine-learning-research). Inverseprobability.\\nRetrieved 12 February 2017.\\n\"Top journals in computer science\" (http://www.timeshighereducation.co.uk/story.asp?sectio\\nncode=26&storycode=406557). Times Higher Education. 14 May 2009. Retrieved 22 August\\n2009.\\nOfficial website (https://www.jmlr.org/)\\nRetrieved from \"https://en.wikipedia.org/w/index.php?\\ntitle=Journal_of_Machine_Learning_Research&oldid=1297370036\"\\nReferences\\nFurther reading\\nExternal links'),\n",
       " Document(metadata={'source': 'data\\\\Machine_Learning_and_Knowledge_Extraction.pdf'}, page_content='Machine Learning and Knowledge\\nExtraction\\nDiscipline Machine learning, artificial\\nintelligence, data science\\nLanguage English\\nEdited by Andreas Holzinger\\nPublication details\\nHistory 2019–present\\nPublisher MDPI (Switzerland)\\nFrequency Quarterly\\nOpen access Yes\\nLicense Creative Commons Attribution\\nLicense\\nImpact factor 6.0 (2023)\\nStandard abbreviations\\nISO 4 Mach. Learn. Knowl. Extr.\\nIndexing\\nISSN 2504-4990 (https://www.worldca\\nt.org/search?fq=x0:jrnl&q=n2:25\\n04-4990)\\nLinks\\nJournal homepage (https://www.mdpi.com/j\\nournal/make)\\nMachine Learning and Knowledge Extraction\\nMachine Learning and Knowledge Extraction\\n(MAKE) is a peer-reviewed open-access scientific\\njournal covering research on machine learning,\\nknowledge extraction and related areas of data-driven\\nartificial intelligence. It is published by MDPI and was\\nlaunched in 2019 with Andreas Holzinger as founding\\nEditor-in-Chief.\\nThe journal publishes research articles, reviews,\\ntutorials and short notes spanning topics such as data\\necosystems, interactive and automated machine\\nlearning, explainable AI, privacy, graph learning and\\ntopological data analysis\\nThe journal is abstracted and indexed in several\\ndatabases, for example in:[1]\\nDOAJ[2]\\nEmerging Sources Citation Index[3]\\nScopus[4]\\nAccording to the Journal Citation Reports, the journal\\nhas a 2024 impact factor of 6.0.[5]\\n1. \"Machine Learning and Knowledge Extraction\" (http://miar.ub.edu/issn/2504-4990). MIAR:\\nInformation Matrix for the Analysis of Journals. University of Barcelona. Retrieved\\n2025-06-18.\\n2. \"Machine Learning and Knowledge Extraction\" (https://doaj.org/toc/2504-4990). Directory of\\nOpen Access Journals. Retrieved 2025-06-18.\\n3. \"Machine Learning and Knowledge Extraction included in ESCI\" (https://www.mdpi.com/abo\\nut/announcements/2661). Clarivate. Retrieved 2025-06-18.\\n4. \"Scopus preview – Machine Learning and Knowledge Extraction\" (https://www.scopus.com/\\nsourceid/21101109601). Scopus. Retrieved 2025-06-18.\\n5. \"Pharmaceuticals\". 2024 Journal Citation Reports. Web of Science (Science ed.). Clarivate.\\n2025.\\nAbstracting and indexing\\nReferences'),\n",
       " Document(metadata={'source': 'data\\\\Machine_Learning_and_Knowledge_Extraction.pdf'}, page_content='Official website (https://www.mdpi.com/journal/make)\\nRetrieved from \"https://en.wikipedia.org/w/index.php?\\ntitle=Machine_Learning_and_Knowledge_Extraction&oldid=1305679691\"\\nExternal links'),\n",
       " Document(metadata={'source': 'data\\\\machine_learning_translation_using_DL.pdf'}, page_content='IJSRSET184226 | Accepted : 15 Jan 2018  |  Published 22 Jan 2018 | January-February-2018 [ (4)2 : 145-150 ] \\n© 2018 IJSRSET  | Volume 4 | Issue 2  | Print ISSN: 2395-1990 | Online ISSN : 2394-4099 \\nNational Conference on Advanced Research Trends in Information and  \\n  Computing Technologies (NCARTICT-2018), Department of  IT, L. D. College of Engineering,  \\nAhmedabad, Gujarat, India In association with  \\nInternational Journal of Scientific Research in Science, Engineering and Technology \\n \\n145 \\nMachine Translation Using Deep Learning : A Survey \\nJanhavi R. Chaudhary1, Ankit C. Patel2 \\n1M. E. Student (Information Technology Department), L. D. College of Engineering, Gujarat Technological University, \\nAhmedabad, Gujarat, India \\n2Assistant Professor (Information Technology Department), L. D. College of Engineering, Gujarat Technological University, \\nAhmedabad, Gujarat, India \\nABSTRACT \\n \\nMachine Translation using Deep Learning (Neura l Machine Translation) is a newly proposed approach to machine \\ntranslation. The term Machine Translation is used in the sense of translation of one language to another, with no \\nhuman improvement. It can also be referred to as automated translation. Unlike the traditional statistical machine \\ntranslation, the neural machine translation aims at building a single neural network that can be jointly tuned to \\nmaximize the translation performance. This survey reveals the information about Deep Neural Network (DNN) and \\nconcept of deep learning in field of natural language processing i.e. machine  translation. It is better to use Recurrent \\nNeural Network(RNN) in Machine Translation. This paper studies various techniques used to train RNN for various \\nlanguage corpuses. RNN structure is very complicated and to train a large corpus is also a time -consuming task. \\nHence, a powerful hardware support (Graphics Processing Unit) is required. GPU improves the system performance \\nby decreasing training time  period. \\nKeywords: Machine Translation, Neural Networks, Neural Machine Translation, Deep Learning \\n \\nI. INTRODUCTION \\nA. Deep Learning \\nDeep learning is part of machine learning methods based \\non learning data representations, as opposed to task -\\nspecific algorithms[9]. Learning can be  supervised, \\npartially supervised or  unsupervised. Deep learning \\narchitectures such as  deep neural networks,  deep belief \\nnetworks and recurrent neural networks[1]  have been \\napplied to fields including  computer vision,  speech \\nrecognition, natural language  processing, audio \\nrecognition, social network filtering,  machine \\ntranslation and bioinformatics where they produced \\nresults comparable to and in some cases superior  human \\nexperts. \\n \\nB. Deep Neural Networks \\nA deep neural network (DNN) is an ANN with multiple \\nhidden layers between the input and output layers[ 1]. \\nDNNs are typically feed forward networks in which data \\nflows from the input layer to the output layer without \\nlooping back. Recurrent neural networks (RNNs), in \\nwhich data can flow in any direction, are used for \\napplications such as language modeling. Long short-term \\nmemory is particularly effective for this use. \\nConvolutional deep neural networks (CNNs) are used in \\ncomputer vision.  CNNs also have been applied \\nto acoustic modeling  for automatic speech recognition \\n(ASR)[1]. \\n \\nC. Machine Translation \\nMachine T ranslation (MT) is a sub -ﬁeld of \\ncomputational linguistics that investigates the use of \\ncomputer software to translate text or speech from one \\nnatural language to another. At its basic level, MT \\nperforms simple substitution of words in one natural \\nlanguage for words in another. Machine Translation \\nsystem are needed to translate literary works which from \\nany language into native languages. The literary work is \\nfed to the MT system and translation is done. Such MT \\nsystems can break the language barriers by ma king \\navailable work rich sources of literature available to \\npeople across the world[11]. Figure 1[11] shows process \\nof Machine Translation in the form of pyramid.'),\n",
       " Document(metadata={'source': 'data\\\\machine_learning_translation_using_DL.pdf'}, page_content='International Journal of S cientific Resear ch in Science, Engineering and Technology (ijsrset.com)  \\n \\n \\n146 \\n \\n \\n \\nFigure 1. Machine Translation Pyramid \\n \\nD. Neural Machine Translation \\nNeural Machine Translation is the approach of modeling \\nthe entire MT process via on e big artificial neural \\nnetwork[18]. Figure 2[18] shows architecture or encoder \\nand decoder. \\n \\nFigure 2. Neural encoder-decoder architecture \\n \\nII.  LITERATURE SURVEY \\n \\nJiajun Zhang and Chengqing Zong have designed many \\nkinds of Deep Neural Networks. There are five popular  \\nneural networs introduced in their research work[1]. \\n\\uf0fc Feed Forward neural network \\n\\uf0fc Recurrent neural network \\n\\uf0fc Recursive auto-encoder \\n\\uf0fc Recursive neural network \\n\\uf0fc Convolutional neural network \\n \\nFNN(Feed Forward Network) : \\nThe feed -forward neural network (FNN) is one of  the  \\nsimplest  m ultilayer  networks. Figure 3[1] shows an \\nFNN architecture with hidden layers  as well  as input \\nand output layers[1]. \\n \\nFigure 3. FNN Architecture \\nn-gram is collected from a text or speech corpus.  The \\nFNN attempts to predict the conditional probability of \\nthe next word given the fixed-window history words. \\n \\nRNN (Recurrent Neural Network): \\nThe recurrent neural network (RecurrentNN)  is \\ntheoretically more powerful than FNN in language \\nmodeling due  to its capability of representing all the \\nhistory words rather than a fixed -length context as in \\nFNN. \\n \\nRAE (Recursive Auto Encoder): \\nThe RAE provides a good way to embed a phrase or a \\nsentence in continuous space with an unsupervised or \\nsemisupervised method. \\n \\nRecursive Neural Network: \\nRecursiveNN differs from RAE in four points: \\nRecursiveNN is optimized with supervised learning; the \\ntree st ructure is usually fixed before training; \\nRecursiveNN doesn’t have to reconstruct the inputs; and \\ndifferent matrices can be used at different nodes. \\n \\nConvolutional Neural Network: \\nThe convolutional neural network (CNN)  consists of the \\nconvolution and pooling layers and provides a standard \\narchitecture that maps variable -length sentences into \\nfixed-size distributed vectors.  The CNN model takes as \\ninput the sequence of word embeddings, summarizes the \\nsentence meaning by convolving the sliding window and \\npooling the saliency through the sentence, and yields the \\nfixed-length distributed vector with other layers, such as \\ndropout and fully connected layers. \\n \\nKyunghyun Cho, Aaron Courville, and Yoshua Bengio  \\ndescribe Systems that learn to attend to different p laces \\nin the input, for each element of the output, for a variety  \\nof tasks: machine translation, image caption generation, \\nTransfer \\nDirect translation \\nInterlingua \\nTarget Source \\nAnalysis Generation'),\n",
       " Document(metadata={'source': 'data\\\\machine_learning_translation_using_DL.pdf'}, page_content='International Journal of S cientific Resear ch in Science, Engineering and Technology (ijsrset.com)  \\n \\n \\n147 \\nvideo clip description, and speech recognition [2]. The \\nattention-based neural machine translation uses a \\nbidirectional recurrent neur al network (BiRNN) as an \\nencoder. The forward network reads the input sentence \\nfrom the ﬁrst word to the last, resulting in a sequence of \\nstate vectors \\n                                {  ⃗⃗⃗⃗    ⃗⃗⃗⃗       ⃗⃗⃗⃗ } \\nThe backward network, on the other hand, reads the \\ninput se ntence in the reverse order, resulting in  \\n{  ⃗⃗⃗⃗      ⃗⃗⃗⃗⃗⃗⃗⃗⃗       ⃗⃗⃗⃗ } \\nThe use of the BiRNN  is crucial if the content -based \\nattention mechanism is used. The content -based \\nattention mechanism relies solely on a so-called content-\\nbased scoring, and without the context information from \\nthe whole sentence, words that appear multiple times in \\na sourc e sentence cannot be distinguished by the \\nattention model[2]. \\nAttention based Neural Network[2] \\nThe content -based attention mechanism computes the \\nrelevance of each spatial, temporal or spatio -temporally \\nlocalized region of the input,while the location -based \\none directly returns to which region the model needs to \\nattend,often in the form of the coordinate such as the –\\ncoordinate of an input image or the offset from the \\ncurrent coordinate. \\n \\nSanjanaashree P and Anand Kumar M  presented a new \\narea of Machine Le arning approach termed as a Deep \\nLearning for improving the bilingual machine \\ntransliteration task for Tamil and English languages with \\nlimited corpus [3]. This technique precedes Artificial \\nIntelligence. The system is built on Deep Belief \\nNetwork (DBN), a generative graphical model, which \\nhas been proved to work well with other Machine \\nLearning problem. They  have obtained 79.46% \\naccuracy for English to Tamil transliteration task and  \\n78.4 % for Tamil to English transliteration.  Bilingual \\nMachine Transliter ation task for Tamil and English \\nlanguages is proceeded using DBN [3]. Figure 4[3]  \\nshows DBN architecture. \\n \\nFigure 4. DBN Architecture \\n \\nThe system architecture is shown in Figure 5[3]. The \\ntwo layers RBM on the right side is the encoders for \\nsource language and the left side is named as target \\nlanguage encoders. The uppermost layer is called Joint \\nlayer that concatenates the output of the top layer of \\nsource and target encoders. For a word to be \\ntransliterated, the source language word is passed \\nthrough the source encoders and then reaches the joint \\nlayer and at last traverses downwards through the target \\nencoders resulting with the transli terated word as the \\nfinal output. In this architecture, each layer has ‘n’ \\nnumber of neurons. Greedy layer -wise training is \\nperformed, the output of a layer becomes an input for \\nthe preceding layer. \\n \\nFigure 5. System Architecture for Transliterian \\n \\nIt con cludes that a  Deep Learning approach for bi -\\nlingual transliteration is handled for English and Tamil \\nlanguages using Deep Belief Networks. DBN is fully bi-\\ndirectional, supports dimensionality reduction and it \\nsupports  unsupervised way of learning feature and a \\nsupervised way for transliteration.  with the small \\namount of corpus accounting to 3000 names, a decent \\nresult of accuracy around 79% is obtained in both ways, \\nbi-lingual transliteration. \\n \\nZhen Yang, Wei Chen, Feng Wang and Bo Xu validate \\nthe hypothes is and propose a simple and flexible \\nframework, which enables the NMT model to only focus \\non the relevant sense type of the input word in current \\ncontext[4]. Firstly, this is the first effort to introduce the \\nmulti-sense representation, which represents ea ch sense \\ntype of the word with a sense -specific embedding, into \\nNMT. Secondly, propose a sense search module which \\ncan detect the sense type of the word automatically. \\nMulti-Sense model is able to detect the sense type of the \\nword exactly, and achieves remarkable improvements on'),\n",
       " Document(metadata={'source': 'data\\\\machine_learning_translation_using_DL.pdf'}, page_content='International Journal of S cientific Resear ch in Science, Engineering and Technology (ijsrset.com)  \\n \\n \\n148 \\nevery test set over competitive baselines. The proposed \\nsense search module enables the model to detect the \\nright sense type of the input word automatically. Since \\nthe sense search module is task independent, it can be \\napplied to an y other semantic related NLP tasks with \\nlittle modification[4]. \\n \\nXIAO-XUE WANG, CONG -HUI ZHU, SHENG LI, \\nTIE-JUN ZHAO, DE -QUAN ZHENG have proposed \\ntrilingual NMT[5]. Based on the Encoder-Decoder and  \\nattentional mechanism, we translate source language to \\ntarget language, meanwhile translate another parallel \\nsource language to target language.  They provide two \\napproaches  called  splicing-model and  similarity-model. \\nBoth of the approaches are in order to enhance the \\nsemantic representation of input sequences. \\n \\nSplicing model[5] \\nIn this model, we get a new vector c which includes \\ninformation of vector c’ from source language1 and \\nvector c’’ from source language2. We think the new \\nvector c is the semantic representation of parallel source \\nlanguage1 and source language2. \\n \\nFigure 6[5].  The process of Splicing model \\n \\nSimilarity-model \\nFigure 7[5] shows the process.  Because the whole \\nprocess is based on similarity of vectors, we call the \\nmodel similarity-model in this paper. And as you see in \\nFigure 2, two systems are independent, so parameters \\nare independent. Only in the training process we need to \\ninput source lan guage1 and source language2 \\nsimultaneously. Once the model is trained, we could test \\nthe performance of the system from source language1 to \\ntarget language by only inputting source language1 and \\ntest the performance of the system from source \\nlanguage2 to t arget language by only inputting source \\nlanguage2. \\n \\nFigure 7. The process of similarity model \\n \\nFeng Wang, Wei Chen , Zhen Yang,  Xiaowei Zhang, \\nShuang xu and  Bo Xu have proposed NMT model with \\nclass-speciﬁc copy network, which is referred to as \\nCSCNMT[6]. With the network, the proposed NMT \\nmodel is able to decide which class the words in the \\ntarget belong to and which class in the source should be \\ncopied to.  Experimental results on Chinese -English \\ntranslation tasks show  that the  proposed model \\noutperforms the traditional NMT model  with a large \\nmargin especially for sentences containing the rare  \\nwords. Copy mechanism has been proposed to deal with \\nrare and unseen words for the neural network model \\nusing attention.  But the negative  point is that it’s only \\nable to decide whether to copy or not. It is unable t o \\ndetect which class should the rare word be copied to, \\nsuch as person, location, and organization.  This paper \\nproposes a new NMT model by novelly \\nincorporating a class -speciﬁc c opy network to \\novercome this issue[6]. \\n \\nAndi Hermanto, Teguh Bharata Adji, Noor Akhmad \\nSetiawa have proposed RNN model for English -\\nIndonesian machine translation[7]. In this research, a \\ncomparison between neural based network that adopts \\nRecurrent Neural N etwork (RNN) and statistical based \\nnetwork with n -gram model for two -way English -\\nIndonesian Machine Translation (MT) is conducted. The \\nperplexity value evaluation of both models show that the \\nuse of RNN obtains a more excellent result. Meanwhile, \\nBilingual Evaluation Understudy (BLEU) and Rank -\\nbased Intuitive Bilingual Evaluation Score (RIBES) \\nvalues increase by 1.1 and 1.6 higher than the results \\nobtained using statistical based.'),\n",
       " Document(metadata={'source': 'data\\\\machine_learning_translation_using_DL.pdf'}, page_content='International Journal of S cientific Resear ch in Science, Engineering and Technology (ijsrset.com)  \\n \\n \\n149 \\n \\nFigure 8. Recurrent Neural Language Model \\n \\nTable 1. Literature Review \\n \\nReview \\nPaper \\nTitle Publication \\nand year \\nDescription \\nPaper-1 \\n[1] \\nDeep neural \\nnetworks in \\nmachine \\ntranslation : \\nAn overview \\nIEEE, 2015 Various \\nNeural \\nNetworks used \\nin Different \\nNLP methods \\nare described \\nPaper-2 \\n[2] \\nDescribing \\nmultimedia \\ncontent using \\nattention \\nbased \\nEncoder-\\nDecoder \\nnetwork \\nIEEE \\nTransactions \\non \\nMultimedia, \\n2015 \\nDescribes \\nAttention \\nBased Neural \\nNetworks \\nPaper-3 \\n[3] \\nJoint Layer \\nbased Deep \\nLearning \\nFramework \\nfor Bilingual \\nMachine \\nTransliteratio\\nn \\nIEEE, 2015 Use of Deep \\nBelief \\nNetwork for \\nTransliteration \\ntask for Tamil \\nand English \\nlanguages \\nPaper-4 \\n[4] \\nMulti-Sense \\nBased Neural \\nMachine \\nTranslation \\nIEEE, 2017 Shows \\ncomparison \\nbetween RNN \\nand Sense-\\nbased model \\nand introduces \\nthe multi-\\nsense \\nrepresentation \\nPaper-5 \\n[5] \\nNeural \\nMachine \\nTranslation \\nResearch \\nBased On \\nThe Semantic \\nVector Of \\nThe Tri-\\nIEEE, 2016 Describes two \\napproaches for \\ntrilingual \\ntranslation \\nwith parallel \\ncorpus based \\non RNN and \\nattention \\nlingual \\nParallel \\nCorpus \\nmechanism \\nPaper-6 \\n[6] \\nA Class-\\nspeciﬁc Copy \\nNetwork for \\nHandling the \\nRare Word \\nProblem in \\nNeural \\nMachine \\nTranslation \\nIEEE, 2017 Proposes a \\nclass-specific \\ncopy network \\nmodel to \\novercome \\nsome issues \\nfaced in \\nhandling rare \\nwords \\nPaper-7 \\n[7] \\nRecurrent \\nNeural \\nNetwork \\nLanguage \\nModel for \\nEnglish-\\nIndonesian \\nMachine \\nTranslation: \\nExperimental \\nStudy \\nIEEE, 2015 Uses RNN \\nlanguage \\nmodel for \\nEnglish-\\nIndonesian \\nTranslation \\nand shows \\ncomparison of \\nthis model \\nwith Statistical \\nbased \\nlanguage \\nmodel \\n \\n \\nIII. PERFORMANCE IMPROVEMENT USING GPUs \\n \\nDeep learning application requires high computations \\nbecause there exists large matrix multiplication, parallel \\nprocessing and number of calculations during training \\nphase. Graphics processing unit (GPU) i s very good \\noption for parallel processing and fast  computation as \\ncompare to the CPU.  GPU not only provides better \\nenergy efficiency but it also archives substantially \\nhigher  performance over CPUs. \\n \\nIV. CONCLUSION \\n \\nIn the present time, machine translation is a very hot \\nresearch topic in natural language processing area. Deep \\nlearning helps to  train  a  translation  system  like  a  \\nhuman  brain. RNN and RAE provides  better  result  in  \\ntext  processing  as  compare  to  other neural  networks. \\nFrom above all papers about Machine Translation I have \\nanalyzed that deep learning is much better than any \\nother methods for giving accurate result. But the \\nprocessing time is high, also it needs more time to \\ntraining a system.'),\n",
       " Document(metadata={'source': 'data\\\\machine_learning_translation_using_DL.pdf'}, page_content='International Journal of S cientific Resear ch in Science, Engineering and Technology (ijsrset.com)  \\n \\n \\n150 \\nV. REFERENCES \\n \\n[1]. Jiajun Zhang and Chengqing Zong,\"Deep neural \\nnetworks in machine translation : An overview\", \\nin IEEE intelligent system, 2015. \\n[2]. Kyunghyun Cho, Aaron Courville, and Yoshua \\nBengio, \"Describing multimedia content using \\nattention based Encoder -Decoder network\", IEEE \\nTRANSACTIONS ON MULTIMEDIA, VOL. 17, \\nNO. 11, NOVEMBER 2015. \\n[3]. Sanjanaashree P, and Anand Kumar M, \"Joint \\nLayer based Deep Learning Framework for \\nBilingual Machine Transliteration\", IEEE, 2015. \\n[4]. Zhen Yang, Wei Chen, Feng Wang, Bo Xu, \\n\"Multi-Sense Based Neural Machine \\nTranslation\",in IEEE, 2017. \\n[5]. XIAO-XUE WANG, CONG -HUI ZHU, SHENG \\nLI, TIE -JUN ZHAO, DE -QUAN ZHENG, \\n\"Neural Machine Translation Research Based On \\nThe Semantic Vector Of The Tri -lingual Parallel \\nCorpus\", in IEEE, 2016. \\n[6]. Feng Wang, We i Chen[1] ,Zhen Yang, Xiaowei \\nZhang, Shuang xu, Bo Xu, \"A Class-speciﬁc Copy \\nNetwork for Handling the Rare Word Problem in \\nNeural Machine Translation\", in IEEE, 2017. \\n[7]. Andi Hermanto, Teguh Bharata Adji, Noor \\nAkhmad Setiawan, \"Recurrent Neural Network \\nLanguage Model for English -Indonesian Machine \\nTranslation: Experimental Study\", in IEEE, 2015. \\n[8]. Biao Zhang, Deyi Xiong, Jinsong Su, and Hong \\nDuan, \"A Context -Aware Recurrent Encoder for \\nNeural Machine Translation\", IEEE/ACM \\nTRANSACTIONS ON AUDIO, SPEECH, AND \\nLANGUAGE PROCESSING, VOL. 25, NO. 12, \\nDECEMBER 2017. \\n[9]. Deep Learning – Wikipedia, \\nhttps://en.wikipedia.org/wiki/Deep_learning .   \\n[10]. Xiao Sun, Xiaoqi Peng, Fuji Ren, Yun Xue, \\n\"Human-Machine Conversation Based on Hybrid \\nNeural Network\", in IEEE, 2017. \\n[11]. Sandeep Saini,  Vineet Sahula, \"A Survey of \\nMachine Translation Techniques and Systems for \\nIndian Languages\", in IEEE, 2015. \\n[12]. Eric Greenstein, Daniel Penner, \"Japanese -to-\\nEnglish Machine Translation Using Recurrent \\nNeural Networks\", available at \\nhttps://github.com/lisagroundhog/GroundHog. \\n[13]. Bing Zhao, Yik -Cheung Tam, \"BILINGUAL \\nRECURREN T NEURAL NETWORKS FOR \\nIMPROVED STATISTICAL MACHINE \\nTRANSLATION\", in IEEE, 2014. \\n[14]. Preeti Dubey, \"Need for Hindi -Dogri Machine \\nTranslation System\", in IEEE, 2014. \\n[15]. Heeyoul Choi, Kyunghyun Cho, Yo shua Bengio, \\n\"Context-dependent word representation for \\nneural machine translation\", in Elsevier, 2017. \\n[16]. Todd Law, Hidenori Itoh, Hirohisa Seki,\"A neural \\nnetwork assisted Japanese -English Machine \\ntranslation system\", International  Joint  \\nConference on Neural Networks, 1993. \\n[17]. KyungHyun Cho, Yoshua Bengio,\"NEURAL \\nMACHINE TRANSLATION BY JOINTLY \\nLEARNING TO ALIGN AND TRANSLATE \", \\nin ICLR, 2015. \\n[18]. Neural Machine Translation Tutorial - \\nhttps://sites.google.com/site/acl16nmt/home')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dac9bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the documents into smaller chunks\n",
    "def text_split(minimal_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=20,\n",
    "        length_function=len\n",
    "    )\n",
    "    texts_chunks = text_splitter.split_documents(minimal_docs)\n",
    "    return texts_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bd7beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_chunks = text_split(minimal_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a341f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "def download_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "245c7f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_25608\\235428984.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "c:\\Users\\HP\\anaconda3\\envs\\medibot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding = download_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8305a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e748662",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n",
    "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c1b102e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pcsk_tFpQk_5LhSYVzfwcyn4XGBKs2XCMScdATagKFy9GYzfGwu8vmTfM5uPJPKUvjRdNxnHEn'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f8e0b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "pinecone_api = PINECONE_API_KEY\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bde6da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "index_name = \"ai-chatbot\"\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\",\n",
    "        )\n",
    "    )\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0452c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "doc_search = PineconeVectorStore.from_documents(\n",
    "    documents=texts_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fe0709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if i already have an index, i can use the following code to load it\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "doc_search = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a610c329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3e281d48-670c-4f26-a2e9-58ba57000868']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add more data/documents to the existing index\n",
    "dswith = Document(\n",
    "    page_content=\"This is a new document to be added to the index.\",\n",
    "    metadata={\"source\": \"new_document.pdf\"}\n",
    ")\n",
    "doc_search.add_documents(documents=[dswith])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e09eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = doc_search.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42f16c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='559aad3f-4f20-4d73-884f-b80346600dc0', metadata={'source': 'data\\\\Hands-On Generative AI with Transformers and Diffusion.pdf'}, page_content=\"('TITLE  II \\\\n'\\n 'PROHIBITED  ARTIFICIAL  INTELLIGENCE  \\nPRACTICES  \\\\n'\\n 'Article 5  \\\\n'\\n '1. The following artificial intelligence\"),\n",
       " Document(id='e1d376f0-483a-42ae-938f-60087de5139e', metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le (1).pdf'}, page_content='results at doing science [2 -6]. AI system s are used as an \\neffective mechanism in diverse scientific fields transforming \\nconventional research practices and expediting discoveries. \\nThe main advantage of AI is that it can outperform humans \\nwhen it comes to processing large amounts of data, detecting \\npatterns and abnormalities that human experts could never \\nhave spotted.  \\nFig. 1 demonstrates the integrated liaisons between key \\nelements of AI. \\nC'),\n",
       " Document(id='97198f01-ecf9-4975-814f-ea3dd96150b5', metadata={'source': 'data\\\\Generative_Adversarial_Neural_Networks_and_Deep_Le.pdf'}, page_content='results at doing science [2 -6]. AI system s are used as an \\neffective mechanism in diverse scientific fields transforming \\nconventional research practices and expediting discoveries. \\nThe main advantage of AI is that it can outperform humans \\nwhen it comes to processing large amounts of data, detecting \\npatterns and abnormalities that human experts could never \\nhave spotted.  \\nFig. 1 demonstrates the integrated liaisons between key \\nelements of AI. \\nC')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrived_docs = retriever.invoke(\"What is AI?\")\n",
    "retrived_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "229e709a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-cohere\n",
      "  Downloading langchain_cohere-0.4.5-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting cohere<6.0,>=5.12.0 (from langchain-cohere)\n",
      "  Downloading cohere-5.17.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langchain-cohere) (0.3.26)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langchain-cohere) (0.3.74)\n",
      "Requirement already satisfied: pydantic<3,>=2 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langchain-cohere) (2.11.7)\n",
      "Collecting types-pyyaml<7.0.0.0,>=6.0.12.20240917 (from langchain-cohere)\n",
      "  Downloading types_pyyaml-6.0.12.20250809-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.12.0->langchain-cohere)\n",
      "  Downloading fastavro-1.12.0-cp310-cp310-win_amd64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: httpx>=0.21.2 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from cohere<6.0,>=5.12.0->langchain-cohere) (0.28.1)\n",
      "Collecting httpx-sse==0.4.0 (from cohere<6.0,>=5.12.0->langchain-cohere)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from cohere<6.0,>=5.12.0->langchain-cohere) (2.33.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from cohere<6.0,>=5.12.0->langchain-cohere) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from cohere<6.0,>=5.12.0->langchain-cohere) (0.21.4)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.12.0->langchain-cohere)\n",
      "  Downloading types_requests-2.32.4.20250809-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from cohere<6.0,>=5.12.0->langchain-cohere) (4.14.1)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.3.26)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.0.43)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.4.14)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.2.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langchain<1.0.0,>=0.3.26->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.3.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-cohere) (1.33)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-cohere) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-cohere) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from pydantic<3,>=2->langchain-cohere) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from pydantic<3,>=2->langchain-cohere) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.1.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.12.0->langchain-cohere) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.12.0->langchain-cohere) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.12.0->langchain-cohere) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.12.0->langchain-cohere) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.2.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain-cohere) (0.34.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain-cohere) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain-cohere) (2025.7.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain-cohere) (4.67.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.1.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain-cohere) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain-cohere) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from httpcore==1.*->httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain-cohere) (0.16.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.23.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain-cohere) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from anyio->httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain-cohere) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hp\\anaconda3\\envs\\medibot\\lib\\site-packages (from anyio->httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain-cohere) (1.3.1)\n",
      "Downloading langchain_cohere-0.4.5-py3-none-any.whl (42 kB)\n",
      "Downloading cohere-5.17.0-py3-none-any.whl (295 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading fastavro-1.12.0-cp310-cp310-win_amd64.whl (450 kB)\n",
      "Downloading types_pyyaml-6.0.12.20250809-py3-none-any.whl (20 kB)\n",
      "Downloading types_requests-2.32.4.20250809-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: types-requests, types-pyyaml, httpx-sse, fastavro, cohere, langchain-cohere\n",
      "\n",
      "  Attempting uninstall: httpx-sse\n",
      "\n",
      "    Found existing installation: httpx-sse 0.4.1\n",
      "\n",
      "    Uninstalling httpx-sse-0.4.1:\n",
      "\n",
      "   ------------- -------------------------- 2/6 [httpx-sse]\n",
      "      Successfully uninstalled httpx-sse-0.4.1\n",
      "   ------------- -------------------------- 2/6 [httpx-sse]\n",
      "   -------------------- ------------------- 3/6 [fastavro]\n",
      "   -------------------- ------------------- 3/6 [fastavro]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   --------------------------------- ------ 5/6 [langchain-cohere]\n",
      "   ---------------------------------------- 6/6 [langchain-cohere]\n",
      "\n",
      "Successfully installed cohere-5.17.0 fastavro-1.12.0 httpx-sse-0.4.0 langchain-cohere-0.4.5 types-pyyaml-6.0.12.20250809 types-requests-2.32.4.20250809\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f16f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "\n",
    "system_prompt = (\n",
    "   \"You are an medical assistant for question answering tasks.\"\n",
    "   \"Use the following pieces of retrieved context to answer \"\n",
    "   \"the question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "   \"Use three sentences and keep the answer concise.\"\n",
    "   \"\\n\\n\"\n",
    "   \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4411d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI stands for Artificial Intelligence, which refers to the ability of machines or systems to mimic human intelligence and perform tasks that typically require cognitive functions. It encompasses a wide range of technologies that can process and analyze vast amounts of data, detect patterns, and make decisions or provide recommendations. AI has become an invaluable tool in various scientific fields, aiding researchers in their quest for new discoveries and advancements.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is the AI?\"})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d1219a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
